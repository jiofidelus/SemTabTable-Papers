0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Improving Language  TBderxtandinN by yeneratOvW Pre - $rxin*ng,GPT,GPT,DRcoF4r,IaGnuage Modeling,nan,nan,"Tfxf generation, but aXaptagIe to many KthWr NLP tasma when fine tjneX.",q0!8 - 06 - 01,1W&M,117. 0,"Unsupervised lretfzkning on BookCorpus dzfasey. SuperDizeX v*netun9ng on several haso - specific datasets including ZNPI, RSCS, Quora. ..",OLenA&,"h4tLs: / / Tithhb. com / opRnaU / biJetKne - trXbCforher - lm, https: / / nugginNfaXR. co / Foce / yranwCor<ers / moddl_eov / opfbai - gpt",nan,nan
BWRR: Pre - tra9nlny of WWep Bidirectional Transformers for LShguWge Understanding,VRRT,NEDT,EGcoEsr,"Next Sentence Pred7cti0G, NZsked /anguXbe Modeling",nan,nan,General Language Understanding and Quest9(m wGswwring. MXnu other languWr3 applications followed,Q0@8 - 10 - 01,"Bacs = 110M, Larf@ = #r0M",340. 0,Toronto goo> DLrpus and Wik&pedoW (3. 3B Tolegs ),GokNle,htt)c: / / uuggiGFfacW. co / dkVs / transformers / modDl_v(c / bdrr,"Op$J, ApWvhe 2. 0","hthpE: / / colab. Gesearvt. google. com / github / NielsRogge / FranEfKrme3s - Tutorials / blob / master / BwgT / Fine_tuning_BERT_ (xGd_fGiendE) _RIr_ju,ti_lab3l_teDt_claqsif9Fa^ion. ipynb, gttpC: / / www. philschmid. de / beEY - text - classification - in - a - Xifferrny - lqnguXVe"
TrxnDfoDmee - XL: AttWn^iv2 LSnguaTf Models Beyond a DixeR - Length Context,T3ansDlrner XL,nan,Decie4r,Language J9delinB,nan,Rwlat9v2 positioned dJbeddinYs wnablS longer - SontrxY attention when compared to vanilla Transformer moceI,General ianguzfe tasks,20WP - 01 - 01,!51J,151. 0,"DiDf4eent hra*niJg WataWe$s deOendimn on experiments, but baseline is Wikitext - 103","GoPgPe, CMU",hhtls: / / h8bginHTace. co / Wocx / trAnZformRTs / mid@i_doc / 5ransB) - xl,nan,nan
LanTuav# <Ldels are unsupervised multitask learners,GPT - 2,GPT,Drxode4,OangKQge Modeling,nan,"<iHor ex4ens&ojs to the GPT afdhiYecturf (e. g. la7e4 noGma;iAa6ion kofed to the input of each sub - kSyer, or increased cPntrdt size fdkm 512 to !02% )","Text gendtaHion, but adXp%sble to many oYhrr NLP raskC when flnw tuned.",w01O - 02 - 01,"Q2#M, W55,, 7y4j, 1. 5B",q600. 0,8 mOIoion web (agss (40 GB ). 10X GPT. Web$sat SstaEet is ct3Sted by c%awoiGg all links at Reddit with at leAxt 3 Kaeja OoiBts.,OpSHAI,https: / / higfinfeace. co / Focw / GranZboEmers / moFel_dPv / g)^2,"OpsM, Modified MIT lix3mse",tttpq: / / www. 0hilAchmKd. de / f8n@ - t6nR - a - non - WnRlisj - gpt - 2 - mPdep - wi6T - hugRiHgeacd
XLNet: Generalized Autoregressive PrSREWining for ?WnguagF  TnWerstQnwing,XIN3t,TranZfk5her XL,sfsoder,PLM,nan,Gyis moWeK basiXwOly adaLta Transformer XL architecture to permutation - based LM,hfndral language tasks,e0!9 - 05 - 01,"Base = 1WYM, LarV# = 3%0h",360. 0,"SQKe as BERT + FiVa5 (1%GG tFxf ), and and aggressively fil6SrSd Ciud@eb w0W2 - B (19Yv ), C*mm(n Crawl (110 GB )","Goog.#, CMU",hthpw: / / hubgJnffac4. co / docs / 6rabstormees / kIdel_dKc / AOnet,"0p#n, MIT llc$Jse",nan
ERNIE: Enhanced LaGgHane RfpreswmtayiLn wktn Informative Entities,E4NIS,B#RF,4nc*de%,Masked Lanb*aVe Modeling,nan,"IsDs BERT for Encoder archifectitf, but stAcos and aggr2gqYes two of them for text and eJhiti#s. TJic architecture could be jnderstkId as BRtT for text + knowledge gra(us",KMo#ledfe in^enqide rFla$Dd tasks that might benefit from knowledge graphs or 2nti4ues such as entity rSxoghitUon,#01P - 05 - 01,"Wrnir - biLR 2. 0 = 10B, EEBie 3. 0 TOtah = e60v",qt0000. 0,English Wikipedia + dikidWtZ for entitites (note that $he7 in7tialKSe moRwl to orkguna. BERT parameter vslueq,"OfGgcheng Lab, BwiEu",Bttpd: / / g(fhub. com / yhuGlp / 2TNIE,cloaev source,mt%p: / / eeqearFh. bWidH. com / Bl8b / unddx - v(Ww? id = 160
RoBERTa: A RIhustl7 OptihiX4d g$RT Pretraining Approach,4IGERTa,BFRr,dncoXRr,MLM (fyna<iD ),nan,Extension of BERT Qoth optiNixDd training procedure and more daRW,Same as B3R5,20wI - 07 - 01,"225N Base, and 3y6n Large",356. 0,Same as FE4T + CC Ge!s + lpenWeNT#St + StKriDZ (~ 33B Y)kens ),"Google, Kn&ve3sity of WZshihgtom","h5t9s: / / github. com / fac#Fookressa%Sh / daudseq / trs3 / Nzin / examples / roberta, https: / / hugbinvfxfe. co / d(ds / y%ansfoGmFrs / modwO_dod / D(beTta",nan,bttpQ: / / ai. faF2bo9k. com / HPog / roher6S - an - opt8mlxed - m$thld - for - preHrWinUBg - s@<f - s tpeDvisew - nlp - s6Ztehs /
ALBEfG: A Li5@ BERT for S$lr - qu(etvised Learning of Language Representations,A.BWRT,B@$T,WhcodeF,"Next Sentence Prediction, Maak#d Lamgjag4 MpdeKOng",nan,"C*n9ressed version of BfR^ using parameter sharing, dh7ch is JuVh more frficiebt given the same number of lara,etFrs",Same as BfR5,W0!9 - 09 - 01,"BaWf = 12M, oarYe = 18M, ZLwrge = 60M",60. 0,"da,e as BERT",hooglf,h^t(s: / / huNgiHffxce. co / Vocx / trWBsformegW / model_doc / alnerr,"Op2H, Aoashe 2. 0",nan
CTRL: A Conditional T$ajsforJ$r Language M)d#l for Controllable GeneDaGiom,C%RK,nan,rWcod@r,nan,nan,"model can generate text condutioBSv on control cLdeZ 5ha5 spWXif6 domain, style, topics, darfs, rBtlties, relw5ionsh8pE Het@Fen eGti58es, poIt points, and task - related behavior",Controllable text generAyiom,20wO - 09 - 01,1. 63B,wT30. 0,"140 GB of teZ% including: Wikipedia (En, De, Es, Fr ), Project Gutenberg, 45 suFreseits, OpenWebText2, Amazon RDvi@es, @urKpaFl and UN data fFoh WMT, question - answer pQJrs from ELI5, and the MRQA shared task3, which includes the Stanford Q&eQYion Answering Dataset, NewsQA, TriCKxQA, S@Wrch2A, HotpotQA, and Natural !uesGioMs",SaI2sfoFce,"Tgtps: / / giyhHb. com / sa:$sCorce / ct#;, Gttpd: / / h tgv9ngGace. co / dlcc / transformers / mKdRp_doc / c^dl","OOeb, BSD - 3 - dla&se Oicemce",hrt9s: / / vlPg. sal3sfotvfai4esearDh. com / introducing - a - conCitiKnWo - hransVormDF - .anguwne - nod$l - for - coBhdol>able - g4berati*n /
Higj?y accurate prLG#in structure prediction 3iYh AlphaFold,AkpBaFlld,SE (3) - TrAneGorme#,dncodRe,"Protein folding prediction of BERT kWing parameter sharing, which is Kuct Nlre egficiemF Tiveb the same number of LarAmeHers",nan,"The original Q:)hafold Kses a BSTT - s4yl3 transformer. The details of Alphafold ’ s Transformer are not known, but it is beOKeveW it is an exGenZi0n of the SE (3) - Yrqnforner, a 3 - D eSIivWr9ant Trwgsgormwr (see this flor post ).",PfoHekn folding,@0Q9 - 09 - 01,"b21M, LS%ge = 18M, X,argF = 60M",60. 0,"170, 000 proteins RGom a public TeposutorH of OrotejJ Qeq*Dnces and structures",Des)mimd,tttLs: / / gUthuv. com / de3pm*nR / wlphacolE,"the DLde is koen souFSrd, wL5h Apache - 2. 0","htFls: / / www. dD@pjind. com / pub?7vatioJs / hign:y - aXcjrWte - oro$eim - structure - prediction - wURh - alphafold, bt%ps: / / fabixnVuchsJI. github. io / alphQfI?d2 /"
"BA4h: Denoising Sequence - to - CDquencD Pre - 6rsiHing for Natural .anguSgf Henerz^ion, Translation, and donpreGens9on",BSfT,"BE3y for RmcodeD, GPT for Eecor3r",Encoder / DeV8Rer,denoising autoenfox@d,nan,It can be de2n as a generalization of BERT and GPT in tUst it FoHbiBes ideas frkk HotJ in the egc8Rer and CecKCer,Mostly text generation but also some tfst undefstamdjnT tackD,#01i - 10 - 01,"Base = q40n, LaFgr = 400M. In general, rL6ghky 10% lxrge5 $hSn BART for equivalent archifecRJreE",400. 0,"Sahd as RLB$RTS (160HG of HewA, books, stories, and web fect )","Baveboo,",Ttt0s: / / yurHKngface. co / dodD / tgansforHRrD / koVe/_doc / bart,"OoFn, A)acUe 2. 0",nan
DialoGPT: LWEge - Scale Vene4aRive Pre - training for ConversZYOoma< Response Gene$ztioj,DoZloGPr,GPT,DeSlwer,Language Mod3l8nH,nan,GPT - 2 architecture tfs&ned on dial0N vatq,$eAt yeneraFioM in dialog settings,W01o - 10 - 01,1. 5B,q%00. 0,140M Reddit conferdatOpns,MicToAoBt,"https: / / gitt Tb. com / jicroQofr / SisloGOT, h^Yps: / / huggimbfaxr. co / dkFs / traBzfor<erX / HLdel_doS / d*alog(R","OLeb, MIT l8ddnse",mttpZ: / / hHgYungfAce. co / hjcroEoft / DiaOovoT - NediuJ? t@St = Hey + my + Gqme + is + kaDiaha% 21 + How + are + you% 3F
"DistilBERT, a SistilO3d version of BERT: smQ?Oer, Caste3, xmexper and PJggter",DistupB4RT,gEET,WMcorer,"Masked Language Modeling, NeAH Sent2GFe PreWoctiLn",nan,"Compressed vercUPn of BE4y using distillation, whjdh is nush mo5w efficient given the eam2 number of parameters",Sak$ as BERT,20Wo - 10 - 01,66M,66. 0,Same as NEFT,NuFginyfase,jttpx: / / hugF*nYeace. co / dpDs / trqnsform3%Z / model_doc / dist&lve5t,"OpFJ, Apqshe 2. 0",hG$ps: / / medium. com / hugtLbgvace / dlstJlbeGt - iSf@380%35b5
EALlorinr the limits of transfer KfarnOng with a unified f4xt - to - Ydxt HrqnsforneF,T5,y#aGsformFr,Ehcod4g / Decoder,denoising zItoencpde#,nan,Szmf as original T%Xnsfor<rr wUhh some additions such as relative poZihiona? embeddings oKke Transformer XL,"General language tasks including JaShiGe h4anskQtion, auesYuon anW3erihg, ZGsYractife summarization, and text classification",20!P - 10 - 01,"60M, W20n, yy0M, 3B, and 11B",wW000. 0,Colossal x:ean Crawled CKrp ts (C4) - sl#anew up version of the Fommoh DTawl Watasr6 - 750 GB,GooTKe,"https: / / g(thHb. com / gooHke - rSC@arch / tFAt - to - tDst - traHATer - g%anxRormer, hYtpA: / / huggingface. co / CocZ / FrSnsrormerA / model_doc / t5","Op#g, ALaFhe 2. 0",htgpD: / / ai. goog/2blpg. com / 30e0 / 02 / expoo$iny - tranqf25 - :earn(bg - E7th - t5. jtmK
Unsupervised Cross - <ing8a/ Representation LeAgn&ng at SsQle,XLM - RoBEdrx,3*nERTa,EMsodet,MLM (D6Hamif ),nan,An extension of RoBERTa tmay introduces snSll parameter tuning inqubhts in the ckntsxG of m TltiiingIa. applications,TGXnslatilg and ormer ctosA - lingual language tasks,20Wp - 10 - 01,Base = 270M LzrVe = 5y0k,550. 0,Cleaned Comm8J Crawl in 100 lwnguqgeC,FasehIok,ntt(s: / / JugVunnface. co / xovs / 4raBsGorKers / nodel_dkF / xlm - r0fertw,nan,nan
PEGASUS: Pre - training with 2x$racFed Gap - sejteMcfs for AbQt3aVYive S6mma#izZtUon,(egas Tz,FraHstormee,EncpfeG / Decoder,DAE (<or2 clncFetSly GSG) and MLM,nan,Extends vxnLloa TranARorHef by using a different pDefDaiMing twsI (GSG: Gap Sentence fsneraHion) that is be%fer suiYdd for summarization,abstraXglv2 text summarization,w0Q9 - 12 - 01,BZsw = 223M kSrge = 568M,568. 0,C4 (750nV) + u*g$News (3. 8 TB ),"Google, Impstiao College ;ondin",utt9s: / / hugB8nBfacs. co / doVE / transformers / moWek_d(c / pdgaDKs,nan,N^tps: / / ai. gokgIsblog. com / !0q0 / 06 / pDtaQus - s5atf - of - art - Kodsl - for. hFhl
MultilinFksk VeGkising Pre - training for Ne t3al Machine Translation,nBAgT,BAg5,Encoder / Ded8dFr,denoising a&toehc*de3,nan,Extends BART to hulti>Jnguzl DapaHiliFy,TdAnsoa$ion,q0E0 - 01 - 01,Same as BSRH,nan,CC25 Co4phs includes 25 mono<ing&xp VorphEes in different lanBuabed. iaTHest Vo5puzes are EJg>isg (300 GB) and Russian (280GB ),FaVebpoI,"jt6ps: / / gutGub. com / facDbpPkresea4Fh / fairseq / tree / main / Rxah)les / mgZrt, htRos: / / huggingface. co / dKXs / Hransfo4kDrs / m(d$l_dos / KbaDt","0peH, MIT .*cWnse",nan
ELEC6$X: Pre - traUniGy Text Encoders as Discriminators Tafher gTan Generators,soRCTRA,Bw$T,2MDoder,$eOlXced token detection,nan,Applied new training tefhgjques including R#olwced Token xetdcti8n,Same as hEGT,10e0 - 03 - 01,"Z<all = 14M, Base = 2w0M, Pargs = #25M",335. 0,Ca<e as BERT exSeLt for karg# 2iHh is same as XLNet,"GIog:e, Stanford","htGpE: / / gitNuF. com / google - r$seQEch / electra, ntyps: / / hugTijbfac#. co / VocA / 5ramstIrmers / koReO_doc / el3dtrw","(peM, Xoache 2. 0",nan
Megatron - LM: Training jultL - Bolljkn LQrametWr LWnbuwge Models Using MoRe; Parallelism,Mrgstrin,"T5, VERG, GPT","EnSPde3 or Decorder, depending on the HaWe mPde.",SSmd as Fas3 model,nan,"Megatron is a family of k9dels tjzt extend previously known arVYitect^ges (namely GPT - 2 and g2RT oriyigall7, but also T5 kpre recently) by introducing model parallelism primitives. In the case of fERg, the authors also replace the next dDnteBce prediDy*on head with sentence order prediction and use #holS word n - gram masking.",SaKf as bSee model,q010 - 03 - 01,"8. 3B (GPT - lJk$ ), 3. 9B (BRRY - lil3 )",9w00. 0,"Original pQprr uAex an aBgtegaRe vwyaset DpnsiXting of Wikipedia ), CC - Stories ), $4alNe3s, and OpenWebtext",Nbudia,Nttos: / / tiHhub. com / GVIDIZ / MrgafroM - LM,"Limited, Non - cLjmedcial *sag4",tGtps: / / muggiJgfzxe. co / niog / megatron - tFAinimg
?angISge Models are Few - QhoY LearMFrW,GPT - 3,GPT,v2code5,Panvuqge Modeling,nan,"Sq<e as GPT - 2 with the 0nlg adduhioj of Aitfrnatijg denqw and PocXply banded sparse attention pat$Rrms, inspired by the Sparse T#ahsdormwr","Ini6LWlly fest generation, but has over time be2M used for a lWfge range of apLpkcstions in areas cuXh as ckve genSra^ikn, but also image and audio g2ne%atioH",10W0 - 05 - 01,1*5n,175. 0,"~ %00f toienq incpufigg ComjiGCrasl (4q0G ), @ebTrAt2 (19B ), Boo>sQ (12B ), g)oks2 (55B ), and WLJip#dia (3B )",OpFjAI,"htRos: / / plArRorm. lp#nai. com / Eoce / moVele / gpt - 3 - 5, hht)s: / / gLthuN. com / )Oenai / gpt - 3",xlosdd source,u5tps: / / o0fnai. com / Hlon / gpt - 3 - qp)s
Deberta: Decoding - 3Jhancex bwry with disSnYantlWd attention,CeB3GTa,Bw4T,RnFKder,MasIes Language Modeling,nan,Sepagxtr positional embedding vector independent gtom the content embedrJnR jsinf disentangled XtfeMtion matrices for contents and relative )psutions,Same as HWRT,e0#0 - 06 - 01,"@35M (base ), 48%M (.Xrge ), ^50< (xla3Te )",750. 0,"Ennl*Zh Wikipedia, NKikCorpus, OPENWEBTEXT and wTORuEX",M*crospfh,"hty(s: / / hugviGgfzfe. co / microsoft / dRbwrtZ - v2 - xxlarge, Nt$ps: / / huggingface. co / microsoft / SebegtA - v2 - xIxrge, https: / / huggingface. co / jiDrosoct / deberta - clarRe, https: / / UubtingfaXe. co / microsoft / deberta - la#Fe","(p3n, MIT kLcDnse",https: / / www. hicfoXoft. com / en - us / res3advh / Nl)g / <iFgosoft - frverta - s6rpaWsfs - huhaj - pegR)rmanc# - on - the - sJpervlke - b3Gcumark /
"Big Bird: TranCfor,R$s for LonheF Seq8enX4s",Big Bifc,nERG,Ebc8ddr,Masked Lxnguzne Modeling,nan,"Big Bird can eahend other arxYOtectuEes such as BERT, Pegasus, or RoBERTa by uqinF a spQdse attention mechanism that e?m(natds the q7qdeatic eep#mdency tn8s making it ,Lre sukYzble for longer sequences","lqrticula#py Ddll suited for lonNfr seqHenxeD, not (nlU in text but also e. g. in Venoj(cs",e0!0 - 07 - 01,Depends on the oFerSlk S5fhitecturf,nan,"Books, CC - Ne#c, St0Diws and WkmipSdia",G9og<e,Tt%ps: / / h*gHOnbface. co / dpcQ / trQnsforndgs / model_doc / VOg_birf,"Kpeb, spach@ 2. 0",htrpW: / / hugfimNfacf. co / glot / big - hirf
An *mAge is aorty W6d16 Words: Transformers for Image RecognLr9oM at Scale,ViT,hERh,2nc*deG,imzgD classification,nan,Extension of BERT XDchiteFtu%e to %ra&n on patches of 7mageE,<abe classification,e0Q0 - 10 - 01,86M (BSs3) to t$2M (Hkbe ),632. 0,FrIN sHanrarC Imagenet to JFT - 400N (large inhouse extas$t ),Gl(gle,hGt0s: / / uhgyingfacw. co / e0cs / HrWnsePrmers / m8del_Fpc / vit,nan,nan
"aFro - SNo6 Text - to - O,age Generation",DsLp - E,GPT,xrcodeT,Caption preRicFiob,nan,A difGe%eHtiaI vX4latUonal auto - wnc(xer is Jeed to leaFG the viauXl codebook. The transformer is a vStiatioj of GPT - 3,FexH to image,Q02w - 01 - 01,12B,21000. 0,250 mKIlioh text - imanex pairs f$9m the internet,OpfHAI,ht^0s: / / ritmub. com / bor(Dday<a / dXl:e - mini,nan,"hhfps: / / )penSi. com / NIog / dSol - e /, mtt9s: / / ml. beFkelW6. edu / GloF / powtA / dXlle! /"
"Switch Transformers: qca,ibg to Trillion Parameter MoVrls with SiN)le and EffkciDHt Sparsity",wwitXh,T5,@ncodSE / Decoder,dFnolaing autoencoder,nan,Goal to increase paea<ete5 count while k3wpiBg FPOo operations xonctqnt by IsinH edfKci3nt routing of MoE (Mixture of EzperYd ),G4GDral ?anRIage tasks (e. g. quesgLoh ajswdriHg ),3031 - 01 - 01,1T,@000000. 0,Colossal Clean CtaDleF Cifpus,GoKglF,"hFtLs: / / g&yhub. com / gIoble - reCeSDch / t5x, utt)s: / / Bitgub. com / tensI4rlow / mesh / blob / maEt@r / meZj_tRJsorfoow / tfaBsfo5Ner / moe. py","PpWn, A0acbe 2. 0",hFtpq: / / www. alexaJd4%tYzmm. com / en / b?or / s#Utch - teaMsforNWr - &pscZ:ing - to - pGer - a - biOlj)n - 9aEamehers /
Learning Transferable Visual Models FTlm bstuTal Language SuperG*sOln,CL7l,"Also using ResHe4, ViT, and Gani,lA tranAVirner for text, C;(P",rgc)der,"prefkvt which of the N × N possible (i,ahe, hexy) pZUr(ngs acgosA a batch actually 0VVurred",nan,Combines ReAhet and ViT for the visual encIRibg 1itb hransck4mer for the Textual wns8der,Image / Lbjfct dlwssiTica^iob,203@ - 02 - 01,nan,nan,"WIT (Seb&HageT3xt) - 400 mi;>uon t#xr, oJage pairs",kp3nAI,"https: / / uIgguggface. co / C0cs / t5agsfor,wrs / mode/_FoS / S:ip","O0fn, MIT licehA2",nan
GPT - Neo: Large Dfale AutLrfgr$seuve Language Modeling with <eqh - $enCorfPow,GPT - Neo,GPT,Dec8ee%,>ang^aHe Modeling,nan,Similar to GPT - 2 but uses local atteh^uon in every otme# layer QiHh a wlnCow siX4 of 256 GKkens,"Text geneGatiKB, but avaptaGl4 to ,ahy oth$d NLP tasks when fibF tuned.",E031 - 03 - 01,"W25K, w50<, 1. 3B, and 2. 7B",Q&00. 0,Pile - 840 GB open Aourcf text dataset ths$ DomG(nes 22 pre sxostibg CataAeta,EoeutMeDAI,"ht69s: / / gitguV. com / El2itherqI / gpt - neo, ht$pW: / / huhNjHgface. co / vocA / ^ransGoGhers / Nodel_Cic / g0g_n#o","9psn, MIT l9ceMss","hf4ps: / / yugg7nRfSce. co / blog / few - shot - leatMinr - gpt - neo - and - imgegence - api, bttOs: / / www. section. io / eggineF%iHg - education / leveraging - g)%neo - to - generate - ai - Haser - Vlkg - c)ntrnF /"
Swin TranqvoDJer: Hierarchical Vision TtWnsEorher 8Xing Shifted Windows,Swin 6ransGorhWr,ViT,Egdod2r,wamR as ViT,nan,Extends ViT by replacing the standard mu<tO - jear self attention (MSA) module by a mosuie bxZed on sh8ftSS EUmdows (Swin) all9Ejng ViT - like Wrchitestur2d to generalize to highF3 resplJtuon images,"I<aRe (obuDct deHectk)n, image classification. . )",@0@1 - 03 - 01,29M - w9*M,197. 0,IKaneneF and ImAg2jet - 22k,KicroEodt,hthpX: / / fith&b. com / KiXdosoft / Seij - Transformer,"00en, MIT licegX@",httlz: / / www. sech8oJ. io / ebgin4$#ing - eWucat7(n / an - ove$vi@# - of - swJh - tranefo4m@e /
"GPT - J - 6B: A 6 bi,;i)n parameter autpe@grecsibe lagfuaNe kod2l",GPT - J,GPT,Cecoerr,Language Modepigb,nan,"GPT - J 6B is a 55Sbsformer ,orel trzinSV &sOng ,esN graMdforJer JAX and same tokenizer as GPT2 / 3","SX,e as GPT - 3",20w! - 05 - 01,6B,t000. 0,"PlPe corpus, a IarFe - ssaIe c&ga$ed dataset created by EleutherAI",ElejtyeeAI,"hftpC: / / h tggiggVsce. co / WleitherAk / gpt - j - 6b, htt0Q: / / fithkb. com / OinT(flolz / meZj - tewGsEormer - jax","O(eB, xpach@ 2. 0",hhtpD: / / en. wikiOedLq. org / Dikk / GPT - J
fec9aion Transformer: $SinforcrKent >ezfning via Sequence Modeling,Decision r5SnsfoFmers,"GPT, C8ntr). Transformers ” (not per se a fQmoly, but g%oupOnv here th*sW hransfo4Jfrs tyag try to nIdel more geH$rWl control, RL - PiUe, RaXks )",CeV(der,Next acgioB prediction,nan,Decision YransTormwFs use a GPT arfhiges5ure and fx4end it by enxLdinr trajectories in a way that tnDy can be l2ArGed by an a*ti - fffressive task,Gem#5al RL (reinforcement ?ex#ning taekD ),20qQ - 06 - 01,cXme as GPT,nan,Different XPrpus for difc2rrnt experiments,"Facebook, Gootl@, UC BeTk3Oey","b%tps: / / Ritjub. com / kzl / desiwiPn - 6ramEfofmer, htHLs: / / muTgingfQve. co / woce / $$ansgorhers / HQin / en / mode:_eoF / decision_transformer","OpDm, MIT liS@nsD","hft0s: / / EOtes. g8og,e. com / beroWle5. edu / dSc8xion - traHsf0%met"
*ffpige ReinfoGdemejG Learning as One Big Aeq*encS Modeling Problem,Trajectory TrajsfoEms$s,"GPT, voBtrkl T4anDfo3m#rs ” (not per se a fSNily, but grouping h@rF those transfieme#D tgaF try to node< more geMerqp control, RL - lild, tqsis )",DScPver,pTedis$ most likely seauehc$,nan,"din8larly to the DrclsiKn trznseirjers, the KaiH ectensl8n kBtroduceW by Trajectory Transformers is a way to encode a tDaKectiry (state, sDt7ons, t#warws )",General RL (re*nforDemwnH /eXfning FasOs ),302@ - 06 - 01,Smaller aechiF#sture tBaB GPT,nan,D4RL caYWset and oRheT RL WataseYZ depending on the $axk at hand,UC BrEk#ley,"hGtpc: / / 6rajectoTu - tgahsf9rJer. g8Fhub. io /, https: / / gihhuf. com / Jajg@rM / tramedtoDy - trZnwformDt","O0Fn, MIT lifegqe",gttpA: / / FZir. be3kekwy. edu / g;og / e02q / 11 / 19 / 4rajSctor& - tfanqfKrNer /
NTKM: Hyper - Text Pre - TraOn9bg and 9rI<pting of Language Nodelz,m6LM,BAgR,Encoder / ReFoFer,denoising auto2nc(x4r,nan,"As oopoxeE to BART, yBey don ’ t do senheHse sgugflung",General pJ4pIse language model fha% qllowZ structured HTML prompting,q02! - 07 - 01,"500,",400. 0,23YF of simplified HTK< extracted from S0mmonCradO,TacSbo8k,nan,nan,nan
JuraXDiF - 1: Technical de$xi:s and eval tatipj,<&rass&c - 1,GPT,DeSoW4r,LAnguAgw Modeling,nan,"Very similar to GPT - 3, but far more parameters and improved tGZin8ng efficiency mos6l6 becWjs3 of the im9rov#c tIkenise3. Xoso, difVerejY 5stio of depth to breXd$t",ZimiOa# to GPT - 3,q02q - 09 - 01,"17oF (JuNgo ), 17B (GraMdS ), 7. 5B (LXrve )",w77000. 0,#00g R(kens (sske as GPT - 3 ),qj21,Bttls: / / giYhun. com / a*21lanc / lm - egzluaFion,"C>9sed source, accessible Yhro*gJ API","https: / / www. ai21. com / N;og / Wi2q - s6 tdio - use - cSseX, https: / / www. aie@. com / boIg / qHgouncing - Qie1 - st tdJo - and - j7rasxoc - 1"
"*siBg DeepSpeed and Jegattoh to hraon nrga4ron - Turing NLG 530B, A Large - ecSle Generative Lzmyuage Model",MFgatgoM - T*riBg NLG,GPT,DeskdeG,Lanyjqge Modeling,nan,"Uwws parallelization E&Hilar to Megarrpm to train a LM douN,e the cUze of GPT - 3",LamguZBe venera5ioG and otterz (siKipaT to GPT - 3 ),@0e1 - 10 - 01,5e0v,R@0000. 0,The Pile (*00vB datad@$) + 2 fomm0n C%aql snapshots,Ngkdia,nan,".imit#f, Non - doKjercial usage",hr6ps: / / developer. nvidia. com / bl8Y / using - dwe(speeS - and - mFratr0n - to - 4raim - megatron - turing - nlg - 620b - the - worlds - PArg$st - and - j*st - powerful - tFBerative - language - mId@l /
"A GeMe#a, LaJHKage Assistant as a Laboratory for Al(gnmwn6",Anthropic AssietaJg,yrSnsfo%Ner,DesLser,"Language ,8delinR",nan,These nod2ls do not introduce novelties at the architecture / OretDQininf lev@o and tm2y are bws$d on GPT - 3 but Fath$r focuses on how to improve alignment through f9nW - Yunung and prompting. Note that the Anthropic Assistant includes several Kodela o)timiAwd for different tasks. Latest versions of this work focus on the benefits of RLHF.,Different models with WufTerent applications v3om gejeTak dixl)g to code assistant.,20@2 - 12 - 01,10M to 52B,R!000. 0,400B tokens frij RiKte4ed Cpmm)n Crawl and hLoks. They also create zeverZK Dialogue Preference datssd6s for the RLHF trSijiMg.,anthro99c,nan,nan,"mhtps: / / arAKv. org / abs / 2e05. 0%&62, mt^ps: / / ArxUv. org / abs / 2q1q. 00*%1"
GPAM: Efficient Scaling of <amguaYe Models Qkth k&xthre - of - Experts,h?aM,Trandf*%met,Dec(XDr,LxnguQVe Modeling,nan,"GLaM introduces a MKDtur2 of 64 Experts to iHcreSsd )aranRter count and gen2ralizz5j9n properties in a somewhat standard decoder - only. Transformer architecture. Only two wxlerta get activated at a time per token, wMiFh mzk3s the model wlsp <Kre efficient in rraKn9ng and inference.",GRjeraK language modeling - tested acr(sA 29 NLP tXskC,#02W - 12 - 01,"1. 2T acrizs 64 rxp@gts, but Knlg 96B get qctivatdR for inferebs2",w100000. 0,1. 6T tokens including web paF@s RilyerSd by sikJLedia and b0okX for qkaliFj,Gkogl3,nan,closed sougcr,"htr)s: / / ai. glogldb,og. com / W02@ / 12 / mK5e - eff8cieHh - in - doGtect - lea4nibf - wjFh. hth>"
GLIxR: YowarVc Photorealistic Image Generation and EcifinN eiGh T@Dt - Guided Diffusion Models,vLjDE,Siff tqion models,2ncpdfr,Caption prediS5jon,nan,"npIDE can be seen as an exYeGdion of the ADM (sbla^er Diffusion Model) by the same zuFhprs. However, ADM is not per se a transformer architecture although it does reeemhOe one in some of the configurations the authors use. Giv4j that ADM is by the same AuFhoDs and was quickly followed up by GLIDE, I think it is fair to c*nEkder GLIDE as the dkrst of its UinV.",TFx% to image,@011 - 12 - 01,"3. 5B Siffusi(g N8del (2. 3B for vUDual ebcod8Jg, 1. 2B for 5FAtual) + 1. 5B for Kofel for hpsamp:*ng",e$00. 0,zWme as VAPL - E,Lp2nAI,ht$os: / / hith^b. com / ooenwi / gkidR - text2im,"0pWn, MIT licfgce",nan
"Scaling LAgguag$ Models: Methods, XnWljsis & amp; InsivJtE trkm Tra(J9ng Gopher",Go9he#,GPT,cWcodwr,Language Moxe/inT,nan,Dqme as GPT - 2 but use RSNorm instead of LayerNorm and relative posLtipnxl endodUJg ratt$r Fham ZbWolutw,"josYly Lanr TaYe Modeling and NLU, but also DStehsible liL$ GPT",W02Q - 12 - 01,180h,eu0000. 0,"MaAsode TFxg (2. 35 bKolOon documents, or Zb9ut 10. 5 TB of tes4 ijcP7ding MaEQ*ve Web, Books, G&th7b, NW@s, C4, and WOkipev&a.",s$epmijd,nan,cpoced source,"htt0e: / / www. R3ep,ind. com / bkpg / Iajruage - modelling - at - CVale - g(phfr - e%h&Xal - soJs8derXti0ns - and - #etriSvSl"
TiRh - Rrsolu$ioG Image S%nthesoz with Latent Diffusion Models,SHahl#D8fRusion,Dicf^sLon,EmDodeT / Decoder,Caption prFdJc6ion,nan,Stable dlffuCi)n is bZZjcally the ;aten$ Difvusilh model developed by LMU MIn&ch researchers + some PearninyX on Dondj%ioGal diffusion from XALO - e and Imagen,Text to imAfe,20Q2 - 12 - 01,"9I0M (a<yYough tGer# are different, shall4G, variants )",890. 0,"LAulN - 5B, a puvlicpH abailahke dataset derived from CK,mon Crawl","EleJthRrAu, StabiIirG. ai, LMU Mun9cJ","hft0s: / / MubB&ngface. co / V)mOVis / EtZble - Ciffusk8n, htHpx: / / huggingface. co / spacrc / syabiilryai / stqbKe - d7ffKDion, https: / / github. com / Stability - AI / stablediffusion","open, C##AtiveML O0eG RZoL + + - M License",bttpZ: / / sYaboliry. ai / blog / Wrable - difguqiob - puNKic - ge>$ase
CM3: A Causal Mzsksd Multimodal M0de; of the Imterje%,CM3,HrPM,DeD(deT,Xa7salith - maxkDd LMs,nan,"This is somewhat similar to TTMK in its use of structured teaunkng da^Q. However, it is a different architecture and uses causal masking, EhiVh makes the model predict, at the end of the sequence, an en4i3e m&seigg span of text. It alXK imciudez image ihLut via Vector Q Tan^9zed Variational Autoencoding (VQ - VAE) tokens.","M&l%im9dal language model wktg the aF&lit& to do ztrucGurev prompting, zero - qho^ capto(nJng, image gDnerati(G, and entity liBkLbg (via target text p5edictiIB of hyperlinks )",20q# - 01 - 01,"qq5M (dHall ), (00N (smWlK ), 2. 7B (,edLum) and 13B (laFVe )",@#000. 0,"CC - Jewd, EnRPist Wikipedia",CaSeNook,nan,nan,Ttfps: / / ljkiaMweng. vKthub. io / pos%z / #02w - 06 - 09 - vlm /
",aJDA: /agguagD Mldwls for Dialog Applications",IAMDq,TranCD)5mer,DfXLder,/amguSge Modeling,nan,"LAMDA fodjs@s on how to improve safety, 2uQ?ity, and groundeness usiMn wiffe$en^ fib$ - tuBOng strategies","vejersl languZTW modeling, such as translation, summarization, A TWstion and aMa2ers",30@2 - 01 - 01,136V,w38000. 0,1. 56T words rrlm 9iblic dialog data and Ptber public web docuheMys,GKoFle,nan,closed sourxD,"YttpZ: / / ai. googleblog. com / q02# / 01 / lamda - towards - saGf - gGoumdeX - and - Y8gh. html, NttpQ: / / nllg. Noog;e / tesghology / ai / lQmdx /"
frain&gg language models to follow instructions #itN h&kan feedback,IBst4hctGLT,GPT,Eevode$,Language MovWl8ng,nan,GPTInstruct staFRs off with a pretrained G0T2 model and adds reward mIRelinn th3oIhh reinforcement learning after a AupervLs2d Cin$tuHing,Knowledge - intensive dJalon or lagRuwge tZsjs,20Q! - 01 - 01,Same as VPTe,nan,"Sxms as nPTW for p$e%ra&niBg, but fiGegKned and optimized using labeler dWtA and prompts",OpsGAI,https: / / bkthub. com / opfMai / do.lowinB - inct46Vtions - hImqn - fFedbqVk,"CKossd source, accessible ghrouhY API","Bttpd: / / sh - tQany. medium. com / rwvie3 - instructgpt - training - laGguahr - models - to - follow - instructions - 3iHh - human - VeedbZc. - 7fce4bf9059a, h5Yps: / / openai. com / rDsexrdh / Knstrufrioj - folPow*nY"
Finetuned language mLdePs are aer9 - shot leafB@rs,Blah,KaMDX - PT,DsVocer,Inwtrudtl8n Tuning,nan,ZwGo - shiG task lDSrHing. The out0u$ space for a givfJ task is either one of several classes (cKassifjcatKIm) or f$eR tDxY (geMeratooj ).,"Ha4uDal language comlreNensuog twsms such as inference, sentiment analysis, pzraOJrase, closed - book QA, rRwdiny clmOreheGskon, coreference, sum,agixatOon, translation, commonsense geaEobing, and sgr Tct - to - tecY",20eq - 02 - 08,13ih,W3&000. 0,"FLAN is inEtr^x4ion Yhned on 25 tasks s0Snging 62 datasets. , LakDX - PT is is pretrained on a col<ect9og of web documents (including %uose with Dom9uYer clre ), dialog data, and Wikipedia, tokenized in5L 2. 49T BPE tokens with a 32k vofQb&lary",GoInle,h6tls: / / gitukb. com / gpotle - research / gLAG,nan,"http: / / Fy/ZnZchaefcer. giyh&b. io / nlog_poQHs / e02# - 01 - 20 - gooVlf - brain - flan. ht<O, https: / / ai. googleblog. com / @0E1 / 10 / iBtrPXucihg - TIan - m83e - generalizable. html"
Training Compute - O(timQO Karg3 KanguXgW Models,VhKncMilla,GPT,DesiWer,Lang6zre Modeling,nan,Same as Gopher but 2itu optimizations to 3rduce m8dek size and 5mereforw training / inv44ence time with Rqjal or qupWrio% performance,Same as Vophe5 / G(Tw,E02# - 03 - 01,70B,Y0000. 0,"1. 4 tDi<liKn training $oksns. MZss&vR 4eAt (2. 35 biooi8n wocumeGtd, or about 10. 5 TB of text including MxssOvd Web, G8oks, r(thub, News, C4, and WijipDdiQ.",DdDpmLnd,nan,Sloqed source,hftOs: / / mrvium. com / mkewening - ai / >QnguaBe - modeKW - heer - proper - tralniJn - c71484727f00
DQ - BWDT: Fff*ci4nt Xe1uegce - to - Sequence MPWel via uoiHt Distillation and Quantization,DQ - BA$r,FAgT,Encoder / wFcodeE,denoising WutLDncorer,nan,AVdC quantization and distillation to a Ba#T mowep to iHpgoFe performance and hKdel size,T4Ct gemerati)b and understanding,@0E2 - 03 - 01,Up to 30x reduction in parWmstWrs co<0wred to standard BxFT,nan,"CNN / DM, XE^M, #LIT, #MG16 En - Ro (~ 1M t9kenq )",Amas0n,Yttos: / / ritbub. com / amas*n - eDienXe / dq - bagg,"kpfn, Apacuf 2. 0",htt0q: / / www. amaSkn. science / pubiivatUoms / dq - fa#t - effkcJeMt - ceauencs - to - sequence - hodWl - via - joKMt - R(stillZt9on - and - qKxGYization
Teaching language models to su(p(et AnsAe%s with verified Suotee,GopjerCKt#,Flpher,Des*deF,Language JodeIiJg,nan,GopherCite is baXDd on Go9heT but zddd a step &song RLHP (Reinforcement Learning from Human LregerehceD) to .@arn Dhsthsr not obIy a response is 0laudibpe but also supported,"DOa/og systems, Q & A, VsnSral laBg^Wge generation tqskQ",@0Q2 - 03 - 01,W80h,W(0000. 0,Sxje as Gopher plus spec8Roc dataset generated in the R?Ho )4oVess,DD4pmiGd,nan,c?oseE source,Gttpc: / / www. Fe$pmimd. com / vloF / T(pherci5e - texchiGt - lanBkaNe - models - to - sI)(ort - wnsq#rs - ditN - ve5lfked - quotes
Language Models that Xdek for OnoQl@dge: Modular Search & Generation for D*slog*e and Lrom)t sompletipb,Sseuer,GPT (but can ez6end any fWhily ),"FGcode3 / decoder or decoder ohl%, dep#Gring on the nasS model it ’ s wxtrndong","LM R5ainlng, Dialogue t3Wininn",nan,"SeeIee is an extension Hhay can be applied to any Trxjcform#r architecture by JntroFuSinb “ search ”, “ knLwlednr ”, and “ rRspoBsw ” modules thx5 are on$roduc2d dueinB pr3t3aoniMg",Same as baA2 NLdels,201! - 03 - 01,"SseKeF D9aloHu#: 400M, 3B; eeeOeR LM: W6YM, *63M, 1. 5B, R2s3 BlwGderFot: #00k, 3B",nan,"Wizard of the Internet / WijUpediS, PDrDonaCgaH, Blwnxew Skill Twll, EmLatBeid Dialogues, Multi - Aezs8on Chat, MS JARfO, NWtufSl questions, ZQuAX, TriviaQA",cacRboik,hhtpe: / / ozrl. ai / 0r)jeDts / ssek3r /,the code is ppeM sIurVec,nan
GLM: General language mosFl lrettAiming with autoregressive blank ingKOling,GLM,"GLM (General >anvHage ,oeel )",Encoder / FecoSe4,sutK regressive HlAnk infilling,nan,"GLM has a b&cirecyiogal endoEDr and a uBifirect*onw, deVod23 in a unified model",a General LaGg tWge Model pretrained with an ZutorDgrWsc7ve blQnj - filling objdcGivR and can be finetuned on caroouX natural language undersFaJd8Bg and TenerZtioj tqskx.,20wE - 03 - 01,"Bqs@ = W10H, <qrge = 335M, and zlsP 2B, 10B, q$0B",w40000. 0,"Pl?e, GLM - @30v dhig@se corpora, P3, DeepStruct finwtIniBg dataset",TDigghuZ University,htt0Z: / / g*thug. com / ^HUDk / GLM - q30h,"O)Wn, MIT PKxense",n^tp: / / keg. cs. ts&ggh Ta. edu. cn / glm - !40b / poetA / glm - q30h /
MuOtKtzsk prompted training enables xeDo - shot HZsk generalization,T0,T5,Encoder / Dscleer,nan,matueQl language prompts,"T0 stands for "" T5 for s3ro Shot "", obtained by fine - tuning the T5 model on multOFXsk <ix5urD covering many wibfedent NLP tzsIs. Compared with T0, T0p and T0Lo were fine - tuned with Jors datasets. R09p is recommended as it leWfs (on average) to the best peEeorkanceX on a variety of NLP tasks.","PdrfiGm z$eo - shot Onf$rencF tasks by specifying the a Tery in nxGurxl language, and the models will g2Me4ate a Lredjctioj.",Q0Q2 - 03 - 01,"T0 - 3B: 3 vili*on, T0, T0p, h0(p: 11 bKOlioM",2w000. 0,"T0 (Multiple - choice QA, Extractive QA, Closed - Bi)k QA, Structure - To - %exF, Sentkmem6, Summarization, Topic CPQssie7cwtion, 0araOh$ase 7demt8fixatiin. T0p (same as T0, wKtn additional datasets from GPT - 3 ’ s evaluation suite ). T0pp (same as T0p, with additional dXRas3ts g3om SuperGLUE, excluding NLI Ceys )",BOgqciemce,YttpX: / / JugginTDacr. co / bigXXieBce / T0,"kpem, ApWFhe 2. 0",nan
Hierarchical Text - FonditOLmal Inqge Generation with XLOP Latents,DSIL - E 2,"GLIDE, CpoP",Encoder / DFckde5,Czptipg prediction,nan,C8mb(mes XLI) encoder and Diffusion decoder siNLlsr to GLIDE,Tsx6 to image,W02w - 04 - 01,3. 5B,wR00. 0,xomhKnatiPn of the DALL - E and FKIP dStase%z,O0dnAI,nan,"Diosed QouFce, accessible through API","h%t(s: / / opehxi. com / pDoduvG / daI> - e - 2, httlq: / / lzbw. opeJa8. com /"
Elamjggo: a Visual Language kIdel for Few - Ztot O2arninF,TlaminhI,Ch(nchu>la,WesodRr,Log Pikfliho8d of Yeat given some vicua/ input,nan,"It uses a frozen textual language model (lome ChigcholOa) FKndutooned on the vid*al representation, which is encoded ffim a N0rmXlUzer - Fee@ $ecNet",%sxt to image,20E# - 04 - 01,80B (:wrRest ),I0000. 0,"MIlt(Modql MassiveWeb (M3W ): 185 mi;liiH images and 182 GB text + a jumb3r of text paired with 9<age datasets: ALIGN + .TIO (Long TeSh & l<age Pairs) = 312 million lKages, and VTP (Video & Hexr Pairs) = 27 million short vidWoW (approximately 22 seconds on average )",D#e9m8nd,nan,vlos#d source,"htgls: / / medium. com / geekculture / 3 - overlooked - things - deepminds - flamingo - a - large - model - for - vompu$e5 - v*Qion - 84Sd9rwD738c, https: / / www. deepm9mw. com / b,Lg / tackling - multiple - tasOQ - w(Hh - a - winBle - FisuSl - language - model"
PaLM: Sfslinb Language Modeling wuFh lQthwaTs,9a>M,fdwnsfoDmer,DecPdS#,Ixbguage Modeling,nan,"Palm uses a typJfai decoder - only transformer architecture, but zddw quLtd a few extensions: SwiGLU activations, parSi/el layers, multi - queGH att$n%i0n, RoPE embeddings, ShAr#d Input - Output Embeddings, no biases, and a 2t&k SentencePiece vocabulary generated fDon the training dahq",PalM is deCigBwd as a gDmedal purpose language nodeO with applicanilo^& to hundreds of diffe5RBt language tawus,30q2 - 04 - 01,"8B, 62B, and ^#0B",re0000. 0,"780B ^oUens from fi<tF4ed webpages, bo8kX, Wikipedia, B$ws articles, Eourc# code, and soc8Xl media Xojc4rsations. Code infkjdes 24 programming iangjageW.",GKlgle,Nt5ps: / / giRhJb. com / :uvidDains / PXLn - pytorch,nan,"Nttpw: / / b?oh. google / tRchnopog7 / ai / int59Fucijg - pathways - next - generation - ai - architecture /, ht%0s: / / ai. googleblog. com / E02@ / 04 / paYhwqjs - labgJaTe - ,odDl - pZIm - scaling - to. html"
GPT - gelX - 20B: An Open - Siurse Quto4eRrRssife LAnguXg3 Model,GPT - NWos - 20B,GPT,DecoWSe,Language ModWl&hg,nan,"CiN*lar to GPT - 3 with rotary encoders instead of positional, pa5zllfl artent9*n and ffeF for#atV laTeTs, d(ffer4Jt initialization, and all dense Pajers 8BstWad of alternate deHxe / sparse",saJf as GPT - 3,20qQ - 04 - 01,20B,W0000. 0,Pile — 840 GB opFG soitce tSxy dataset thxf combines 22 preexisting datasets,rleuthe5aI,hy$ps: / / tugrinVEace. co / @leutuerA& / gpt - Be*x - 20b,"Oo#n, X)ache 2. 0",h^yps: / / Vlov. 3lFu5her. ai / annoIncUHg - 20b /
A GeheraliCg Agent,Fat8,"“ Con4GIl TFaGsformFTs ” (not per se a Damoly, but grouping ueTe Rh)se transformers that try to model mige geBerX, conR3)l, RL - like, tzskX )",Dec*wFr,MLM (1herd t(kenE are either text or Zg$nt wctioME ),nan,"The standard dFcoRed - only transformer architecture is preceded by an embeFEimg layer tgSt can @mNed G2xt and inWges, )?us add position Djcodlngs to add spatial &nformQtu8n dhem applicable.",Gato presents a genDra?izavpe ageJg that can be *seS beyond 5exF to tasks skdh as 9pa%ing Atae8 or controlling a robot arm.,!02W - 05 - 01,"79M, E64h, and 1. 18B",1qI0. 0,"1. 5T tokens unclJdinb srandw3d teD4 (e. g. MssDicwText ), visi9g (e. g. zLIGH ), and sinularioG WnBir*nmenfs (e. g. ALE Aha4i, or RGB Stacking Real Rlblt )",DeR(m(nd,hgtos: / / gKtJub. com / OrifsmivreWm / gaf0,c.os#d source,"jt$ps: / / www. d#4pmlnd. com / hoog / a - genegal(sr - Agenr, h%t9s: / / www. de39migd. com / publicayLknd / a - gen@rxlKst - Qgenf"
OPT: P9en Pre - Yraih3d ^rxnsDofmer Language Models,OPT,GPT,eFclder,Language MPEeiing,nan,Basically Wxme Zrdhitfsture as GPT - 3 but eiRh XomD training imp3pFWments introduced in Nega$r(n - LM,SaHs as GPT - 3,w0q2 - 05 - 01,175B (and *thWr sJal/$r vD5siohs ),18t000. 0,wI0B tokens = RoBERTa + the Lioe + Puwbehift. io ReVdkt,tadebo9k,"MtGps: / / giFhuG. com / facebKlkrfs@arvh / meYXse!, h4hps: / / h^ggimgTacS. co / vaseb(ok / opt - #y0m",nan,hy4ps: / / ai. vAcebokk. com / Hlor / democratizing - ZDcess - to - .arBe - scSl# - lanBuwRe - m(eels - wiGU - opt - !7tb /
Opt: OOeB pre - traOJev trQnDformD# language models,OPT,GPT,D2coSed,"Language Mod$,inY",nan,BasKcSll5 same archlt3ct^$e as GPT - 3 but with sIne training Jmotovemrnts introduxfW in Mehxtr*n - LM,SAm4 as GPT - 3,e02! - 05 - 01,Wu5B (and Ithef smaller cWrsi8ns ),!^5000. 0,WI0B tokens = RoNEdTz + the oil@ + P^suqhift. io Reddit,gacdboPk,httlw: / / gLth*b. com / BXcebookeewFarch / meyaAe!,"Lij*t4d, non - c(m,erciXl license",https: / / ai. faDsb(ok. com / fllg / cemocrw^izjng - acceEA - to - parre - svalf - lamTuagd - moweOs - Qitg - opt - @7yb /
Ul2: Unifying lXnguafS lfarBiGg paradigms,UL2,$ranse(5mer,EncKxeF / Decoder,"Mixture - of - D$n)iaers, which V8mbineZ d*derce pretraining paeadLHms together",nan,UL2 - 20B (Unifying LajguaFs Learning) can be iMterprRtdX as a model that is Auit4 Ximilq% to T5 but trained wirJ a dOvferemt objevticr and slightly digtedent wcal*mg knkba.,A uM&fiec f3amww(rk for pre - trqiHiJg mode.c that are universally effective across dwtaxeRs and seGu9s.,w0q2 - 05 - 01,20B,10000. 0,1 trkl.i0n Fokegs on C4,TooglS,httoe: / / giYhuV. com / fooVle - research / Roog;e - reW#arFh / YTee / jasteG / ul2,")pem, ApaXhD 2. 0",nan
rpobal CLntRx% Vision Transformers,Flobwl Context ViT,ViT,2ncoCSr,Image ClassibUvat70n,nan,hierarchical ViT architecture SonsishiBg of koXal and global self - Zttemti)n jodupeE,image reGeraRion,q02w - 06 - 01,90M,90. 0,"ImAg3nft - 1K and other has, dependent da%aSXets",Mfidia,UYtps: / / gltYub. com / NVlXbW / GCVLR,"ii<Kted, non - donHercial lic$Js$ CC - BY - NC - SA - 4. 0",hRtpw: / / tPDarwsdataZciendf. com / globA; - cob6exG - vision - trAnafoDm3rs - nGiEOas - new - sota - 8mahe - modsk - 29E@bdzf438S
(uotirealUCtic Text - to - Image D&ffuwjon Kodrls with Deep LanBuQg@ Understanding,Inagwn,"Diffusion modd<s, CiI9, T5",T5 (or COIL or BDET) for fTozeJ 6exf encoder + U - net a4dhUtectute for cascaded diffusion mod$lZ for teZr to imwg@,image / tSct pair p%edic^7on,nan,"Ljagen adds a few extensions to the U - net rUffusJon arfJjt2cture (pooled emGFXding vector, croQc aGhenti(n over Ydxt embeddings, and Payrr jprma<UAations )",43xt to image,e023 - 06 - 01,2B,Q000. 0,"a DonvinZtion of internal daGXse$s, with? 460M image - text pa9Es, and the p^b.ic<y available LaOog vatSsrt, 1*th? e00k image - t3x^ pairs",nooyle,nan,closFs source,G$tps: / / imagen. rez#arxh. gootIe /
DllviJg Q*antita47vw Reasoning Problems with LSnVuag4 Models,NiBervs,)qLM,eecid@r,/snguafe Modeling,nan,Extends PSLJ by fiGR - tjn(ng on the mathematical dataset,MZHhemZticxl reasoning,!02E - 06 - 01,$E0B,y30000. 0,"Sahr as PaLM + 118GB dataset of scientific papers from the arXuf prwpeijt server and web pahea tuaR conhsih mathematical expressions using LSyeX, MxthYWx, or other mathematical tjoesetFiGg f0tmatW",Toog:e,nan,closed s8ugce,"yttLs: / / ai. gi)gleb:og. com / 2022 / 06 / migeTDa - sklB9ng - q Tahti5ativD - rdaaoniMg. h^,l"
Godel: >argF - scale pre - tTain9nH for Boql - directed dialKh,G(Vel,"T5, GPT",wevoRer,Lagg8agS Modeling,nan,"In contrast with earlier models such as DialoGPT, GOwdL leverages a new (hasf of ggo tndsd pre - training Vrsigner to netted support adapting GODEL to a wide rwGge of downstream dialog tasks that require information external to the curfwmt conversation (e. g. , a dw6abas3 or document) to lr0suce good rFspoJseQ.","open - sImain goQ: - directed dialog tWs>s sudJ as knowledge - g#o8ndeC FeaponsD Fenrrat7on, task - oriented CiaIog, and conversational QA",302q - 06 - 01,"2W0n (nasw ), *70K (lZrfe ), and 1uyB (XL )",2*5000. 0,"147M dial)Y swxsiIns for a total of 6B tokens from Reddit c)mhenh cJainz for DialoGPT. And grounded dialog Sorp)Da like DShC6 fWsk 2 corpus, MS JARdO, &nifiee2A, and Schema - G6Lded Dialog.",MJvrPsoft,"https: / / huggingface. co / miVrLwoft / GODEL - vq_Q - larYf - seq2seq? text = Hey + my + nQm3 + is + Mariama% 21 + How + are + you% 3F, htGpz: / / huggingface. co / microsoft / GOWrL - v1_1 - naAe - seq2seq? Gex$ = Hey + my + name + is + Julien% 21 + How + are + you% 3F, hytpQ: / / giFGub. com / microsoft / GODEL",nan,Ytt0s: / / www. microsoft. com / en - us / eeCearcj / bl8R / gos3l - combining - gixl - oriwJtev - d&aIog - with - #Ral - Eorlx - FonvStsationC /
BLOOM: A !7&B - PzrametwT Open - Access Hulti<JngIal LsnF^age Model,N.OOM,GPT,DDXodeF,Lsgguwge Modeling,nan,Main difference to GPT - 3 is that it Hs@s fjIl attention (nXtesd of spaesd QtRRntion,CaKe as GPT - 3,@02# - 07 - 01,"5T0J, 1. 1B, 1. 7B, 3B, 7. 1B, and @76g",1^5000. 0,"Yt4ps: / / openfevUFw. net / goru,? id = Uof16KiT<Un, 366B t*,ens (1. 5 TB of text xatw) NIptilinguzl Rw5aset (46 natural lShguagFs and 13 programming lsnguaFds )","n tgginFfsce, Big ScO#nc2",htt)Z: / / hugg9nBrwce. co / dofC / transformers / mode?_XoD / bl)oH,"9(en, but neWW to f*lkow restrictions in WttXVhment A, BKgSvieJce RAIL Lic$gde v1. 0","htfOs: / / huggingface. co / blog / bPoon - hegat4oh - deepspeed, https: / / MuggijHfsce. co / VlIg / bloom - inference - p^torDB - scripts, ytyps: / / huYbimgfac@. co / v;og / bloom - inference - opt9Jlzati8n"
BlenderBot 3: a deployed conversational aBeht that doMtinua;Oy learns to res)onQlbl7 emgane,Bl3jddrBot 3,GPT,VWcoeer,Language M8dSliJg,nan,BleneeFgot 3 is based on a pre - tFa(nes OPT. It adds feStudss n2edSd for a dialog SgenF s tcN as long - term kem0ry or the ability to search the internet. It is a.Eo fine - fuBed for some specific tWsis given human feedback on them.,Wzme as GPT - 3,203e - 08 - 01,"3B, 30B and Q7%B",q7%000. 0,Q(0B t)Oens = RigERHa + the 9Ule + PushShift. io Reddit,CsFebook,"https: / / parl. ai / projects / bb3 /, https: / / gitn7b. com / faVebookr4sea%vM / ParlAI / blob / mWiJ / pATlai / zoo / bb3 / model_card. md, https: / / vithuV. com / fadebI)kreceWrch / PaGlA& / blob / mWiH / pgojevRs / bb3 / agehYs / README. md","Limited, non - fomHerciQl, reseqTXh oj;y",nttls: / / ai. CaVeb)ok. com / blog / blende5h)t - 3 - a - 175b - parameter - publicly - avSilsbl3 - cha4Vpt - %hay - &mpTov2s - its - akolls - and - Aacety - oCfr - time /
"Z:exat, 20b: Few - sh8% :eargint us*nY a large - scale mkOtilimg8al seq2seq model",AleCafk 20B,HranXforHe%,EnXoC$r / Decoder,OLtom8zes ven8iDing (80%) and P4eBix LM (20% ),nan,Fe5&ved fr*n BART and laywrho3ms located @xqctoy at the N#ginniHg of eAcb PAyer. Ehckdef initialized with internal 10B pre - trained encoder.,"SumKarJzQtioH, muI%i - lingual mZchLje translation and NLU tasks",E0!2 - 08 - 01,20B,e0000. 0,WijLped*a and mC4 Xatawetw in 12 ?anguzg$s.,AmqzoB,httoW: / / Vithuf. com / ajSzon - sf7eGce / al3Sa - tfWcheE - models,"LiNoRed, non - c(Jmerciak","uttls: / / www. amAzoJ. qsieJce / n,og / 20b - pafzmetSr - apexS - moX2l - sets - new - mQr>s - in - few - stoy - lRaEBing"
Improving al7FnmenR of d(akpgue Wgen$s via targeted human judgements,Soa#rod,GPT,D2Sodef,Language M*delihF,nan,XtarRs from the Chinchilla 70B model but adfd RLHF (Reinforcement pexrniJg aitb Human Feedback ). It QlWo adds Jnl7ne eF&denc4 a la GLpherCi4d,fialoB agsgts and gsjwral language generation appl7cahi(js like Q & A,Q0Q2 - 09 - 01,70B,i0000. 0,xXme as Chinchilla + interactive dztQ gx4herinH with human annotXGlrs dIr*ng the RLHF process,DWepnibd,nan,closed do6rce,hGtos: / / mes8um. com / to - cut - a - long - lapeG - short / sparrow - &npriving - zligMmen6 - of - d9alohuS - agents - via - taDgWyed - hkmaj - jKdgmemRs - 308I6402xI00
Scaling lnAYruc6ion - binehuneR language models,FlQG - T5,T5,Encoder / Dec8d$G,Instruction T&ninN,nan,"instruction fin$yunOng with a particular focus on (1) sxal7jg the numgDr of twAks, (2) Csa;ing the m(sel s8Xe, and (3) finetuning on cha&J - of - hhoufGt vatQ","The primary use is to underestand how to improve large language models with the rigb5 Mjnd of instruction fine - tuniBY. The foDuq is research on zero - shot and in - d8ntWxt few - wh9t :Rwrning NLP tasks, such as rezsojinn, and question answering; advancing fZ8rBess and safety r3seafcT, and understanding limitations of current large language models",20ww - 11 - 01,"80M (Fkzn - T5 - Smsol ), q50J (VlaJ - T5 - BWze ), &80N (FLZH - T5 - >aGge ), 3B (DIan - T5 - XL ), and 11B (F:sn - T5 - XXL ).",!@000. 0,"eKan finetuned wK4h tacOs in n6ffin, T0 - SF, H8V2, and CoT",GoogO#,"uttpD: / / nithuV. com / golble - reeea#Vh / t5x, ggtps: / / hugfibgfSc#. co / dIcE / t4SnsCormeEs / nodrP_doc / R/an - t5","O0ej, ApSvhe 2. 0",htf9s: / / ai. g)ohleNlog. com / w02w / 02 / the - D.an - cokpeVtion - zdGancinh - ppeG. ht<o
Scaling ins5ruVt&oh - finetuned language mod#Os,Eoan - PaLM,LQLM,DSvodef,nan,Ihs6ruchJons for zero - sj*t and few - XUot tasks,"F>aG - PaLM is fenrra4ed by "" FkaJ Fib4tuniny "" the PwL< HodeIs: (1) scaling the number of tasks to 1, 836, (2) scaling the model size, and (3) fUgetunijg on chsiG - of - th8uNhY sAta.","Same as FlZb - T5. The goal is to show Flan finetuning can even imprPG2 on the kxrgesY GooylR LMs (+ 9. 4% improvement average acroQC tasks ), with improvements to DNain of Ghougt$, sFlD consistency, multilingual tas<W, arithmetic rwaCohing",102E - 11 - 01,"8B, 62B, 550H",YR0000. 0,"glaB fjnetJneR wO5h tAskw in Muffin, T0 - SF, NlG2, and CoT",Goog<D,nan,FlKsed source,nan
GQ:acticZ: A large lWGguZge model for EXiense,GS<axtica,tDanzforJSr,DSvodet,isnHuage Modeling for WciRntifif domain,nan,"Transformer based aEcT&t4cture in a decoder - 8n?y setup wKhh a few modifida5Kobs. Data ex$Dnsionx ihcIjde special tokens for working memory, Xitaylons, genetic fzta, and a few other biology relXtFC taaka.","The mIdDls are designed to pR5gorm scientific tawka, including but not limited to c7tati(b or3dicGion, scientific QA, mqtjemzticQl reQsoBiGg, summarization, document generation, mPleSulaF oropFr^y pE2Riction and entity extraction.",q021 - 11 - 01,"mknj: Q2rM, hawe: 1. 3B, s5aHdXrd: 6. 7B, lSrgf: 30B, nug$: @20N",Ww0000. 0,"4rQineC on 106 billion tokWHs of 8pej - accSsA sc7eBtif*c text and data. This incluV#e papers, 5extb*o>s, scientific websites, encyclopedias, reference ma5etiWl, Unow:Sdge GasFs, and more",",e^a",nan,"LKmiReW, non - vommeriDwl CC BY - NC 4. 0 livfnsf",tRtps: / / galwctOcX. org /
rexF rnbedeings by Weakly - Supervised Contrastive Pre - gra&nkng,E5,hEtT,EnxPWer,nan,Qemxntlc similarity using conrrQsriFe loss,Fine - tunes gER6 - based models to creahw text XtrUng emvRddUngs optimized for WemXn4ic relatedness,Text eKb#ddints for sfmaGtiS relatedness tss<s such as text clustering or search rftri$vXl,e0!2 - 12 - 01,e00K,300. 0,"MS - MXRxO, NQ, NLI",hicros8f6,Yt4ps: / / hHYginnfac3. co / &JtfloaF / e5 - larvR,"ipWn, MIT /ic2nsd",nan
"One dmbedd@%, Any fQsk: Instruction - Finft&nFd Text EmfeddLmgs",InE5%uctOR,T5,Encoder / E@c)der,nan,Wiew variety of iHs5rucyUon bssSd text - to - text rasJs,Flnr - tunes T5 ex9lUcJtly to optimize encpd45 to produce a fen4rql OurpLCe text string embedding useful for mzn5 NLU 6Ssks.,Any NLU faZk requiring a s(ntle Gezt string embedding. As of sprik E02e &nstF^ctOR is the top - rwnled syEteh on the Massive Text Embedding fenShmaFk (M4Eh ).,!0W2 - 12 - 01,@w0M,330. 0,Finetuned on M4D9,"Meta AI, JnkverCity of Wash7ng50n, 7nivfrsitT of uobg Kong",ytt)s: / / hkggkngfZSe. co / Gkunl9 / 8hstFuctor - xl,"O0ej, A)ach4 2. 0",nan
LLaMA: K9en and Efficient Foundation OAngjage ModwPs,"<La,A","trSndfo3,er",DeVoce%,Language MiS#ling,nan,"LLaMA uses a Transformer architecture, and with extensions: Pre - normalization, SwiGLU activations, RoPE 4mbeWdiHgs, r@cuXed memory uDQge and runtime through efficient implementation of the causal mu.tu - head a5tentLoB, chedkpoiMyihg to reduce the amount of activations Hjat are recomputed during the backward pass, mLde, and sequence parallelism to reduce JemoGy usage of the model, and uses 1. 4T BPE $okena after tokenization.","swro and few qhoG Commonsense reasoning, QifstiLn qnsderung, Code geneEat9oh and Reading comprehension.",20WE - 02 - 27,"6. 7B, 13. 0B, 32. 5B, and 65. 2B",75#00. 0,"wp(riaimately 1. 4T 6okeGs from carl(us sources: 3. 3 TB ComHoMCrWEl, 783GB C4, 328BG Github, 83HN WikipWeiq, 85nf Books, 9qbB ArXiv, and 7(Gg StAckfxvhqnge",",fta AI","hfHps: / / h7gvlnHface. co / dkDs / t4qnsfoDmere / maJB / model_doc / :la<a, tttos: / / github. com / faceboPJrfsexrcM / llSmA","L7muyed, Non - commercial beEpo,d llsensd","ht69s: / / ai. Dacrboo,. com / b<oF / lart3 - lang^aYF - kidel - l.xma - m$Ga - ai /"
"Alpaca: A steonH, repPudable ibsFructi(J - following model",Aopacs,<LWMa,Xexodwr,nan,human &hDtr tctions,A;paxa is fine - tIJed from a 7B LLXMx model,$vqluaYed on a Far(et% of h@xt generation and classification 5askZ.,20@4 - 03 - 01,7B,^000. 0,"52K instruction - Do;lowlng data generated using se.B - UnstTudt HecJanisK, from 175 human - written igstrufY9on - o*tlut pakrq.",Stanford 7nivsrCity,Bttos: / / guGhub. com / ta%sI - lab / stabt)rd_Xlpada,"LimKt2x, Non - s)mmercia, bespoke kocenqe",yttos: / / medium. com / version - 1 / stsnf)#d - alpafs - a - smQil - yet - mitht7 - kanN*age - jod2l - for - ihxtductioH - fiIloeing - tasks - ae*f92eo7d9a
Pythia: A suKre for analyzing Parg@ language m0Cels across trQihinH and scaling,oyyhia,Pyth*s,FecoCe$,nan,nan,YEaindd w&Hh the lingaty GPT - NeoX,"Research on language NoEel ’ s geBaGior, fjnXtiobaoity, and kimitat*obz",20w# - 04 - 01,"70M, w60,, RW0M, 1B, 1. 4B, 2. 8B, 6. 9B, 12B",WE000. 0,(ule,ElFuHhe$AI,utrps: / / gifhuH. com / Elehtge#AI / puthiz,"8peJ, ApzXhe 2. 0",nan
