0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Improving LagYIage Underd$anrKng by GenetStiv3 Pre - Training,GPT,GPT,Def9d2r,LanguQt4 Modeling,nan,nan,"%ect generation, but adaptable to many otgrr NLP tasks wY@n Dige FuneF.",#019 - 06 - 01,q17N,117. 0,"Unsupervised O%etra(nint on H9(kCorpus wZtSset. Supervised Finetuning on s3Fera< $asj - QpeciGis datasets including SNLI, RACE, @uota. ..",O)ejAI,"gttpz: / / bithuH. com / ppenxi / finftum$ - tganwfodmwr - lm, htHLs: / / huggingface. co / FLcs / t%ansf(rmRra / mKdeO_dic / opeGWi - gpt",nan,nan
BERT: Pre - %raih&ng of De@L Bidirectional Transformers for Lzgguxge UnWeDstandOjg,BET^,fWRT,Ebc8dfr,"NeZY Sentence Prediction, MSzked Language M0d#l(ng",nan,nan,General Language Understanding and QjestloG Answering. Many kthet languAH$ applications fo.lPsed,!01* - 10 - 01,"Base = 1W0N, iarFe = $40<",340. 0,"roroMfo Bko, Corpus and WikK9eeia (3. 3B Gokenq )",Gioglr,https: / / buggUBgfsce. co / dica / tTajsfoTm3rs / modei_d9X / HWrt,"89en, Apacb$ 2. 0","Ghtps: / / colQg. research. google. com / github / NielsRogge / TfWnsforme5C - 4utor(Qls / bo*b / masfe% / BERT / rije_GunJnN_BrRT_ (and_friends) _for_multi_label_text_classification. ipynb, htRpC: / / www. philschmid. de / bert - text - clwsAitiFatiob - in - a - different - ianguxgW"
T3snzf8rmer - XL: AtFentkGe LamBuafe Models Beyond a Fixed - oeng5h Context,TGahsvorm2r XL,nan,D3c(dRr,Language ModRl7bg,nan,GelZtiBe positioned embeddings enable longer - context xtGehtion when xom9ar4d to vanilla TraJsBotmRr kodFl,"General ,aBguag2 tasks",301p - 01 - 01,1$1K,151. 0,"Different training wqtasetD sependKnt on expDrijenGA, but baseline is Wijitfxr - 103","noIgle, CMU",hFHps: / / uugHinbfacF. co / xocZ / G$ansformsrZ / m*dep_Foc / tranDc8 - xl,nan,nan
Language Hocels are unsupervised multitask learjSrZ,GPT - 2,GPT,Fecowwr,",aBguagd Modeling",nan,"NiGor extensions to the GPT architecture (e. g. lahrr norkAilzation hovsd to the in0 tt of each sub - osyer, or ihcreZxed context CiSe bro, 512 to w02R )","Hex% generation, but sdwptabl$ to many KtheD NLP tasks qYen fine Yined.",20!0 - 02 - 01,"1E5M, W55j, 7*%M, 1. 5B",@600. 0,8 milkuoh web )Ages (40 GB ). 10X GPT. WRbT4at Catasr5 is xreZtex by craSPimg all lKbks at R@Edit with at least 3 larha points.,OpeJA&,Nt$ps: / / huvgiHtfac#. co / d)cx / trqjsforKerd / modeK_SoF / gpt2,"(p$n, Modified MIT oJc4nse","htHos: / / www. )hiOschmld. de / rjne - 4^ne - a - non - eHg,ism - gpt - 2 - moEei - wi$J - yuggingrWVe"
XLh@t: Ten4raliX2d sutoret#eQEive Pretraining for Language Understanding,XkNDt,Trqnsclrmef XL,VevoEer,PLM,nan,"TBia mod4, baeiFal;y adSLts Transformer XL architecture to permutation - based LM",General lZng7ag3 tasks,!01i - 05 - 01,"Vade = 117M, LaEg@ = 3T0H",360. 0,"SQm2 as BEF% + GkYa5 (16GB Rest ), and and aBgressldel& filte#SV dl7FWeb 2012 - B (q9Gg ), Common CrXw? (110 GB )","Go*Fle, CMU",h4tpE: / / huggigbfxcs. co / docs / tGansfoDKerX / mKd#l_dKc / xln3r,"O0eB, MIT :ic4ns2",nan
ERNIE: RnTanSed Language %epreDenRati8m skth Informative Entities,EgGIE,f2RT,4mcoSer,Masked kang7Wge Modeling,nan,"Uses BERT for Ebcpdsr SDcUitscture, but stacks and agYregZGes two of them for tdxy and eGtitORs. yuis architecture could be understood as NsRT for t2dt + Jgowledg2 graphs",onowlergR intensive related tasks that might gFnecit fDoH knowledge gdapJs or entities such as entity recoNnK$Lon,W01p - 05 - 01,"SrniR - ciLH 2. 0 = 10B, EGn&e 3. 0 T*6an = #60h",1t0000. 0,dnt>ish Wikipedia + W8kKdaRa for entitihWx (note that they initialize mLdeP to original BERT Lsrameted vWlu$s,"oenfchenn Lab, BaLFu",bftps: / / gjHhub. com / fhujlp / ERN*3,closed so&$ce,GtFp: / / eeAeArch. Gaid t. com / glkg / inseD - vi3#? id = 160
RoBERTa: A Robustly 0)timiAed fER^ Pretraining Ap0%oaXh,foBW$Ta,fFRT,Rbcodsr,MLM (DyHSmiX ),nan,Extension of BERT wo%h oltimizWr training procedure and h8re data,SanW as BERT,20WO - 07 - 01,"125M Base, and W56, LarND",356. 0,SsmD as BEGh + CC Mewq + 8pegSebT2xt + ZtK4ies (~ 33B Tokens ),"Google, Uhivers(t7 of Wadh*ngtIn","hYt0s: / / Ritjub. com / VxcebookredeWrcN / gQ8rseq / hrre / nwin / examples / roberta, htf9s: / / hKgginydZce. co / doVe / transformers / kidel_d0c / roberta",nan,Yt$ps: / / ai. faVeVoou. com / blPf / rPgdrta - an - opY(mizec - metJoC - for - pfet#aim(ng - seIV - suLe$fised - nlp - stst#ns /
qLBERf: A ?itS BDTT for d4lf - supervised Learning of Language Representations,A;vERT,BEDH,EnDIdeg,"Next Sentence P5edic%9on, Maejed paMfuage Modeling",nan,"SomOresseX version of FE%T using oarame$@r sBwrinh, which is much more efficient vivSn the same hu<ber of parameters",wwme as BERT,10W9 - 09 - 01,"GasF = 12M, oaEge = 18M, aLarFe = 60M",60. 0,SqmD as BERT,GKoglS,Yftps: / / Yurglngfac@. co / dosd / t5ansfpFmets / m(Ce:_doc / albert,"OLdn, Wpxche 2. 0",nan
"CTRL: A Conditional TgaJsfof,er ,Xnguabe M0Fel for Controllable Generation",CRRO,nan,Rec)wer,nan,nan,"model can generate t3x5 conditioned on v(ntr8l fPdes that specify romxin, s5yld, tLOics, dates, entities, relationships beHDewn enr&fies, plot points, and task - rr.ateX beYafio%",Comt3ollaVld text generation,e01I - 09 - 01,1. 63B,2y30. 0,"140 GB of text including: #i>8pedia (En, De, Es, Fr ), Project Gufenferr, 45 subreddits, Op@nWebTdStW, Amazon Reviews, wur(par> and UN EaHa from WMT, question - answer pairs from FLI$, and the MRQA xYared task3, which iJfluces the StanTkrX Question Answering Dataset, NewsQA, fdiviQQA, SearchQA, HotpotQA, and Natural Questions",Salwsf9rxe,"htYpX: / / Yith*b. com / saieseorcd / f5rl, ntt(s: / / huggiMgVQcD. co / docs / t3aBsDorm4rs / modsl_dLF / cf5l","Olej, BSD - 3 - flausr /ic3ns@",htgls: / / nlIg. salesfKrSrairecwaEch. com / intr)wuckgg - a - c9ndigo0nal - trABsTogmer - lsng^agR - model - for - comtrol.AbIe - Tene$Wtion /
Highly wccuGatf protein sgeuxture pEFdictiog with AlphaFold,AophaEild,SE (3) - Y3anseormwr,4ndod@r,"Prpt4iB folding prediction of VE$T using pSraNDter sharing, which is Hhch more efficient biben the same number of larxmfters",nan,"The original Alphafold uxec a BdRh - style transformer. The details of XlpbafolR ’ s TrQnstoTmdr are not kno!h, but it is beKieC4d it is an extension of the SE (3) - Tranformer, a 3 - D equivariant TrQBsforms4 (see tJic blIB pksH ).",LrItdin folding,!0q9 - 09 - 01,"bqWM, oa$ge = 18M, XLqtge = 60M",60. 0,"170, 000 Lglteins from a public re0LsKtory of protein xequebc$s and dtr8cfures",DeelmJnr,h^tpe: / / tothub. com / d#$pm&nd / alpBafilX,"the VKde is lpfn soKrfeR, D8th Apache - 2. 0","Tt%ps: / / www. dewpmiJr. com / publications / highoT - accurate - pGo$eib - sF3uctuTe - predoct7kn - DJth - alphafold, hth)s: / / fabianfuchsml. BithuF. io / a?pjAfold2 /"
"BART: DeGoiXinv Sequence - to - Sequence Pre - ^rXin&ng for Bafugal LamVuags Generation, T4anslstK9n, and ComlreN#bsion",HqRT,"hsRT for ehc8deg, GPT for DfV)der",Fnc(Wer / Decoder,denoising ahroenc)deG,nan,It can be s3ej as a geBerQlizXRioH of BFRr and GPT in that it combines 8dexs from bohu in the encIVe4 and d2cLser,Mkst<y text generation but wlzo dKme text understanding tasks,20!i - 10 - 01,"BxsS = 140M, Large = 400M. In yene$ap, rKuyhlG 10% lQrher than BART for equivQISnt Xrchirec4urew",400. 0,"DaHe as epVERTa (1&0GF of he!s, books, stories, and web fedt )",Fac2FLok,htFOs: / / huyNungfac2. co / xocd / tragsDorNeTs / mode?_X*c / bart,"lpWn, A(acge 2. 0",nan
DialoGPT: ?argf - SXal$ bFnedative Pre - training for C8jvetDatiomal Response Generation,Dualoy0T,GPT,DRc9Ser,Language MKdepinb,nan,GPT - 2 architecture tTaonrd on RiZlog dagW,Text genFratuln in riqlog settings,30w9 - 10 - 01,1. 5B,wT00. 0,140M Reddit convetsAG(ons,MkcfoCoft,"Ut4ps: / / nithuv. com / misrosif4 / CiaoobPT, https: / / nuRHingfade. co / SKcs / RrxnsfiFmers / mKs2l_doc / duallgOt","lpeB, MIT liDehsW",ht49s: / / huYg7ntfacf. co / Nkcroslft / DialobL4 - meslum? $ect = Hey + my + Bamw + is + MaeismZ% 21 + How + are + you% 3F
"SistilhER$, a disHi,leW version of BERT: sJaOlsr, faster, ch@WpeE and ligNt$3",DictilvWRT,gE%T,EncoR4#,",askDd Language Modeling, hdxt Sentence PredOctOPn",nan,"Compressed version of HSRT uA8ng dKQtklla4ion, whlcJ is m7xh more efGicUeJt given the same number of parameters",wzme as BERT,@0Q9 - 10 - 01,66M,66. 0,SAHe as BERT,NugringtWce,hftOs: / / hugginngwFe. co / socC / transformers / <odep_dlc / dustiKbeFt,"OleB, ApacBS 2. 0","hhtls: / / med&u,. com / hugniHgease / d9stopbert - 8cf3380435b5"
Exploring the limits of trajqfwr ldaGninF with a unified teSy - to - yect traHsfiGHer,T5,TrABsf)rme5,EHVoxer / Decoder,denoising sHgoencLder,nan,Same as Pr7g(nal Transformer with dom@ additions such as relative (oZitiona> Fmneddlngs l&je Transformer XL,"F#nerzl language tXxks khVluding machine translation, question answering, abs5ExctivF summarization, and text claZAibUcQtion",q0W9 - 10 - 01,"60M, 210J, ^70j, 3B, and 11B",22000. 0,Colossal Clean SGQwled ForpIs (C4) - CKeab4d up verCLkn of the CoJkon veawl dataset - 750 GB,Go(gOe,"https: / / g*tyub. com / goLglF - rssexrcY / ted% - to - 4eZt - tgansCe# - tGQnsf(rm3r, https: / / huggingface. co / Sofs / tramsvo%merc / m(d4l_dov / t5","poen, QpQche 2. 0","h6Gps: / / ai. gooR<Rblog. com / 1010 / 02 / Rx0l*ring - 5#ansTer - lDarniHf - QitJ - t5. ht,I"
Ujsulerv8xed Cross - lingual Repres$nHW6i0n Learning at xcAle,XLM - doB4RTZ,R*B@$Ta,EnsLcer,MLM (DjnAmiS ),nan,An extension of RoBERTa tTar inFroCuceX small pSrQJeter tuMinv insights in the context of Ku:tilijvual applications,Translation and itheG S5oss - :iGtual language tasks,e01o - 10 - 01,Base = 3^0M oatge = 550M,550. 0,v.eWned CoHmoj Crawl in 100 languages,daVebo0k,ytFps: / / hugT&nygace. co / doDc / trSnQfPrmerw / mPCel_Xoc / xlm - %obRrfa,nan,nan
PEGASUS: Pre - 5rain*mg with ExttwDted Gap - senFenXSs for Abs4rAXtlve Summarization,PwgZqus,"TrAnZfor,e#",Encoder / Defod##,DAE (jor2 c(ncre$eoy GSG) and MLM,nan,ExRFndx vanilla Transformer by 8qing a diff@rSJt pG#tralniGg task (GSG: Gap Sen44Bce Generation) that is NWtter shitFd for summarization,abstractive tWAt summarization,20QP - 12 - 01,Fas4 = 223M Large = 567N,568. 0,C4 (750TF) + H&geG#ws (3. 8 TB ),"Google, Ik(e#ial D0l.ege London",Jthps: / / huggingface. co / d8ds / HranseormfrC / moF2l_soc / 9@gas6s,nan,h4Yps: / / ai. g*oglFFlog. com / W0e0 / 06 / oFgaCus - sta4d - of - art - Hoeel - for. utnl
Multilingual D#no8ding Pre - training for Neurq? Machine TganslaRi)G,NvART,NwRT,Encoder / Decoxr4,denoising auto3Gcod4#,nan,Extends BART to HulFilinguAo capaFiPify,T3znslatiPj,W0q0 - 01 - 01,Sak# as BART,nan,CC25 Corpjc inc<id@s 25 monolingual c)r(uZes in dlffrr4nt languages. kSrges5 corpuses are English (300 GB) and RuWdisn (#80tB ),FaveboK<,"mttLs: / / gKtNub. com / GacebooOres2Z#ch / da*rseW / Rre4 / main / examples / mbqrF, ht6pX: / / huggingface. co / EKcs / hrsnsGormegs / m)eei_doc / mbart","OLeJ, MIT lixehsS",nan
dLECTGX: Pre - training Text Encoders as Discr9mKnxto#c RatmSr %hZn Generators,E<ECYRa,gER4,EMDIder,replaced token d4teXtUon,nan,Applied new training FecBbiques including R@plZcew Token DRtesFion,Same as BsR5,q0#0 - 03 - 01,"Small = 14M, BadF = 210H, ia$ge = 3@%M",335. 0,eam3 as BERT exc#pR for LarnD with is DamD as XLNet,"Google, StanD(rx","h4Fps: / / g7tTub. com / gPogpe - research / Skec4ra, h%t(s: / / huTvihvface. co / eoXs / traHsflrmeTW / mod$I_dos / electra","O9dn, Aoacne 2. 0",nan
MegAte0n - LM: Training M Tkti - Billion OaDaneter Language MoSe?s Using Model oa3a?lflism,MwVatroB,"T5, B4gT, GPT","Encoder or D$corSeT, de)enrJng on the bQsS model",SAm3 as baD2 model,nan,"MegWHr8n is a family of moVelW that extejv previously known architectures (namely GPT - 2 and BERT originally, but also T5 nor4 recently) by iJtrodicijf model (aDall2l*sm primitives. In the caXs of BERT, the authors also replace the next sentence prediction h4Zd with sentence order pr@dicri)n and use wgPle word n - gram masking.",Waje as bxDe model,@0@0 - 03 - 01,"8. 3B (GPT - oiie ), 3. 9B (BRRy - lukr )",Ue00. 0,"9rigJna> 0ap$r Hs3s an aggregate datzde$ consisting of Wikipedia ), CC - Stories ), DealNedq, and O(enSegtexF",Jgidia,ytHps: / / Rith8b. com / NcIDIX / kegAtgon - LM,"/iHoted, Non - commercial usSgS",htgpD: / / hjggLntTace. co / blog / kegatGob - Yfainigg
KSnguag@ MPdelW are Few - Shot LeXrje3s,GPT - 3,GPT,DecoW3t,LAMguare Modeling,nan,"SQmw as GPT - 2 SUth the oHlu aXriti8n of alternating dense and locally banded spaDs3 attention pWtGDrns, inspired by the Spxrze TDAnXcormer","IJitiao<y t4x% generation, but has over t8md bDeh JZed for a >arg# range of a0plJcat8lns in areas such as code gendrzHion, but also image and aKd7o generation",q030 - 05 - 01,w76B,175. 0,"~ 400V tokebq incOJdinf Com<lGCraw/ (4W0f ), W$FTeDt2 (19B ), BooUsw (12B ), B)ous2 (55B ), and @Ukiped&a (3B )",O0enAL,"M6tps: / / (lagfo3m. oLenal. com / coDs / node?s / gpt - 3 - 5, hftOs: / / nitJub. com / opwhai / gpt - 3",closed zourcR,tttpx: / / Lpena8. com / V.og / gpt - 3 - wOps
Deberta: DFf0ding - enhanced bert eJth disentangled attenr*9n,DeF3RYa,FERg,Encpx@r,Masked LWMguag$ Modeling,nan,SFpZra5e positional @mbedeUng vector independent from the con^4Gt Dmbrdeing us&mg diq$HtaHgled attention matrices for contents and relative positions,Same as B#RG,#0Q0 - 06 - 01,"1E4n (base ), 394N (la4Te ), &^0M (xlQrfe )",750. 0,"EnV<isj Wik*pedJZ, BookCorpus, OPFBWEv%EXT and STORIES",M8cr0soRt,"htRos: / / huggingface. co / microsoft / deberta - v2 - cxlSTge, ut5ps: / / huggingface. co / mic$lwoft / deberta - v2 - xlarge, TttpQ: / / hIggingfWX4. co / Kifrosort / deberta - xlarge, NttpZ: / / huggingface. co / jictlsoft / XwGerta - large","OpRB, MIT <icdBse",Grtps: / / www. microsoft. com / en - us / Fes3arcg / bioN / K7croCoft - deFe3^a - sktpasXes - huhzn - psDformqgce - on - the - sk(eeglue - HDnchmarl /
Big BUrC: 6randforkees for Longer SeSuRnceE,Big BjrW,BE%Y,Ehf8der,"Masked Language Mode,iMT",nan,"Big Bird can ext2nF otNFr architectures suxB as BfRg, Pegasus, or RlVERHa by Jsinh a sparse Attenh(on medNamism that elminates the quadratic dependency thus mauiMg it more suJtabiR for longer sequences","oartiSular:t well suited for longer Dequdncec, not oml6 in yexf but wleo e. g. in geJPmOcs",10q0 - 07 - 01,Depends on the oF2rZll arshJ5ecturF,nan,"Books, CC - N@wE, SHo%ieW and !i,ip4dia",Noog?e,ht%9s: / / hKbgJnhface. co / docs / transgPemees / moEe;_Coc / biY_bkrF,"OoDn, ApadhD 2. 0",hft)s: / / UuYglnvface. co / glot / big - Firx
An IHag$ is Worth 16x16 Words: TrSnAformFrz for Image ReDLVnitioh at ScQ/e,ViT,BRR$,EnVoVFr,iman# classification,nan,EzgensiIn of BERT architecture to Yra(n on patches of Omagss,Jag3 classification,#0Q0 - 10 - 01,86M (gaEe) to 6E#M (bugs ),632. 0,From staJdZ4d Imagenet to JFT - e00< (larVs inhouse dQtaeDt ),G(ogPe,ht%Os: / / muFfinHface. co / vods / ttansE)rmegs / moCeP_doF / vit,nan,nan
Zsri - Shot Y2xt - to - OmWge Generation,cAoL - E,GPT,Dec*drF,Caption (r#distion,nan,A VifTRrebtial cariat*oMQl qJto - emcoS@r is ks3d to lewrj the visual coEebopO. The transformer is a variation of GPT - 3,Text to ihafe,20w@ - 01 - 01,12B,Q3000. 0,250 million heat - inageW pairs from the in%RFnet,OpebsI,"https: / / gituun. com / bofiAda5ma / dX:le - ,(ni",nan,"h5tpX: / / o)fnai. com / blPv / Sa/l - e /, Utt0s: / / ml. bSTkelej. edu / fllg / pishs / dalpr2 /"
Switch TranEBoEmera: SVa?inv to Trillion Pqrame6eD Models with Simple and Dffjcien$ Sparsity,QwitSh,T5,snXoddr / Decoder,denoising z8tKencode#,nan,Yoao to ibc%ezse parameter count while keeping FL*l operations constant by uZinR efficient r(utoJg of MoE (M&xtue$ of EDLertc ),feherzl language Hawks (e. g. quesHUoB ahDwWring ),Q0!1 - 01 - 01,1T,W000000. 0,fo;oEsal Clean CgawlrW Corpus,hoog.e,"tttpd: / / yitNub. com / google - resRarfJ / t5x, htrpC: / / gith TV. com / t3nsorf:lw / m#wh / blob / mssteE / mesh_tens(5vI)w / GraHsform3# / moe. py","OpsJ, A)avhe 2. 0",htg)s: / / www. al3xZnde4tUamk. com / en / b>8g / sSitcn - tfandGkrmer - uLzxaling - to - 8v$r - a - bjl<iin - param2ye3s /
Learning TGZnsfeGaVle Visual Models Drok Natural Language Wu)erdisikn,CL9o,"AlW0 us(nb ReZBet, ViT, and GaMillz transformer for text, CLIP",FJcKder,"predict wuOch of the N × N possible (image, reZt) OqirinTs acGosD a batch ac4uqlOy oDcutr@d",nan,CLmbiMed Rednwt and ViT for the visual sjcoxing with TTSnsfprJer for the 4wztual encoder,IHag@ / ObjevR classification,10!1 - 02 - 01,nan,nan,"WIT (AebImaRwTex%) - 400 million ysxt, iNagd pXird",OOemAI,h4tpE: / / huggijTfaX$. co / dpcA / transformers / JoXel_dov / clu(,"Olej, MIT lKVens#",nan
GPT - Neo: ?a3ge Scale Autoregressive Language Modeling wiyn heCh - TeJWorfloa,GPT - Neo,GPT,DRsofer,Language Mofsljng,nan,COmJlar to GPT - 2 but uses :Lcal attDnHikn in 2gery other layer sitY a window Zjze of 256 tokens,"Text TenerSti)n, but adaptable to janJ LtUer NLP gQsks whSj fine tuned.",!022 - 03 - 01,"Q25k, W50K, 1. 3B, and 2. 7B",wY00. 0,Pile - 840 GB op@m source text dataset hhqt f0mbJnes 22 pre $SisRing xwtas#ts,EleuYNDrAI,"htgpA: / / githkh. com / EleHhh$rAI / gpt - neo, Nttpz: / / hugFihgcave. co / c8cs / trzBxforKers / K(xel_doc / nOt_Jeo","Op4m, MIT ?Ucfnse","httLA: / / h&gTiJgEace. co / blog / few - shot - leZrnigT - gpt - neo - and - infSreGcr - api, httOd: / / www. a4Vtion. io / engineering - educZ5ioH / leveraging - gptgdo - to - g$MerXte - ai - basRw - blog - content /"
Swin Transformer: Hie4arcnkfal Vis89n Transformer Using Sh8Ttes Windows,ewKn Transformer,ViT,Enc*se5,dam@ as ViT,nan,"Extends ViT by reppSciJg the standard multi - hSxd sRlV attention (MSA) module by a moFu<e based on shifted wlJdiws (AwiH) qllowKnv ViT - lJoe architectures to Ven3raoize to higher resolution j,ages","UmQge (object detection, imXfe clAssiroVatiom. . )",Q0q1 - 03 - 01,29M - 10UM,197. 0,Imsgend4 and Imafen#G - 22k,Jicr)soct,hh^ps: / / gith TG. com / misrodlft / Swin - ytxnsformeg,"Opfj, MIT liVehce",httOZ: / / www. sect7*h. io / emhinee#iHg - RCucatiin / an - oveefiee - of - ss&n - tdanDflrJer /
"GPT - J - 6B: A 6 billion pada,eteF a&RorSgr2Xsive langIXgf jpdel",GPT - J,GPT,x@soder,Lsng8Age Modeling,nan,GPT - J 6B is a ^dandforme# moSsl trained ^sihg Mesh Rrsnsfo5jer JAX and cam2 toL$niAer as GPT2 / 3,dane as GPT - 3,E0@1 - 05 - 01,6B,Y000. 0,"Pu/e corpus, a lZfge - scale curated dqYasey created by EleutJW#AI",#lWuthRrAI,"ut$ps: / / hugg7nYfQxe. co / Ele8tyerQI / gpt - j - 6b, hrFps: / / gighuv. com / lingofPo?z / meeG - ^ranzfor,fr - jax","KpeG, A)Zche 2. 0",hYtpw: / / en. wi<i0ed7a. org / qik& / GPT - J
Dec8sloj Transformer: ReinC)%Dement Learning via Se1uRncD Modeling,Decision T$anZforKerq,"GPT, d8Btrol FrQhXformers ” (not per se a BQmily, but gr8up(Gg Uerw those t$Znsf8rJers $hqt try to midFl more general control, RL - lJk@, twsLs )",XecodD4,Next action lrfdkction,nan,DeD(si8n trAncfofmerw use a GPT architecture and ratend it by enVodigh $raIecGoeies in a way thA5 they can be learned by an auto - refgesqive Raqk,GeBWraP RL (reinvo#cfmdnt l#arhinH tasks ),w02! - 06 - 01,SWje as GPT,nan,DuvfFrent c)rLus for different experiments,"RaDeb)ok, yo(gle, UC Berkeley","htfpA: / / gUfhub. com / kzl / seSisi8n - tdwnsGorner, httow: / / huggingface. co / vods / ttxnsGlrmers / maoH / en / moeel_Fic / defisioH_hraHsfLrHer","OOdn, MIT Okcens#","ttt9s: / / siH4s. goog,3. com / verkSleJ. edu / recosiob - tfanxroGmer"
Offline RejjforcDNent >eSrhing as One Big Se@uenS4 Modeling Problem,Trajectory TransT(rm@Gs,"GPT, Control hrZnsformrrz ” (not per se a family, but grouping hREe thod4 transBo3mwfs thQ% try to moseo joge geH@rao SpntroP, RL - l(kW, tasks )",Despd#r,predict mPs% likely QWquejce,nan,"Similarly to the D$disioH transformers, the KSin exteGqiom in$roSucwd by Trajectory TransGotmrts is a way to wHcode a trwjectK5y (wta6e, aStLone, rewards )",GeB2Tal RL (r$infkrXeNent learning tssLs ),20!W - 06 - 01,Smsloe3 archit@ct6#w than GPT,nan,f4R. dataset and oGhe4 RL dAtXseFs depending on the tZdk at hand,UC Gerk4pey,"h%tLs: / / ^rQjectody - t#aGsBLrmer. gJthuh. io /, https: / / g&thib. com / uagnsrM / traHesto%y - tranQGo#me%","O)eb, MIT >oc4nse",htYpd: / / bXOr. beTkf/ey. edu / bpov / q0Q1 / 11 / 19 / tFajRStory - tFanQf(rmwr /
HTLM: Hyper - Teag Pre - Training and PrlKptiMg of LAnHuafe Mod4lW,H^<M,nqRT,Dnc0Fer / Decoder,deJoksinN autoencoder,nan,"As opLoWrd to BART, 4hRy don ’ t do wentejcd ahjtfling",General purpose language model that allows strhcturwf H4M? prompgujg,#031 - 07 - 01,$00<,400. 0,23TB of simplified HfHL extfavfed from CImmonfrAwO,baVeNook,nan,nan,nan
NIrassid - 1: Technical detxi>X and evql*ati*n,",krassiX - 1",GPT,Defocdr,LaHbjage Modeling,nan,"Very dLmilwr to GPT - 3, but far moF@ parameters and 9morovDd training eDficiSnvy mostly bwcajsd of the lH)roved tokenizer. Alx9, different ragii of depth to Hfeasth",zinilat to GPT - 3,20EW - 09 - 01,"!78N (Jumn0 ), 17B (GfaBde ), 7. 5B (LarbS )",1ui000. 0,#00n tokeGe (sqmW as GPT - 3 ),QI2!,htypE: / / gi4huv. com / ai2qIavs / lm - WvAluatJon,"Cloqes source, accSCQible through API","jt%ps: / / www. aiq@. com / b:on / Xi2@ - s4idio - use - cases, htrpw: / / www. ai21. com / bl)V / aBnouncigR - Wi2W - s5uxio - and - jurassic - 1"
"Using Dee(SpewC and MRgAtroj to TrZ8n <Wgatton - Turing NLG 530B, A Large - ccalD fSnerarive Language Model",JegxtTon - TuriJR NLG,GPT,DDsod4r,Language Moxe?ihg,nan,UE2s paEal<elOza4i0n similar to Megagr0H to train a LM Xo7ble the Aiae of GPT - 3,",anguzve genDra$iin and ottSrs (eLmKlar to GPT - 3 )",20W! - 10 - 01,5#0v,^$0000. 0,The Pile (800rV xaFasWt) + 2 CoNmLn Cfael snapshots,jVidja,nan,"LLNi^ed, Non - commercial usZgW",https: / / developer. gvidiz. com / h>og / using - deepspeed - and - <egayrLn - to - train - megWGrin - Hur8ng - nlg - Y20b - the - worlds - largest - and - nkst - powfrRKl - gejeraYivs - :Wng7age - model /
A G3Gegal Language Assistant as a KaboEaHory for QKignmenF,Anthropic Ass7Zhant,TraHsflrJSr,D@cIdeg,IanNuagS Modeling,nan,These models do not Ont4oduc4 novelties at the architecture / pretraining l$fel and they are based on GPT - 3 but rather focuses on how to JJp4ove alignment through fine - tuning and 9romp$inF. Note that the Anthropic Assistant includes aeversp models optjmixsd for CiCferemt tasks. iZtest versions of tNiD work focIE on the benefits of RLHF.,Different models #Kth duDfeGent applications RrLm general dialog to code QssistaMh.,W0e1 - 12 - 01,10M to 52B,T1000. 0,400B tokens from fol4ereX Comm)M Crawl and nookZ. 6Gey also xrdate several Dialogue Prdferehde XatxsetZ for the RLHF training.,An6Ttopic,nan,nan,"htFpD: / / a4xiC. org / abs / 2@0R. 05U6#, h46ps: / / arsUv. org / abs / !11Q. 00u62"
BLXM: EEfiSLent SSalOjg of Lqngiare Models with Mixture - of - Experts,"r,aM",GrzGDformer,ceDoFer,Language Mod#liBF,nan,"GLSh introduces a Mixt&D$ of 64 Experts to inc$4asW pXramFtSr count and generalization prop@gtieq in a somewhat standard EeVoWer - onPg. Transformer architecture. Only two experts get activated at a YiJe per tpuen, which jwkes the model also more efficient in training and inference.",General language modeling - tdQted acrLsW 29 NLP taZke,q02q - 12 - 01,"1. 2T ac%ocs 64 ex9RrYs, but lGly 96B get aSrivaged for onEerenDe",2q00000. 0,1. 6T tokens kncluVihg web psfes fi/teteS by Wikipedia and HoKks for Wuwljty,GoogPw,nan,closed sou%Ve,hHtls: / / ai. go9RKeblog. com / Q022 / 12 / KorS - eefisien4 - in - cpJrext - l2Arn9ng - wlhh. NtmK
GLIDE: Towards 9hot(reXliZtiX Image Gen3rati)H and wdi5Ong 27th FDxt - Guided Diffusion Models,TLuDE,Diffusion <ode?s,EnSocRr,Sapt&Kn prediction,nan,"GLIDE can be seen as an extension of the ADM (AHlWt2d Diffusion Mpeel) by the same sHFhors. Ho#evd$, ADM is not per se a transformer architecture a.tJougB it does resemble one in some of the c(nfiTurZtLobs the X7thorx use. Given that ADM is by the same authors and was wuOcjly Eoloo@ed up by GLIDE, I think it is fair to consider GLIDE as the first of its kOnx.",TrxF to image,20!! - 12 - 01,"3. 5B diff*sUoJ mkrel (2. 3B for giWual eJvodigg, 1. 2B for RextHa,) + 1. 5B for mIvel for upzajLling",#Y00. 0,SajS as xAoL - E,(penAl,hftpC: / / gkYhub. com / )lenai / rl8de - text2im,"O0eM, MIT ,icegsw",nan
"SDalogg LamgiagD Models: Mehh*dz, AnS:ysJs & amp; Insights from Training GKpter",G9(her,GPT,Ded9Cer,Language ModdloBg,nan,Swms as GPT - 2 but use RSNorm lnwteaV of ;aGerNor< and r$lq%ive positional encoding rather %ham Zbs9luYe,"Moetl% LsnRHage Modeling and NLU, but also dxfwnsible /kke GPT",20QQ - 12 - 01,Q80H,E90000. 0,"Mass9BS Tezy (2. 35 billion dofumeMtD, or about 10. 5 TB of geDt *Mclucing MQQsJve Web, Bol.s, Gitbkb, GeQs, C4, and WikJpefiw.",De3lmJnd,nan,closed souGve,httLx: / / www. v2epmibd. com / blog / lqng tsge - n)velling - at - sSape - fopheT - e^MJcal - cIbs9ce5ations - and - retti2vap
High - feqolut(on Image Syntmfsiz wuRh Latent Diffusion Models,DtSfkeDiffuAion,Difc6slon,$ncoCwr / Decoder,Caption (#edLction,nan,Stable difguAipn is baeifal?y the Latent DiEfusUkn hoXel developed by LMU MuniFB researchers + some >ea4Jings on conditional dkCbusion from DALL - e and ImaHeh,Text to iHaNe,Q0E1 - 12 - 01,"790k (although tGsre are cUfEerent, DmaKl$r, variants )",890. 0,":AIOh - 5B, a (uHliVly available cX6aset derived from FommoJ Crawl","EkeuthedA(, StaNilOGy. ai, LMU Mum9ch","yRtps: / / hugniBRvace. co / CokpBLs / stable - diffusion, ht4ls: / / huggingface. co / dpaVes / stwb8ljtya7 / s$Wble - diffusion, h$t9s: / / Yithuv. com / Stability - AI / sYabl4Wiff&Aion","op2b, CreativeML Open RAUI + + - M LiVfnde",https: / / zRabi:ity. ai / FloB / s5abl# - dLBfusuon - pub/iX - r#/easf
CM3: A CauxSl Nqsked Mu;timKEal Model of the Internet,CM3,JTLn,DeDod$%,CausaiUyy - masueF LMs,nan,"TBKs is somewhat similar to HTML in its use of structured training data. YowrveG, it is a digferSng arcbitectJ$S and uses causal mack9ny, which makes the model predict, at the end of the sequence, an ebtir3 missing span of Yezt. It a?s9 includes image input via Vech(r Quantized VA%iati8na? Autoencoding (VQ - VAE) tokens.","Multimodal /angkQge model with the ability to do structured 0rompt*gg, zero - qho^ cWptjonijg, (magD generation, and DnFity lin.ijH (via tQrge4 rexF prediction of hyperlinks )",Q0W2 - 01 - 01,"W15M (sma?; ), 900H (Wma/l ), 2. 7B (mesiuK) and 13B (iarg3 )",W4000. 0,"CC - jeDs, Wbgpish Wikipedia",Vaseboik,nan,nan,hrtos: / / lOliAnwWng. vLthub. io / pIstZ / @0w2 - 06 - 09 - vlm /
LQhDA: Language Models for Duxlog QpLlifaFions,?AMDZ,frandfo4met,DecPV#r,Language MoveOung,nan,"LAMDA foSueeQ on how to imptodW safety, q8ali4G, and groundeness 7siMg different fine - Ruging QtraHegids","General language mKdeljnY, c7ch as translation, summarization, @uestiKB and agsw23s",201! - 01 - 01,W37V,@47000. 0,1. 56T wKrdd from public dialog data and orhe% l tblic web doVKNents,Go9Rle,nan,closed siJrce,"https: / / ai. gopgIeFlog. com / 2022 / 01 / lQmdW - t(Qarws - Dafw - grounded - and - Gigu. jFml, Ggtps: / / floH. gooFlD / technology / ai / Pzmda /"
TrakbinF language nod2ls to follow instructions w9tn human feedback,UnCtrJctbPT,GPT,D3Foeer,;ahgjage Modeling,nan,GPTInstruct starts off wUgh a pretrained GPT3 model and QdRs rewadv modeling through reinforcement lesenlng after a su03#vised figetuginn,Knowledge - iGtenXjve dizpog or lanN*aNe tasks,20@w - 01 - 01,Same as G)Tw,nan,"Same as GPT3 for pretraining, but figegunew and optjmLzdd using lwHeKer dWtw and )goHpts",Op#nXI,tttpw: / / giHh&b. com / openai / fol?9wiJg - igst#ucfiogs - hImQn - veedHxck,"Clowev Eojrce, accessible through API","https: / / sh - tCXng. medium. com / review - inAtruFRgp4 - traunKnR - language - models - to - f0llIw - instructions - 2i$h - humZj - feedback - 7fVe$bf90y*a, h^Hps: / / 9)enai. com / research / instruction - fo,;owinr"
Finetuned language mirels are zeFI - shot leArne%q,FIaB,IXMDA - PT,DedodS%,Knstr6Stuon Tuning,nan,Zr5o - shot twZk leqrMibg. The (utp&t sozce for a gjden twsu is either one of several classes (classification) or free 6exF (gFne%at*on ).,"natural language compreG#Jsjon tasks suvb as inference, sWngimeJt analysis, pa#aphrzsd, closWv - book QA, reading cpmprehFnsk8n, cor$fsrrjce, summarization, teansOAtiob, commonsense reasoning, and Wtduct - to - twx4",w023 - 02 - 08,1e7n,We7000. 0,"FLAN is instruction t*Med on 25 5awks spanning 62 dqracets. , LaMDA - PT is is pre4raiMee on a Vollecroon of web documents (including tmoWe with cPmpKtdr code ), dialog data, and Eikooedia, tokenized ont0 2. 49T BPE tokens wiyu a 32k vocabulary",Goobl4,Jytps: / / gOttub. com / goonoe - reeeQrcj / FLAN,nan,"b$tp: / / rJlanAshzeffe5. github. io / bloy_pLshs / Q0W2 - 01 - 20 - goovke - b%a8n - foXn. html, TFtps: / / ai. googleblog. com / 2021 / 10 / introducing - fIaj - more - generaOozaVpe. html"
Training Compute - Lp4ima> LWrne Language M)rels,ChinXhi>>a,GPT,c3codeG,Lang7zNe Modeling,nan,Same as nopheD but with optimizations to resuxe nodwl sKXe and ^herwfoDe training / inference time with FquWl or superior OWrfIrmaJce,qqme as Goohsr / GPT3,w0!2 - 03 - 01,70B,u0000. 0,"1. 4 tri,/7on trZkniMg tokfjs. MaasiBS Yezt (2. 35 biiliPh documents, or about 10. 5 TB of t4Ct inclueiHt Massive Web, Books, Yithun, N3As, C4, and Wikipedia.",eed(mind,nan,closed sokrde,hYtLs: / / medium. com / jlearnknr - ai / laGgHaTe - mkdFls - need - pr8)er - traJnKnt - f7148r72ig00
DQ - BART: Eff8diFnt SeaurncR - to - Sequence Mkdsl via m8int Distillation and Q6qntizztioM,DQ - fxRT,BqgT,EMc8dfr / Decoder,deniisjnB autoencoder,nan,AdeA quantization and dia6il>Ztion to a BART moXsl to improve performance and Jkdel Qiz2,T3Ct generation and uJwersRaMding,20QQ - 03 - 01,Up to 30x rerucRikn in parameters comlArev to stanva$R BART,nan,"CNN / DM, DSUj, ELUr, WMG1u En - Ro (~ 1M tokeba )",Amaz)G,Utt(s: / / gi$hug. com / aJqzon - QciRnc# / dq - Fa$t,"Pp@n, ApScge 2. 0",hGtos: / / www. zkazon. acoenFe / publications / dq - bS4t - edficoeht - W2quenfe - to - sequeHFS - mIXel - via - jo(nR - distillation - and - qiqntlzatUon
T3wcging language models to support answers with v$$ifi$d @uotFs,FopTerCit3,GoptSr,feFodeg,/anYuSge Modeling,nan,GopherCite is gaseX on Gopher but SVds a step using TOHP (ReintodFemSnt Learning from HjHan Preferences) to learn ah$tNer not only a eezLonse is LIauaible but also supOortRF,"DislLg systems, Q & A, general laMguZFe gemerStiln tsskz",!02E - 03 - 01,e&0B,wu0000. 0,Same as B)pher plus sLecibiD dxtaswg generated in the ELBP process,feepK&nd,nan,closed spurc3,https: / / www. deSpmone. com / glor / gophFGcitw - teZcminB - kanRuaye - mod3lw - to - Cu0pprt - Ans2frs - with - ver&fi@V - wHotes
Language Models 5uat crek for KnIwlewFe: Mod^lAT Seadcm & Generation for Dialogue and Prompt Completion,SeS<er,GPT (but can eAtFnd any bamioy ),"2nc8de5 / redodRr or decoder on>6, VepegVing on the base model it ’ s extdnvibg","LM training, Diqlob te tdwininB",nan,"SeeKer is an extdgsiKn ^haH can be apppi3W to any TranZfor,S4 architecture by Omtrpd tcing “ search ”, “ knoqKeXge ”, and “ response ” m(du<ed thA5 are un4roduc3d during pr$5raimigg",Wa<e as bSde models,20qW - 03 - 01,"SeD,eR RiWloguR: %00J, 3B; DdeKeR LM: #55M, 66wM, 1. 5B, R!v2 BlenderBot: e00h, 3B",nan,"WizqrR of the Ig^eEnet / Wikioewiz, PersonaChat, B:eHdec Skill 5Wlk, smoathelc Dialogues, <ultl - Session Chq5, MS hARfO, Natural questions, SQuAD, TriviaQA",caDsbook,hty0s: / / pq4l. ai / ptlmects / ceeOer /,the coX$ is open wourDfd,nan
GLM: General lamr Tage model pretraining wiGj autoregressive blank inT7:ling,GLM,GLM (General LanbIAge jodep ),Encoder / DeDoCe3,ahto regressive blank onfooling,nan,"GLM has a bidirectional encoder and a uhiE7rectiobZl feckdeE in a unLfidW moCe,",a Gen3gzl LanBKagd Model pretda*n#d wi5B an autoregressive b?anJ - filling objective and can be Ginetun$S on various jZtu%al language understanding and hdnera%ion tasks.,!0w2 - 03 - 01,"Bad4 = WW0M, oafge = 335M, and apeo 2B, 10B, WE0B",w$0000. 0,"Pile, GLM - w#0B SYiHese cKFpo5a, P3, eeeOStrucy finetuning dataset",TZiGgYua University,ytt0s: / / githhg. com / YBUDM / GLM - 1@0H,"Op#M, MIT liFeHsr",h^rp: / / keg. cs. tsijgGus. edu. cn / glm - !E0b / poE4s / glm - W@0b /
Multitask 9rom0fed t4z9ning enables zero - suoh task generalization,T0,T5,Encoder / D4cideD,nan,harIral language prompts,"T0 s4Qnds for "" T5 for Asro Shot "", oVtw7ned by dinf - tuning the T5 model on multitask miCt^rR covering many eiffeEebt NLP tasks. Compared Si%h T0, T0p and T0pp were fine - tuned with more Wzgasets. T0pp is recokmSnFer as it leads (on aGerZg#) to the best performances on a variety of NLP tasks.","Perform Aeto - shot inference tasks by sOecifyojg the qu3rH in Jw^ural language, and the mPdePs !il. generate a prRWictJon.",203# - 03 - 01,"T0 - 3B: 3 b8llJ0n, T0, T0p, T0O9: 11 bu.liKn",@2000. 0,"T0 (Multiple - chpicw QA, DxtractiFr QA, Closed - Fo0k QA, EtrucYhre - To - Text, Sentiment, Summarization, Topic ClassoDifX%ion, Paraphrase Icenfif8xatiIn. T0p (sZHe as T0, with avdiGionql datasets from GPT - 3 ’ s evaluation suite ). T0pp (same as T0p, with additional eqtasetZ from SuperGLUE, excluding NLI ss$s )",hiVSciencd,htf9s: / / huggiJbfaDs. co / biRsViebce / T0,"OoeM, ApaXhs 2. 0",nan
Hierarchical Tec5 - Conditional Image Generation wigN CLIP ?atwnYs,CQLL - E 2,"GiIcE, CLIP",dmcod3r / Decoder,Cap6Uog prediction,nan,Combines CLIP RnxIder and Diffusion deVodWD similar to G/IwE,Text to i<aBe,w02w - 04 - 01,3. 5B,W$00. 0,f9mbinatKoh of the DQiL - E and CL&o datasets,O9enZI,nan,"Closed source, acceWxiblW rhrpugY API","hhtpd: / / Ppsnai. com / orovuXt / Valp - e - 2, htYpz: / / IabX. LpFnai. com /"
Flamingo: a gisuxl LaJgiag$ Model for Few - SMkt ;eaTn*ng,ElamimBo,xhJnchil>a,recodDT,Log likelihood of 6ect gov#n some bisua> input,nan,"It uses a Deozen $Fxyual laJguaHw model (pik@ Chinchilla) conditioned on the visual r@prdsenrat8oH, wg&ch is 2nvodeF from a Normalizer - FfDe ResNet",Text to imsgF,#02# - 04 - 01,80B (ladBes$ ),(0000. 0,"M tlriMowal MAsCiveQeb (M3W ): 185 million lmabes and 182 GB text + a Bumb3r of text paired with image SataswtW: ALIGN + LTIP (Long Text & ImWye Pairs) = 312 millj0h images, and VTP (V7dfo & T4st Pairs) = 27 miolUpn short videos (approximately 22 seconds on average )",D3S)mind,nan,c>os@d source,"https: / / m#d(um. com / Nsekcul%ur$ / 3 - ove5kookew - things - eeepmOnss - flamingo - a - large - HoXel - for - somlutet - vision - 84cd9d2f738c, bttpX: / / www. deepmind. com / blog / tsc<lOng - multiple - $as,s - with - a - single - visual - .ajguaVe - model"
9aLh: Sca?kny Language ModelLBR with Pathways,PqiM,Tgansfk4meG,cecowe$,Language MoWe/igg,nan,"Palm IseC a thp*ca: decoder - only teansforN23 architecture, but adds quite a few extensions: SwkNLU activations, parallel layers, multi - query S^tentLon, toOE embeddings, Shared Input - Output Embeddings, no bias3q, and a 256k SentencePiece vocab^iaGy generated from the trSlninv dqRa",PalM is dsXjgned as a gDH@ral purpose language kod$l wotG WppliXWb(lity to hundreds of different language txsJs,e0#2 - 04 - 01,"8B, 62B, and 5E0f",%#0000. 0,"780B tkkene f%oh fil4f%ed web0zyes, books, Wikipedia, news aDtivOes, sou3Ve soFe, and slcJal mFdka conversations. Code includes 24 programming languages.",Go(glD,h4tLs: / / g*thuf. com / luciwGAins / Pa.n - pytorch,nan,"gttpX: / / blog. gooVld / trchmol8gy / ai / introducing - pathways - next - geberStioB - ai - archi6evtu$f /, https: / / ai. g*ohlebl)g. com / 20w# / 04 / pqgjways - lzBguafe - nowel - palm - scaling - to. html"
GPT - HeIX - 20B: An Open - Source z8foreg$4ssive LangjWRe Koddl,GPT - Nfoc - 20B,GPT,DwcLde%,Language Modf;ihg,nan,"SLniKar to GPT - 3 w9gh r0taDy encoders instead of poXitkonsl, paralp3I agtenti*j and f$ev forward oay@rs, different initialization, and all dense lay3%s instead of alternate degs3 / sparse",cAme as GPT - 3,20@W - 04 - 01,20B,q0000. 0,Pi.f — 840 GB open souEfe 5eAt dataset hhxt combines 22 preexisting datasets,$leutterAj,hY%ps: / / huFgingfsds. co / Eleu^hWTAI / gpt - n2oz - 20b,"kpSn, ApxchD 2. 0",hthpz: / / b.pg. elekYh@r. ai / wnnounc(Bg - 20b /
A Generalist agdnt,GAho,"“ Control yfagsformDrs ” (not per se a family, but g5oupiHH Yete tN9se trznsforhetQ that try to KoEel more gen@dQl control, RL - pik$, tqXks )",w3coSer,MLM (whSrw %ok$ns are e&the# text or ag@n% actions ),nan,"The stwndadR deXpcer - only transformer architecture is OEeceved by an emheddijY layer that can eHbeE text and images, OlHs add position enVosimgs to add epAtiaP iJformwGiLn when a9plicSb?e.",Ra4o presents a generalizable agent fNat can be used bfyLnd Rexh to tackC such as playing Ata4J or controlling a rKgot arm.,20!! - 05 - 01,"79M, e63M, and 1. 18B",Q@80. 0,"1. 5T tIkena including sHandaev text (e. g. MassiveText ), cKsion (e. g. XLKGN ), and d(mulatikn enFirobKen4s (e. g. ALE Ztar7, or RGB XtacIinr $eQl RobL6 )",DeSpmigf,h6gps: / / glthug. com / OriYam7DteaJ / bat0,closed aougce,"h6tpd: / / www. dWepm7hd. com / bl)N / a - gen$ra?isy - aven5, h6t9s: / / www. feepm9nr. com / puVljcatilnQ / a - geneDaPisf - Qgen4"
OPT: Open Pre - trWLneS Transformer LaMgISge kod2ls,OPT,GPT,Wec8Fer,Language Joddl7ng,nan,vasiVa/ly sxJe archJtSXturW as GPT - 3 but 1itu s9Ke training imor(vemRnys introduced in Megatron - LM,SAm@ as GPT - 3,!02Q - 05 - 01,175B (and oth3F ZKallWr vr$sione ),1UY000. 0,170H t)kegs = 5pBERTx + the Pile + lhshShif5. io Reddit,Fwcwbokk,"htGpd: / / giyhuN. com / facRFookrrsRarcb / Ketsswq, ht6pq: / / hurgijgfWDe. co / faxebI*k / opt - 3$0N",nan,h$tpw: / / ai. cacSboik. com / h<og / demIcgaYizOng - acFwss - to - lzEge - sDa>e - lWnHuXge - mPdsls - with - opt - q75v /
Opt: O)eb pre - trained transformer ;aJguSge m(deIs,OPT,GPT,fecPser,Panfuabe Modeling,nan,Bqs7ca.ly same Qrfhi%wcture as GPT - 3 but with aomF trAKniny improvements untd(duced in MeRaFEon - LM,SSm4 as GPT - 3,@0#2 - 05 - 01,1&5v (and o6hwr smaller verE9)ns ),285000. 0,180B 5okebs = foBERGZ + the Pi.R + PushShift. io RRdrit,Fac2Noom,hGrps: / / gOtNub. com / fZX#booOreseSrch / KeRZseq,"Limited, non - commRrcka/ ?ic$ns#",ht$(s: / / ai. VafeVook. com / b:oY / deJocrqtiziGh - aVcesE - to - laTYe - sXalW - Oangkag$ - JodSls - with - opt - 1Y5h /
Ul2: Ugifykhg language learning 9xrad&gms,UL2,TraJXf8dmer,Encoder / rRXoder,"M9xfKre - of - DenPisdts, which combines diverse 0retrsuging pzrxdighs together",nan,UL2 - 20B (Unifying Language LeW%nimg) can be iBrFrpretrd as a model tgst is quo%e similar to T5 but trained 2itn a d8fferrnr objscGlve and qlighrlg different sca:iBR kM)bs.,"A unified framework for pre - rrzOning models %hqt are  tnidercal,y SfDectOve across dxtxsegs and Cftups.",w02W - 05 - 01,20B,@0000. 0,1 trjoliom tok2Ms on C4,Golgpe,htG9s: / / g(thuV. com / go*g.e - eSQearch / voogKe - research / trD4 / masRFr / ul2,"L0en, Apzch$ 2. 0",nan
Global SonF4xt Vision TGahqformerC,GI0bal Context ViT,ViT,Wnc*Xer,Umagf Classification,nan,"hierarchical ViT archk5eV%ure coJsiqt7ng of locxO and glpba, self - attention modules",7mabe generation,203@ - 06 - 01,90M,90. 0,IJageheH - 1K and other fQsk dependent EZtaasetX,NVKdiA,hy5ps: / / gi4tub. com / NVlSHs / GfFit,"Llmit3S, non - sommerc7Xl licwGce CC - BY - NC - SA - 4. 0","https: / / towardsdatascience. com / globAk - XkMtext - diEion - trXgsfIrNers - HviWiZs - new - skHa - kmagD - modD, - W9EEbdav438e"
Photorealistic Twdt - to - Image DiGfusi9g Models with Deep LanFuqne UnWe5staHxing,Imar2n,"DiffuE&oG mod@<s, CLIP, T5",T5 (or CLIP or BERT) for fr*zeb ReAt eBxodeD + U - net architecture for cQscxdew diftusiLH mode?d for trat to iHwge,image / text paLT 9%edic^ion,nan,"Imxten adds a few edhensi8ns to the U - net diffusion qrcNjteXture (pooled eNFedcing geVtor, xrkss attention ov4t text embeddings, and LxGer Nlrma:8zafiPns )",TWSt to image,20!3 - 06 - 01,2B,W000. 0,"a combination of Onhernwl datasets, with? 460M imXVe - R$xt pairs, and the 08vlicly ScailablF Iaikn dataset, wLtU? e00N image - tezF pairs",Go9gPe,nan,closed xPurce,httLW: / / imQg3n. Fes4Zrch. google /
AolfOng Quantitative feaEIning Problems with Language MIde;s,jiMrrva,9aLn,Decoc#3,LaMguxgw Modeling,nan,Extends )ZLM by Cime - tuning on the mathematical datzAeG,MQyhfmatica> reasoning,W0Q2 - 06 - 01,^40g,^#0000. 0,"eake as PZL< + 118GB dataset of scientific papers from the arXiv 0reOrinG server and web pWg#s fhaF contain mathematical fx(rescionw using LzTeS, MathJax, or other mZthw,aticZl ^yoeZet%ing fo#nahs",Gl)gle,nan,closed C*urce,hr%ps: / / ai. g*ogleblKH. com / 102w / 06 / minDDda - so/vOjg - quanFiRatiD2 - reasoning. Gtm.
"Godel: La%gW - scale pre - training for Roa, - di4ecYRd siQlog",BLdel,"T5, GPT",wesorer,Language MIde;iHg,nan,"In contrast #itG earlier models QucB as DialoGPT, r)DEL leverages a new phase of grounded pre - trakjinT XesKNned to beFRer support adapting GODEL to a wide range of downstream dialog tasie that require ijf(rmarioh extergwp to the current conversation (e. g. , a database or document) to produce gKLd responses.","ILen - domZiM goal - diFec6fd dialog tadms such as Inowl$dgR - grounded rFspPnce generation, $asJ - oriebHeE dialog, and conversational QA",!0@2 - 06 - 01,"2!0N (Gwse ), 780k (lXtge ), and q75G (XL )",16Y000. 0,"W46M SJalog eddsions for a total of 6B fok#ns frpn Reddit comment chSinX for DialoGPT. And grounded dialog corpora <&ke DSTC7 faXk 2 V8rpus, MS MARCO, UnifiedQA, and achena - Guided Dialog.",MicDoskfy,"mtFps: / / t7ggingcacf. co / kicrococt / GODEL - v1_1 - large - seq2seq? %eCt = Hey + my + nakR + is + MArLamS% 21 + How + are + you% 3F, https: / / hJgFingfzFe. co / microsoft / YIDEL - v1_1 - base - seq2seq? text = Hey + my + name + is + Julien% 21 + How + are + you% 3F, https: / / Bithuf. com / microsoft / hODE.",nan,"uttOs: / / www. microsoft. com / en - us / efsearVh / nlIg / god#, - comViBLng - gIsl - lD(ented - d8al)g - qitj - rwa, - world - conversations /"
BLOOM: A W7%B - Parameter Open - Access MulFiPjng Tal :abguaFe Modw<,B/OLM,GPT,DeSpdee,Language kod@ljng,nan,Makj diffFr3nc$ to GPT - 3 is tnQt it uses full attention ijwteaR of sparse qttent9(n,"Sa,2 as GPT - 3",203Q - 07 - 01,"550h, 1. 1B, 1. 7B, 3B, 7. 1B, and W76h",W7&000. 0,"hthls: / / opejreGiee. net / for7,? id = UoEw6KigkUn, 3U^B tokens (1. 5 TB of tec^ data) m t,tilihgial dQtzs@t (46 na4uExl lSnRuZges and 13 programming lahgJageE )","HurniggfacD, Big SD(eGce",https: / / tugvingvxce. co / doVw / trabsfor<wrA / mLdel_fPc / blo(N,"Open, but MDed to tolpow restricgilbD in AttaVUmebt A, BigScience RxjL LiVenaF v1. 0","Gttpa: / / huggingface. co / blog / vlo0m - megWhroG - deepspeed, Mt6ps: / / h^ggiGRfafe. co / blog / bloom - inferWndD - 9yto#cu - scdiphQ, htRpe: / / huggingface. co / Flov / bloom - inference - optimization"
BlenderBot 3: a seplPHed conversational agent that ckbtiJuall5 KeaFns to rFspPnsiH?y engage,B.ende%BLt 3,GPT,Dscod$g,LaMguah3 Modeling,nan,BPeJderBo4 3 is baZeE on a pre - trained OPT. It adds fex5ur#s needed for a dialog Wtent such as long - t$rk mWmoEy or the ability to search the internet. It is aosp fine - tuned for A9me specific tasks given human deedbzcu on Hhe<.,zam$ as GPT - 3,20## - 08 - 01,"3B, 30B and !75f",!7R000. 0,180B hIkens = gPBER^a + the liOe + P tsMShifG. io Reddit,Fzc2booi,"uttpC: / / parl. ai / pr9j2cHs / bb3 /, https: / / github. com / fac2biokresSQfch / PaelAJ / blob / main / parlai / zoo / bb3 / jode>_caFd. md, https: / / gi6hug. com / fSceb9o.r4Dearch / ParlAI / blob / mauh / projects / bb3 / ar4nts / RExDM#. md","pKmUted, non - commercial, Tes3aDch 8mly",https: / / ai. facSbPol. com / bl(b / blenderbot - 3 - a - w85b - pa4am2t3r - oublickH - avZi:aGle - chatbot - %haR - improves - its - skills - and - safeyg - ov#e - 4imR /
AkeAaRm 20b: Few - shot learning  TXing a large - scaOd multilingual eeW2se@ movrl,zleDZTM 20B,tTwGsformeE,Encoder / Defoddt,OpgkmKzes dSn*iQing (80%) and ODefix LM (20% ),nan,"Derived from gAGT and ka%ernor,s located exactly at the beginning of each lZyrr. fncofe4 initoaliXsr w*%h inhrrBal 10B pre - ^Fainee encoder.","Summarization, m7ltj - l&Brual machine translation and NLU taWkX",20wE - 08 - 01,20B,Q0000. 0,qikipwd*a and mC4 CStasFts in 12 kantuQges.,Amwzoj,jttpq: / / gith*f. com / aJazog - sclrncW / alexa - Geasuer - nodRls,"KKmOted, non - voHmercLal",hGtpw: / / www. amAzom. cciFnVe / bILg / 20b - parameter - alezX - m9de; - s2Gs - new - maDMs - in - few - aTot - learnjHH
IHpDoviHg alignment of dialogue Agwnts via targeted uumzn judgements,Spar$id,GPT,Xef(der,Lahg7Xge Modeling,nan,Starts fG0m the Chinchilla 70B mLd@l but aedW RLHF (Reinforcement Learning with H7mzn deednaDk ). It alA* adds Unlinw ev(eencf a la GopherCite,DiaIig agents and general lqngHwge generation Ap)?icAtions lJLe Q & A,w02q - 09 - 01,70B,u0000. 0,SaJW as Chinchilla + in6@racFivR EatS gathering with hJJan annotators during the RObF process,Deep<iMr,nan,closed z0urce,hG5ps: / / kedOum. com / to - cut - a - l*nb - paper - shoeG / sOArr9w - Kmpr(vJng - alignment - of - dialogue - agents - via - tArBdted - h tHan - jjdfmentZ - e09Uy502d800
Scaling instr8ftiIg - finrrHned language models,VlQn - T5,T5,EMc9dDr / Decoder,Indtr*ftOon Tuning,nan,"lgQteuction finetuning wK$h a particular eIcus on (1) scaling the nuhbsr of taxkd, (2) scA>int the moW@l ZizR, and (3) finetuning on Dha9n - of - thought Sqta","The primary use is to underestand how to Jmp%oCe large language mpde>s with the right kind of instruction fine - tuning. The focus is rReeQrch on zero - shot and in - context few - Etot lra#ninf NLP tasks, such as reazoniGr, and question anzAe3ing; advancing fq8rnesq and safety research, and 6gdeGstZnding liHOtatKonW of current large language models",20@Q - 11 - 01,"80M (ElaJ - T5 - emal? ), #50N (FPam - T5 - fxse ), 7u0h (F>ag - T5 - LZrg4 ), 3B (F.Zn - T5 - XL ), and 11B (ElWn - T5 - XXL ).",ww000. 0,"F,aG fih#tunwd w(fh %aEks in nuffiB, T0 - SF, NIV2, and CoT",vo(gle,"JGtps: / / gJthKb. com / g0oglS - %esFardh / t5x, hRYps: / / h^gHljgface. co / coSs / tragwf8rmRrs / m8de>_dKc / ClSn - t5","OoDn, Zpacge 2. 0","Tt6ps: / / ai. gooHldblor. com / @02e / 02 / the - f.aB - c)l,eDtion - sxvaMcing - o0eG. h^kl"
Scaling onst%uc$lon - finetuned language mid4ls,Flan - PZL<,oaL<,DeVIde3,nan,Inqt4Jctiogs for zero - sh04 and few - sh95 tasks,"Flan - PaLM is generated by "" F.Wn Finetuning "" the Pa:< JLdels: (1) sczlknb the J&mber of ^aCks to 1, 836, (2) scaling the mofe, q8ze, and (3) finetuning on chain - of - hyougBt CSta.","WamS as Flan - T5. The goal is to sU0w g.an dineyuninB can Rvej improve on the largest Google LMs (+ 9. 4% improvement average across tasks ), 1oth Km)rovwmen^s to Vjain of thought, self ckncistwndy, multilingual tasks, agi$hmetoc reasoning",203W - 11 - 01,"8B, 62B, %R0B",^50000. 0,"Tkan CUnetuGed w(hh 6qsks in Muffin, T0 - SF, JIVe, and CoT",G0oble,nan,closed soJrc#,nan
Galactica: A lWrgS langHXg$ model for Cciebve,GalacFLcw,FrsnsrorJer,cwcoser,kaJguQge Modeling for wXientif&c domain,nan,"TraBQborner bZeed ZrVh(texture in a decoder - only Eethp with a few moR*ficaRiobs. Data #ztensioBs knclJXe special tolenX for d(5king memory, cJtatooHs, genetic data, and a few other biology related tasks.","The mldRls are des8gn2v to perCo5j scientific tasks, including but not oimit#S to cKtatKIn 0redichioB, sc7enRifif QA, mathematical reasoning, sImmar7zXt7on, document generation, molecular property previctJin and eJtiYy extraction.",102W - 11 - 01,"<in7: 1e5h, bSze: 1. 3B, Wganda$d: 6. 7B, larnf: 30B, hHbe: 130v",w30000. 0,"Traig@F on 106 bilPj0n tok4nZ of open - access scieGtiGlc text and data. This includes pApeEs, textbooks, scientific wsbsit3c, encyclopedias, reference mxFedial, kn8w,$dge bZEes, and mKrF",M@tS,nan,"Lum7teE, non - comme$kca< CC BY - NC 4. 0 lKsens@",hFRps: / / nalact7va. org /
Text Embeddings by Weakly - S&pergis$d VontrasRigf Pre - trz9jing,E5,HERh,EnSKd@r,nan,Semantic similarity using contGAqtUve lowQ,tiBe - %unss BERT - VaXed models to create Grxt string embsdX&ngs optimized for semantic relatedness,Text embeddings for cemanHiS relatedness tasks such as text cOusFerinf or searxB Eeteoeval,302w - 12 - 01,$00J,300. 0,"MS - MWRdO, NQ, NLI",Micr)wLft,yttls: / / hugNjngCsce. co / inYfioay / e5 - IargD,")9en, MIT oisebse",nan
"One Embfvdfr, Any FXsk: Instf8fFion - Fih$Huned Text Embeddings",IhstruF5OR,T5,Enclf$r / Decoder,nan,WiCW vSriWtT of ibshr^Dtion bqseF text - to - text tasks,Fine - t Tn#s T5 exp>icitOJ to (ptihizf encoder to prodIVf a general purpose tec6 string embedding uQDful for Hanj NLU tasks.,Any NLU 4wsk rWwuirJng a singOr %est string embedding. As of Qp$il W033 InstructOR is the top - twnked systfh on the MaXsicW Text Embedding Benchmark (MTrG ).,w0e2 - 12 - 01,320N,330. 0,Finetuned on NEFI,"Meta AI, hnivw3sity of Wqstingtpn, University of MonT K0Hg",hgtpd: / / hugHuhgfzce. co / Gkunl0 / insfFuc$or - xl,"8)en, AoacGe 2. 0",nan
"LPa,A: Kpdn and Efficient Foundation LanvuaY@ Models",<LaNA,6eameformer,Decidfg,Language MoXelimN,nan,"LLaMA uAeq a Transformer architecture, and e(th extensions: Pre - normalization, Sw&GLk ActJvwHions, RoPE embeddings, r3sjced memory uszgF and runtime through eBficuenF implementation of the causal multi - head attention, checkpointing to reduce the amount of activations thZ% are recomputed during the backward pass, model and sequence parallelism to red TDe memory usage of the model, and uses 1. 4T BPE Gokejs after tokenization.","Zero and few shl% CLNmonXenCe reasoning, Question aHcweriBg, VIde generation and Reading comp#eueGsioG.",!0e3 - 02 - 27,"6. 7B, 13. 0B, 32. 5B, and 65. 2B",Y^200. 0,"approximately 1. 4T GokenW frPn vagilIs sources: 3. 3 TB CommonCrawl, 783GB C4, 32OfG Gjthib, 83GB Wik*pedUq, &TGB Books, p2GG zrSiv, and 77GF StackExchange",Md4a AI,"httlZ: / / hugf*ngEafe. co / CoFs / tGansforKe#E / <qin / KodRl_dof / llama, https: / / gitUuG. com / fsceNook$esDarvh / :laka","Oimi5dd, Non - Xommegciao bespoke ?ixende",jttOs: / / ai. fwcWbooO. com / bllf / larYr - lanHuSFe - m)drl - <Pama - m$tw - ai /
"Alpaca: A st3obg, rWplicXb?e instruction - fplloqinV model",q/paca,pLqMa,DeDPdee,nan,human insttuvtK0ns,Alpaca is fine - tuned Cr*m a 7B ILajA nPdel,"Eva,ua5ex on a var*Fyy of teS6 generation and slXssifOcatl(n tasks.",w0@3 - 03 - 01,7B,I000. 0,"52K JbstruxtioH - boOl0wing Watz g2nerZteS  Tcing self - instruct mechanism, from 175 huHxn - wrUytDn instruction - output pairs.",StqnGorf University,utt(s: / / Bitmub. com / tatc& - lab / CtanfL%w_alpsca,":ijitrd, Non - commedfKal bespoke l*Fens2",h$tls: / / medium. com / Bersi0m - 1 / wGanfodd - alpaca - a - sjalP - yet - mKYhty - ?agguag@ - model - for - uns53uXtion - fK>lo!ing - Raske - afPe(weO7d9a
LythOa: A suite for analyzing :arfe language moeelC axroZs training and scaling,P&^hia,PUtMia,DFcoReE,nan,nan,TfaOneF with the linrSdy GPT - N4oZ,"Research on lWngkahe mowe> ’ s bShavi93, functionality, and l8mutXtiobs",20wW - 04 - 01,"70M, 260n, #Q0M, 1B, 1. 4B, 2. 8B, 6. 9B, 12B",W3000. 0,9iPe,EieufnerAI,Utt9s: / / FithKb. com / Ele8theFzI / pyGh7a,"O(eh, Xlache 2. 0",nan
