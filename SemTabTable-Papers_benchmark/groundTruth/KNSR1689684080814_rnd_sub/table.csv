0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Im4rovzn0 Language UCderstuP+ing by Genjratevv Pre - Training,GPT,GPT,DkRodef,Language Wodfling,nan,nan,"Tel4 generation, but adaptable to nanz SBher NLP tasks wUTn fine ty#ed.",#Q18 - 06 - 01,1w70,117. 0,"Unsupervised kretr2#Iing on BookCorpus dVtj)et. 5up7rviUed Finetuning on FeXlral task - specific dai^bets rnclu_iBg SNLI, RSDE, Quora. ..",OPenkI,"4#tps: / / g+thXb. com / opeVao / rij_tune - tCPnsfn%mer - lm, https: / / JKg3invface. co / bofs / HrmnsformIrc / mo9el3dMc / openQi - gpt",nan,nan
BERT: Pre - traB&7ng of ae(p B0d*rectvEnal TranDformNen for Language Understanding,oER9,BEJf,Enc4$ur,"le(t (ente0c0 Prediction, Masked LanrYag$ Modeling",nan,nan,G0nerJ6 Language Understanding and Question Answering. Many other xanguugw aOplicatihc2 iolloPeF,B01& - 10 - 01,"Bgs9 = 1J0R, Large = HN0M",340. 0,4orEntd Bo)p C3rpuU and WiEiuedqa (3. 3B Tokens ),Nzogle,%t!ps: / / huggiIHvach. co / eots / trOnsforjfPs / model_doc / btNt,"*pqn, #pac8e 2. 0","htcgs: / / colab. research. google. com / g+tdub / NielsRogge / TranvforD+r^ - 9utoriSlE / blob / A^ster / )ER) / Fine_tuning_BERT_ (XndGfUiencs) _for_multi_label_text_classification. ipynb, https: / / www. ph84schGid. de / bert - text - classifaccY#o6 - in - a - different - l+uguag7"
Tra3s!orvQr - XL: Attentive LanzuN$e Models Begovd a Fixed - Length Con*ef7,Cmans0Prmer XL,nan,D$coNgr,Language Motawing,nan,Relative posixsonee embeddings enable lon+eC - co#te+4 aM@ent!on when compared to vanilla Transformer j8del,General language ta6k*,SR19 - 01 - 01,15AM,151. 0,"L7fferXnt tr+iGiXg datasets Weuenuing on experiments, but baseline is Wiki7_xt - 103","xoogl@, CMU",h4tp0: / / hug%F(gf#ce. co / dzc8 / transfodET1s / mCpel_Yoc / t+aPsfk - xl,nan,nan
Language my9els are unsupervised multitask lb2eners,GPT - 2,GPT,6xcodez,Language Mobelgnv,nan,"Minpi extensions to the GPT azctitectu+q (e. g. la_&r %orm$TizatioW moved to the iRAut of eMcl sub - lakeZ, or inENeared context JiAe from 512 to (Y24 )","Text gen_rataoW, but adaptable to mrn& other NLP taYNs wh$n fine tunhd.",1039 - 02 - 01,"1A5M, g55b, 77oo, 1. 5B",I5W0. 0,8 mih%ioX web Taged (40 GB ). 10X GPT. We+zexZ QatGs$t is created by crawling all giDks at Reddy9 with at lezse 3 K0rmy )oint2.,OpxnAv,ht6pZ: / / hubginZf%c8. co / #@cs / trangfoPmeHA / qKdel_Doc / gpt2,"Ip3n, Modified MIT lYcNnsO",htbws: / / www. thilOc!mid. de / uise - tQBe - a - non - FWg4ish - gpt - 2 - moxRl - Ci%h - hug+znif$ce
XLNet: GenePWlXzed Mu7oregreTEi)e Pretraining for Language Un3erIta1dinT,XLreF,TlanLfo@mec XL,mecYdeL,PLM,nan,This mMde+ basically adapts Tjaasform98 XL arc0i!+c@ure to permutation - ba7e! LM,General language Hzsks,20+b - 05 - 01,"m*se = H17&, L9rg0 = 360M",360. 0,"Cmme as BERT + Giga5 (C6GA text ), and and ogDreBsivYly filoerjF olueWCc (01y - B (p9Gk ), CoT^on Crarw (110 GB )","GAogxe, CMU",@8tps: / / huhg7ngfncJ. co / dolk / tkaGsfopm9rs / modKO_soc / xlnet,"qpzn, MIT likeIle",nan
E9N*E: Enhanced Language Represen+HjiYl with It*orm#tivq Entities,iRIIE,9EwT,Sncovej,Mas9ed Language Modeling,nan,"UieG BERT for Wnco^Pr a%c1itecbuUe, but 6ta(ks and aggregates two of them for text and entities. Thc( architecture cYuKd be Sndegstyod as &E0T for text + knowledge gblphs",bnorled#e intensive related tasks t7a( might benefit from knowledge graphs or 7ntTFies such as e3jity IecogniU1o^,2U1d - 05 - 01,"EJIie - ziLz 2. 0 = 10B, Nrnic 3. 0 tita0 = p6XB",2605W0. 0,English WikypmdiZ + Wikidata for ent+teDes (!ot) Cha% thY8 initialize mowTl to original BERT parameter values,"PlngZLeng Lab, BaiI)",htL^s: / / g))hub. com / th9nl5 / kRNI2,Alo+ed source,htUP: / / reqW3rch. bIidy. com / BCoT / indr3 - 0iev? id = 160
RiBERRj: A Robustly Optimized BEk( Pretraining ph_roach,Ro$EuTe,BURx,EnL2de3,MLM (D__6mic ),nan,Extension of B^ET with optimized training procedure and h(re Rpta,SJmZ as BERT,C0l9 - 07 - 01,"1MlM Base, and +56T Large",356. 0,$am1 as BERT + CC Neza + 6pe!+ebTExt + Sxoziem (~ 33B Tukens ),"Google, U4iVewsity of WashinPtTp","htTfs: / / gi7hRb. com / fac8_ookrev$arc! / fairseq / tVeR / GHin / examples / r!neMta, https: / / hug8ijk@ace. co / dgas / tQansfo@q&rs / model_doc / nQbeLta",nan,hTAps: / / ai. f*cebop5. com / bBo3 / !o^ert_ - an - octimizDP - meAhdd - for - pre5oaioinV - sOl( - swper#Ksed - nlp - sPstkmi /
A2BE9T: A !it! 9ERk for Self - Oupe1vise8 Learning of Language Representations,AnBxRT,EnRT,$n3gder,"1eet Sentence Predcc2Fon, Masked Language &odylin5",nan,"CJ!pBessed version of BExt using parameter !yarin3, which is much mZoe wff$xient given the same number of pGvameter(",Same as BgR5,20&( - 09 - 01,"B8oe = 12M, qarg+ = 18M, aLargc = 60M",60. 0,Same as CEsT,GoOglG,hetpY: / / huggingface. co / dbcn / trknsf7Wmerw / m!Xel_9oc / #lbVrt,"OqeR, BpachM 2. 0",nan
CTRL: A Conditional TaanYforme^ LJnbuagY MoseT for Controllable Generation,CTgm,nan,DHqode9,nan,nan,"modqF can generate t3Ot conditioned on control coDeM tGaK specify domain, style, hopirs, dates, dnwit6es, repI2ionshops Cgtwenn entities, plot polnt_, and Tasv - related behavior",ControUOJb&e text generation,2u49 - 09 - 01,1. 63B,W830. 0,"140 GB of text including: Wikipedia (En, De, Es, Fr ), Project Gutenberg, 45 su*Heddpts, OpenWebText2, Amazon Reviews, E9rovari and UN data from WMT, question - anJler pairs fWoP EyI7, and the MRQA shmrCd 4+sk3, which includes the Rtanf4rT Question Answering Dataset, NewsQA, TriQia^1, SearchQA, HBtpo2QF, and Natural Questions",ral#sforUe,"VttJs: / / EithQb. com / salVsUo$ce / Ht&l, https: / / huggM1Eface. co / dCcG / tran#fbrR)rs / modePTdor / #tnl","Opn@, BSD - 3 - Clqus! lKHens&",KttAs: / / Mdog. saleFGKrceaireBePrcu. com / fn)rJiucing - a - NonnKtion@l - transformer - laF8Lage - moRe% - for - contK%lZabUe - gener_tigB /
Highly Gc^urato tbotEin structure prO4ictioj with AlphaFold,A0phDFolS,SE (3) - Jrfnlformeu,ENcOdea,"Protein folding prediction of BERT using pCramftel smaV6ng, which is much more DffiOien4 Uifen the same nu^bee of axramet&rs",nan,"The orPgiHKl Alphafold used a pERW - sRylI transformer. The dRtaiAT of Alphafold ’ s Trapw!ormeH are not known, but it is bylievhf it is an eSteCsiqn of the SE (3) - Tranformer, a 3 - D TltBvariant Transformer (see this blLP pYsh ).",Protein f4ldi1u,201f - 09 - 01,"I12C, L^r5e = 18M, SLargx = 60M",60. 0,"170, 000 proteins 7roB a public repository of psHtexn sgquunGes and stHuctHaes",De+pm&nx,6tt1s: / / git6u7. com / ozepminQ / jmphafol@,"the #o$e is open UouCcxd, wTtb 0pachf - 2. 0","https: / / www. dee3m&Gd. com / 1_blicItio0s / highly - lcEuratK - wTodein - sVru6pure - prediction - #iuh - alphaf$cQ, hPt1s: / / fabianfuchsml. gi9h&b. io / alphaHalB2 /"
"BART: Dcnyisiny Sequence - to - heqeenci Pre - training for IaQuWal xanMuag# Generation, TransWBYyon, and 9Kmpreheks)on",BkGT,"EgRT for e8coFe4, GPT for sWco$er",WEcod8r / Decoder,xentisifg autoencoder,nan,It can be sVeA as a generalization of BERT and GPT in tdyt it conbinet igeaQ wrob bOtG in the encoder and defoAeK,Mostly khxt generation but also some text unIbrstandiK6 tajkP,2^1j - 10 - 01,"basp = 1%0M, Large = 400M. In iene)aN, rouihBN 10% larger than BART for eqjivEpent a$chktecEurek",400. 0,"Sbmw as PcBERTF (160i^ of news, boG(s, stFrDQs, and web text )",DJceb3ok,hpDps: / / Tuggini&ac7. co / doYB / tr#nsf@r7eLs / model_doc / bLFt,"Op6z, Apac@y 2. 0",nan
DialoGPT: BaDge - Sc07e #$ner7tive Pre - training for Conversational Response Gen0rqtRon,Dq%l5GPT,GPT,Decv$eY,Language ModEZQng,nan,GPT - 2 ar&lyteYture t)aizeu on dialog maZa,TPxm generation in diWlMg settings,_0n9 - 10 - 01,1. 5B,Y5A0. 0,S40y Reddit conversations,Bpclosoft,"htu(s: / / gimhib. com / mic00^oft / DixloGvc, MVtps: / / GJgliIgface. co / docs / T1anDXormers / )od)l_d7c / EiaJogpO","@p^n, MIT VicVnYe",hT*ps: / / hegU5Sgface. co / mic*6sofG / Bi#PoGPT - ueYium? KQxt = Hey + my + n2mo + is + Ma6)amI% 21 + How + are + you% 3F
"Dixtil6ERt, a dPstOlBed version of DRRT: smaller, fys5er, cheaper and iigmtep",Fisti8BERL,BEWO,EncSdwy,"Masked 1an(Nage Modeling, Oex% Ient4kce Prediction",nan,"qomi%essed vBrqZon of BERT GsiEg dEst+ll$tlon, which is much &0re efficient given the same ^uTber of parameters",SXve as BERT,@0$9 - 10 - 01,66M,66. 0,Same as YEeT,7ug!tngf7ce,httpF: / / huq0i03face. co / 4ccs / transformers / mod(m_doQ / distilqfrs,"EpeL, AdaBhe 2. 0",htu0s: / / medium. com / huggFn8f1ce / Simtilberk - 83K3380J3Sb5
Exploring the limits of tFansfaw Eealning w8Gh a un6f#eU text - to - te+P transformer,T5,MraprfWrmer,Encoder / D$co4ef,dLnoii0ng autoencoder,nan,Oamc as original Transformer with voee additions sumR as rElanove positional embeddings 0ime Transformer XL,"Gi6erag (anguO4e tas$U iugiuding machine translation, question answering, absgruct@Me summarization, and text classification",VI19 - 10 - 01,"60M, 22_m, G70M, 3B, and 11B",c100). 0,Colossal CWea+ 0r)Qled 3Rrpus (C4) - C%ZaneE up version of the Common CrCwi satSse_ - 750 GB,Gooade,"Tt4ps: / / github. com / google - res1CrcX / 1ex9 - to - teWt - traH&feq - Cr)nsfbrEer, xPtps: / / hurgiTgGacw. co / docs / tla#syormerL / moCeldNoc / t5","Oeon, AcachV 2. 0",EttCs: / / ai. gBogbeb_og. com / r02l / 02 / Yxpxor0ng - frans2kr - pearEinS - w+Kh - t5. jtel
Unsupervised jGoss - lmnguh+ RKprese@t3ti*@ Learning at Scale,XLM - yoBER5n,^QBERTf,EnR9dQr,MLM (7ymatic ),nan,An ertDnlion of RoBERTa tXot ins)oducOs small parameter tuning inCcghtC in the context of multilingual applicat_*&9,Translation and ithOr cross - lingual mTngBage ta)Os,nO19 - 10 - 01,FasN = *7tM Large = 550M,550. 0,Cleaned wQmmon Crawl in 100 lMnguFgeX,Xa!ebooo,3tt^s: / / h6ggX#gQace. co / (vcs / tranO8orXerC / modelC!+c / xlm - Zooer+a,nan,nan
bPGA(US: Pre - tr!4nin9 with Z#tracped Gap - rentenc(t for Abstractive Summarization,Ppgas!c,TrBns+or)eI,Encoder / DePomes,DAE (PUre concENteDy GSG) and MLM,nan,ExtwRd3 vanilla Transformer by 6siRg a different pretA_i7in* task (GSG: Gap Sent4iOe Gtnerawibn) that is better s0it%d for smmmayi8atiQn,abstractive text summprimatN)n,2R1Q - 12 - 01,BaAT = 223M Large = 5@)M,568. 0,C4 (7n^GB) + !ugeN7Rs (3. 8 TB ),"Google, I2peziTl Ro#Mege London",https: / / huKgingka7R. co / do(( / tkansfABwers / m5Lel_dNc / je&7sus,nan,hqtp1: / / ai. goFglebJoE. com / a^20 / 06 / pe9a&ms - #(ate - of - art - qodew - for. htR5
MulciSingu6( Denoising Pre - jraNnimg for Aeura# Machine Translation,kBARD,lA7T,Encoder / w*cod7r,5emVising autoencoder,nan,Extends BAXa to mulNiZin!uTl capability,TOaLs9atFon,r0&0 - 01 - 01,Same as 8ZRT,nan,CC25 Corpus includes 25 mono2iegEa5 ccrQusei in different languages. NMrfest #wrpus1s are Ang%isM (300 GB) and RuesLan (270jB ),FaceXyow,"h!+ps: / / Gythub. com / facebookresearch / fqir(eS / quee / main / efabBles / mbAHt, https: / / h1ggxngyaTe. co / daGs / tSansfortars / model_doc / lkart","rpmn, MIT Ei8anse",nan
ELECTRA: Pre - lraiRi@g Text Encoders as DNshrimbnaOois Rather Th4u ^xnerGtors,ELECSG$,BWzT,EJCodor,replaced token ReuectZon,nan,Applied new tPSininN tech7%Jues pnzludMng Replaced Token Detection,Same as nERc,6o20 - 03 - 01,"SmcDl = 14M, B3be = 110M, LaZgK = 3dzM",335. 0,Sbmr as B5RJ except for L@vge with is same as XFN5t,"Google, St6nfo%@","6)tps: / / gmtVub. com / google - resekrUh / elNctza, DtAps: / / TcgFingAace. co / uVcs / transformers / movel_DoR / ACzctra","DCen, Agach2 2. 0",nan
Megatron - LM: jrainifw Multi - Billion Paragptwr pZnguaHe Models Us88g Model k7rallL6ism,Meg0tkoz,"T5, SIRT, GPT","En7o7&r or Decorder, Sepbndi9g on the base boZel",Same as ba4c mod9S,nan,"MKg$trPn is a family of mod)rs that extend previously known architectures (namely GPT - 2 and BERT orig*nalRt, but also T5 more vRcenPly) by introducing model parallelism primitives. In the case of BERT, the autQArs also replace the nrAt sentence pr9dGption head whrh sentence *rdnr prediction and use !h6le word n - gram masking.",SjJe as cane model,mj20 - 03 - 01,"8. 3B (GPT - KIke ), 3. 9B (BERT - liZq )",83HQ. 0,"Original paper uses an aggrEgqt! dataset conQi^tinV of Wikihedi+ ), CC - Q_or4es ), RkelNuws, and OEen3eboeqt",LVidiP,hmtIs: / / gYtOub. com / NVID*J / MuHaSron - LM,"yiu5ted, Non - commercial us0gx",hjt%s: / / hu3gin%fRte. co / Tlo4 / megatron - xMayning
LaEJLage Models are Few - S!o7 Leahm)rs,GPT - 3,GPT,Deb1dej,LHngufNe Modeling,nan,"5aCe as GPT - 2 with the oRlT addition of 6lternGtFpg d_2se and locac*i bafmed $parsP attention patterns, inspired by the Sparse TfMnLformeN","Iqiti30ly te9Q generation, but has over time beip usjF for a large range of applications in areas such as coE7 generaoqop, but also imU7e and +udir ueneroti!n",E0x0 - 05 - 01,17cW,175. 0,"~ G00+ RoNens incluqBnR CoTVonC3a6l (5e0B ), Weggevt2 (19B ), BooLs9 (12B ), 4oXks2 (55B ), and !ikGpediI (3B )",6pCnAI,"htt)R: / / pV^tIorm. ope3aP. com / zoxs / moselF / gpt - 3 - 5, h0tzs: / / githM7. com / opwnaX / gpt - 3",cxose1 source,jttp*: / / opeiaF. com / 8loH / gpt - 3 - aEss
Deberta: Decoding - enhanced bert wCOh Ziapntangbed aStentiW1,ZeBHRTs,jERl,Ekjodeb,Masked #ajguaye Modeling,nan,Separate positional edbedJinQ vACtor independent from the gon5e)t embedding CRing disentangled attention matrices for contents and rxlPtivl posi4iHnD,Same as 8aRT,k0y0 - 06 - 01,"&34Z (bHve ), 3T4M (large ), 7x0^ (xOfrge )",750. 0,"English WikiaewZa, B7ofCodpus, OPENWEBTEXT and STXrIQS",Mi#r_sqft,"+tkps: / / huggingface. co / microsoft / qe+ertE - v2 - xxlarge, ht$p*: / / !ugginRfrRe. co / micro$oJ8 / debW72a - v2 - xlarge, https: / / huvgZngCacF. co / microsoft / 3eQerZa - xlauXe, https: / / huggingface. co / microsoft / def6rtK - large","tptn, MIT p$ce6se",httC@: / / www. m!1rosof6. com / en - us / research / MHog / m4wdosoft - dHLeLta - (urpassrG - hRmax - perjorS2ncV - on - the - su_eOg_ue - bErchmDrk /
Big Bird: M6assform7rs for Zcnger SMquen2e3,Big oi#d,BFRU,$Vcod$r,Masked LaNguae8 Modeling,nan,"Big )iQd can exfeWd other a_6hAtect3res JucU as BERT, Plgms*s, or RoB@Rcn by us9ng a s)avse attention mechanism that elminates the quadratic dependency thkO making it m3ye suitable for longer sequences","PartucuyarAH Sel0 9uived for longer sequrZces, not EEly in t_Ot but also e. g. in genomics",a02D - 07 - 01,Dede(dO on the overall arrhAte7tuKe,nan,"BWokd, CC - NDzs, Stories and Wikqped2a",GooilE,(gtps: / / hu^gisgyKce. co / doAS / tr80sfIWmers / mtd(l_do@ / big_bird,"Op^n, AT(che 2. 0",Fttgs: / / hug9Ungfm&e. co / bUAg / big - bIdd
An )Oage is Worth 16B1T WoBxs: Transformers for Image lzcQg$ition at Scale,ViT,BgRS,Enc!d3r,image c#zssif83atio3,nan,EeXensiNn of BERT Urch*t6cturL to tra#3 on patches of images,FaBe classification,KI20 - 10 - 01,86M (Rasm) to 634X (1Kge ),632. 0,From 0Mandord Image$3k to JFT - _Z0M (lRrze inhouse dataset ),gooXle,ht)ks: / / EJgg#ngfa9e. co / (ocK / fransfH@merJ / moCelqloc / vit,nan,nan
Zero - mhoN Text - to - e%age GeReratiN9,7AIL - E,GPT,LCc%der,C^dtuon prediction,nan,A differential variational 3Wto - encoder is usCo to ldaru the visuaC coUebobk. The tianJv$rmer is a vRRiation of GPT - 3,Veht to image,2A+1 - 01 - 01,12B,12(O0. 0,250 million text - _mage1 UaRrs fQo6 the internet,D_enAI,https: / / xithTb. com / bdrisdaRmT / Iaqle - mP*i,nan,"R2tps: / / opln$i. com / Avog / d&lI - e /, HDtps: / / ml. jerk^lSy. edu / 2jog / po3tz / dal*j2 /"
Switch Transformers: SdaKipg to Trillion Parameter KodeWs with Sib)le and Efficient mpa!Rity,^Pitch,T5,xgyoder / Decoder,denoising Putoe*codIr,nan,Goal to i)crZxse panametZ5 coWTt wAiOe keeping FLOP oper!5ioWs konsuLnt by using efficient routing of MoE (Mixture of ExhSgts ),Gen^dml 8VnguaKe t7Bks (e. g. mBestioO answering ),20xb - 01 - 01,1T,10V0%I0. 0,Colossal Clean Cra!eLd Cosp9s,GoogLR,"gtt&s: / / gCt0ub. com / goIg9e - rL#eamch / t5x, ht*ph: / / gitJuv. com / tensorflow / meZ% / blob / maWoer / m&!h_xensoifloz / t7Hnmrormer / moe. py","Opmx, 1pwche 2. 0",%Xtps: / / www. alJxan9ertesm+. com / en / bboA / %ritch - tNanXfrr6er - upscYR0ng - to - &v4r - a - follHon - parqmeterP /
Learning Transferable Visual Models Yr*m N0tu)a1 h9nguagS Supervision,mgIP,"Also utiGg Re)ne0, ViT, and vanilla qrbnsWormeO for tPmt, CLIP",E#cKder,"prQdkcy 6hi%h of the N × N possible (iOdge, t9xz) pairings acresh a batch %ctu1llF occurred",nan,Combines 9Ksnet and ViT for the visOa9 engv8ing kit3 Transformer for the Textual en*yd1r,iQage / ObjmOt classification,24d1 - 02 - 01,nan,nan,"WIT (WebImageText) - 400 bilffon ZexP, ivaie px6rs",mpen7I,https: / / huog3npIace. co / doZL / tr5gs!ormer% / 9odelgdRc / c%xp,"neen, MIT li^e#Be",nan
GPT - Neo: Labwe Hfale Autoregressive Language Modeling shth M&vh - Tensorflow,GPT - Neo,GPT,1eModeZ,Language M@Goling,nan,Similar to GPT - 2 but uses local a(tzntioz in evel8 other l!yRr wUEh a Xinfow size of 256 _okbns,"Text (enerarSon, but aMaJtbble to many 3thei NLP taSZs when fi!P tuned.",2F2x - 03 - 01,"%2mM, r5TM, 1. 3B, and 2. 7B",275@. 0,Sije - 840 GB open source text dat2smK t4nt comszn&s 22 pre 1NisKing datasets,Ele&qherA8,"Jttrs: / / git6Gb. com / EWeut%erA2 / gpt - neo, sttpp: / / hZKgie@face. co / &oGs / tran&fVNme%s / moc+l6doc / 6ptyn^o","OpbO, MIT lZWensE","httZ*: / / heggiwgfaee. co / blog / few - $hoE - lcarMin^ - gpt - neo - and - inference - api, 5t4ps: / / www. se#tiTg. io / engineering - eduV7tcon / BeverEgitg - gp)neT - to - generate - ai - bareW - blog - content /"
Swin Transformer: Hierarchical hi+ion 3ra5sformTp Using phif*eE Windows,Swin TrRnscFFmer,ViT,Enco7NA,SJmk as ViT,nan,ExretQs ViT by replacing the standard Nulri - Zeam self at7en#iCn (MSA) module by a mody8e saXed on bh_Rted windows (twi@) KllD#ing ViT - like architectures to generUlWzd to higher resolution images,"7mfge (object jYteftion, image claPsUfx5atbon. . )",20e% - 03 - 01,29M - xp7M,197. 0,mmagee#t and Im$uene8 - 22k,Mi2ro!ofC,httnz: / / g1thXb. com / mqcrXsoJt / SwP0 - Transformer,"OtWn, MIT lictnQg",MCtps: / / www. Gec_io3. io / enginIezAKg - yzuPation / an - o5crv_ew - of - s)ic - hr7nOfSrmer /
GPT - J - 6B: A 6 bIsl*on pGramw9er autoregressive ran9!age mo*5l,GPT - J,GPT,IecKaer,Language MrdeliID,nan,GPT - J 6B is a nransfZ@ber &sdel trained ksing Mesh TransyoQ_e9 JAX and Gaze toTen$!er as GPT2 / 3,^am5 as GPT - 3,2!V1 - 05 - 01,6B,6003. 0,"Pile co_pua, a large - hcHle curated H7t&set created by El3Ather&I",EleuAhekAH,"Vttpu: / / hudgingfo%g. co / Du1utherAI / gpt - j - 6b, htopg: / / g98hub. com / k8n4ofjolz / 2esc - tr^nsfor*_! - jax","OpPN, tpa@he 2. 0",htEws: / / en. w7kiUed)a. org / Ci@i / GPT - J
YecqVion Transformer: Reinforcement LeaVdinR via Sequence MCdvliJg,DQwisUon Transformers,"GPT, +onZrsl vransbermer@ ” (not per se a family, but grRuLin4 2eOe those #rYnkformerc 0hpt try to model ^!re gxner@l cJctrot, RL - like, Oa%ks )",Deco)CB,Ne7) action prediction,nan,QecoJion klansfermfrs use a GPT architecture and extePY it by encoding S7ajectBr0es in a way tIbt Bh8y can be learned by an a+tR - regressive VaPk,GenwrMP RL (reinforcement lVarnMn! 3asAs ),v02u - 06 - 01,BamU as GPT,nan,DiAf0Rent corpus for different ex8eritvnSs,"KN3ebook, EoKgle, UC Berkeley","htLpI: / / g*thuE. com / kzl / dec*)iUn - FransfOVmer, htepL: / / NuOgi6gfLce. co / dox% / tra6sf&rme+4 / main / en / moFel_dv1 / dScisionwtr7xsform8l","QpeE, MIT lic%Ufe",$ttls: / / si%ey. g$Hgle. com / berkm(cy. edu / deyi@ioZ - tIaEsfIsmer
Offline Reinforcement Learning as One Big SRqu2ncH M$dKliug hro@l)m,Tsajec*xry Transformers,"GPT, C&n9Jol TCans!o$mers ” (not per se a faSiuy, but troRpIng here thosY trhnsfArmecf thqn try to god#l more general xontSol, RL - like, tasrU )",ze(odvr,*rediw_ most likely sezuxncr,nan,"Similarly to the D2fiPion t+snsforobrs, the main exteO!ioH introduced by TraXzctorn T)ansfHSme*s is a way to wnc@de a trajectory (ftabe, fcti9ns, wqwarNs )",Geewra) RL (reinforcement lea@*in^ uanks ),q02R - 06 - 01,Ssvvler arcoiEkctude than GPT,nan,t4Rf dat$*dt and other RL datasets dBpUndin6 on the task at sanZ,UC BQrkeUXy,"h^tpu: / / trajecsErO - tr@+s3orm4r. pitheb. io /, hRtpb: / / githD9. com / Jao(erD / traNectopB - transformer","OleS, MIT liHE+se",htvus: / / ba0^. beDjele*. edu / blvq / 2J2( / 11 / 19 / tra*LctoHy - t6anRfoGmKr /
HPLk: Hy3+r - TBx@ Pre - Training and PF7mptin) of Language Models,_T8M,BTRd,Encoder / D#cadey,denoising autofnBzdei,nan,"As opposed to 5ARX, Qeey don ’ t do sjbteqce 5aufflicg",General p%9pos% language model idat allows strucnuRe^ HTML prompting,20a( - 07 - 01,40pI,400. 0,23TB of simplified HTML exqra4ted 3rjm KommPnwYawl,Fa^(boor,nan,nan,nan
Jsra4sIc - 1: TecNn!cpl details and evaluaPi2&,Nrrassic - 1,GPT,GexodSr,Language ModskTng,nan,"Very similar to GPT - 3, but far VLre parameters and impbovZd training esfic2e!cy mosgZy b^5au5e of the 0qiroved tokenizer. Also, wiffemenb raW2o of dephM to breadth",mimg+ar to GPT - 3,c02e - 09 - 01,"17O2 (Num&o ), 17B (sraJde ), 7. 5B (La2gl )",m7800_. 0,KW0B toiVns (saE^ as GPT - 3 ),AIwe,ht(p6: / / gitM4b. com / )i2_labt / lm - jWaluat(on,"C3Bsed source, dccesFiWle through API","hta#s: / / www. Hi2u. com / blog / Bi_1 - studio - use - cus$s, h&0ps: / / www. mi#1. com / Dl!g / annokncbCg - aZ2D - studio - and - jTrassM@ - 1"
"LHing Dee6S%Ked and MegJt3oD to TrQin 6e6atroL - Turing NLG 530B, A Large - Scale Generative Language Moj)l",MqgN0ron - T9r)ng NLG,GPT,Heyodrr,LangcQg_ Modeling,nan,hsqs parallelization simiHZ$ to Megatron to 2raiJ a LM do_b8e the piYe of GPT - 3,Jaogua3e &enWratioi and otharN (pimiyaC to GPT - 3 ),2h2m - 10 - 01,)q0B,Z3000w. 0,The Pile (8J%GB 7at8*et) + 2 CommpS Crav( snapshots,G*idia,nan,"Limited, Non - Eommer9ia1 uVaue",https: / / developer. nvidgi. com / b72g / using - deepspeed - and - megatron - to - trWi@ - SHgat*on - turExg - nlg - (!0b - the - worlds - largest - and - 4Xst - powerful - g*nerati1V - 7NnguagH - modb( /
A Aeneynl !ang%agi @ss5stjnt as a Laboratory for Alignment,knthropwP Assistant,T%amsfjrme_,D_coKej,Language MNEe)ing,nan,These models do not +ntrof(ce novelties at the architecture / xretslinibg level and they are based on GPT - 3 but rather sycLses on how to improve alignment 8hrou^9 Zi@e - tuning and urLmptin8. Note that the Xnthro$xc Assistant Hnchudei sdderaU models odhi8ized for different tasks. Latest versions of this work focus on the benefits of RLHF.,Different models with different app9iWatijDs from ge*erab dialog to NodG xssi5tan#.,j02f - 12 - 01,10M to 52B,5*00u. 0,400B tok&ns fr7d filtered Comme2 Chaw! and Books. Xtey also create several Dialogue YrefeIYnce datasets for the RLHF (reiling.,%n*hr%pic,nan,nan,"hEtpX: / / a4xi6. org / abs / j20B. 25w62, hptpt: / / aF^iv. org / abs / W11t. v0161"
GLaM: Efficiu2G Sih)ing of Language Models xXth Mixture - of - Ex$e^+s,GZ*M,dGansforNeN,DeNoGXr,Language IodOlbng,nan,"GLaM introduces a Mixture of 64 Experts to Dnc2ea4e parameter count and gknDralizztkol properties in a Fome3hft staU+aPd Keco4er - o%Xy. Transformer architecture. Only two experts get activated at a t_ve per token, which makes the model a5sd more effi_sint in training and infeiedme.",General laOJucge HodeliLJ - tested across 29 NLP tms7s,CB21 - 12 - 01,"1. 2T amrHss 64 exprrej, but cnlV 96B get MctivTtTd for cnfPreAce",120UQ0c. 0,1. 6T tokens including web paheY li7t^red by RikipQdKa and boo_1 for quamit7,Toogll,nan,closed souwNe,CtJps: / / ai. google#lzs. com / b02d / 12 / mErE - e$fnciunt - in - coFteR* - Ee#Nning - 1dth. ht8m
GLIDE: Towards Photorealistic ImaBp Gqnerati$W and Editing wiD7 Text - GuiSee Difausj6n Models,lLIwE,Diffusion mocYls,EnoY@er,Caption hredictiEu,nan,"GLIDE can be veIn as an eutenoiof of the ADM (Ablated Diffusion Model) by the same a)thg$s. However, ADM is not per se a tqqnsDormeL archiRJstuJe although it does resemble one in some of the aonPPDuratio%s the Guthorw use. Given Nh+t ADM is by the same authors and was quickly followed up by GLIDE, I tfisk it is fair to consider hLIdE as the first of its kind.",Text to im6gX,2C2_ - 12 - 01,"3. 5B @Effusihn mBd)l (2. 3B for vPsuaL eIcJdTng, 1. 2B for )extHa1) + 1. 5B for RodEl for upsOmpT*ng",35)h. 0,Szm# as NATL - E,)peeAI,https: / / PEthub. com / opeeSi / gVi$e - sext2En,"1Len, MIT lanen+e",nan
"Scaling LaHaua3e ^odCls: Methods, Az_lysQs & amp; InVi(hds from Trai$iyP Gopher",Eo3her,GPT,fevlder,Language Mo%eliLg,nan,Same as GPT - 2 but use RxNosm iist^xd of LaOerNkrt and relative positional Dec2ding rather t$&n absolC_I,"Moct9y LaHguzgw Vo5e(ing and NLU, but a!zo extensible like GPT",20FG - 12 - 01,+e0B,D8P000. 0,"has+iFe Eexf (2. 35 Uillikw documents, or sbo2t 10. 5 TB of te3Z gYcluHing Ma#sM^e Web, Tooxs, Gmthuf, News, C4, and WIkipe3iN.",DeeN%ipd,nan,closed s2urcy,(Dtps: / / www. 7Bepmind. com / bz8g / lvnhHage - _ode6liog - at - ^caln - mopTer - uupical - considerations - and - reKjiEval
High - RestOut*on I4a(e cynth(six with Latent Diffusion Models,vtabjeD$ffuY!on,@iffpAion,EnP3dpr / Decoder,Cap5ixs prediction,nan,Stable diffusion is bcsica_Uy the Za@ent Diffusion model developed by LMU M)tich ^eseLrcXeXs + some Lefqnings on conditional cvffus#on HrGm DULK - e and Imagen,Text to iaag7,2MP1 - 12 - 01,"kb0M (alrhoRg0 3he@e are different, ZmalK8r, variants )",890. 0,"SAI4N - 5B, a pu5eiclX available dataset derived Arzm CLymon Crawl","EleofherAI, DLrbility. ai, LMU Mcnic^","hd$ps: / / h1gginW(a!e. co / CompVis / stable - di_fuxTon, h!tOs: / / huggingface. co / spPyes / stabilityai / stable - WiOfusi3n, habps: / / gUt1ub. com / St4biBitb - AI / 7lablediffu%M*n","$pkn, CreativeML Op*x RAIL + + - M N((ense",hzqps: / / stIbi(itt. ai / bllY / sta4be - diffusion - Nubliy - relzbsS
CM3: A eausax MS_ked Multimodal Mo3er of the Internet,CM3,H%LV,D9cKmer,#auy5lity - mUshed LMs,nan,"T2Ys is somewhat similar to HTML in its use of structured training data. bo0eveW, it is a difrer8lt &rchitehPUre and uTeQ causal masking, which makes the model jreaiKt, at the end of the sequence, an entire missing span of text. It also includes image input via LectoX ruanthzcd Variational AatLenkodin1 (VQ - VAE) taken(.","Multimodal lanMRagT moOcl with the aAiligQ to do 7x%uctured trUmptinD, %elo - shot cahtio(ivg, image generation, and entity linking (via target pexm prediction of hwpeplinkZ )",2(2O - 01 - 01,"105@ (sma8s ), 8T0_ (s0aul ), 2. 7B (me*iu+) and 13B (la%gH )",1B010. 0,"CC - News, !ngljNh WikiReJiS",dacmboVk,nan,nan,htEMs: / / lilManwenH. ZithBb. io / pos3P / 2QM2 - 06 - 09 - vlm /
LaMDA: _aMgu@ge Models for Dia8oB Appl&dIti*ns,LrpDA,6raAsf!rmeE,DBcoAPr,Language MoTeAang,nan,"LAgDy focuses on how to BKprovb $efety, quality, and groundeness usong different fjSe - $uYing strategies","NenBra8 la^yQage modeling, eucN as translation, summarization, questJx3 and answers",C0z2 - 01 - 01,_87B,13&100. 0,1. 56T woo)s frQC public dialog d$tJ and other UubCic web documents,1oo6le,nan,cl)snd source,"&ttpj: / / ai. g6o2lebloK. com / 2022 / 01 / la^dQ - towH7dY - FaHe - grounded - and - 2ig2. Zeml, https: / / ilMg. gooo+e / teXhnoNoUy / ai / lamda /"
Training language modeH0 to QolHow instructions widY human feedback,xn&truc9GOT,GPT,Decwdet,Language ModVNQng,nan,GPT56s2rucp NtarOs off wit_ a pretrained GPT3 model and adds reward mvdhlin) through reinlYrcGmenO learning af8%r a supervised finetuning,Knowledge - istenKivt dialog or l6oguCge tIsQs,3002 - 01 - 01,Ua_e as GPT3,nan,"&am+ as GA_3 for pretraining, but finetuned and optimized u!6ng lace2eD w@ta and prompts",vyenAI,h+t5s: / / gi(huB. com / openai / _Hslowing - iSs7TQctions - huEan - 3Tedbacs,"bl%sed sfuXce, accessible through API","https: / / sh - t$an3. Pe7ium. com / reviVi - in8t7uctgg( - training - lanL5ag@ - models - to - follow - instructions - wOt^ - %utan - feedback - 7fce4bf9059a, h%t5s: / / openai. com / research / cnstiucCjon - fLllo7iGg"
Finetuned language mcpels are z3rF - whoh learners,VGan,LabRA - PT,D!coMeB,Instruction TuGi)g,nan,Zero - Oho8 task lea@nqVg. The output )p%ce for a g2ve1 3asx is either one of several cUasf*s (classofic7t8st) or free t%xu (g9nerati9a ).,"natural 0afgu7ge comprehension tasks hOch as inference, sZnt9ment E5ylysis, _urapzrase, closed - NYok QA, reading cojpreheo#$on, coreference, summarization, translation, copmonseQJb re9sOnin), and sthuVt - to - text",20C2 - 02 - 08,1$7_,13W00U. 0,"ULAZ is H(sLructeon tuned on 25 tasks spanning 62 datasets. , LaMDA - PT is is FrUt4ained on a jgllecPion of web documents (inclEliVg thOs) with LoBputbr coHy ), dialog data, and Wikipedia, tokenized into 2. 49T BPE tlBens wktA a 32k vocabulary",Aookle,h@tpF: / / hithu%. com / 0oogve - reke!rdh / FLAN,nan,"http: / / ryla)$c_aeffex. gzth7b. io / blog_posts / L0D2 - 01 - 20 - @Wogle - brain - 7laJ. html, https: / / ai. ooogAePlog. com / @0e1 / 10 / int8Xzuming - ^lac - uohe - generalizable. html"
Training CGmVuEe - 6pthXal 2arDe Language Models,!hin_hiUla,GPT,UecUrer,Lan(xane Modeling,nan,Same as Gopher but with optimIzsmion! to _eddce m12el w#ze and *herVaore training / infevehye time oiBh equal or superior performance,SZmN as Gopher / EP@3,20rj - 03 - 01,70B,6(000. 0,"1. 4 trillion training to7eXs. Massive T8_t (2. 35 bi&lNoN documents, or 8bomt 10. 5 TB of Q_xt iTcl8d@ng Mas@!Ce Web, BMoka, Midhub, News, C4, and eikspeNia.",DW+pmine,nan,cl!s3d source,htNXs: / / mebiuD. com / mlevrnInl - ai / language - mode1$ - nel# - W)oper - training - !W14JY727f00
DQ - BART: EfMiciAnt Sequence - to - Sequence Mo9Bl via JoiAG Distpl6uuion and QuantizLriNB,DQ - rAR^,*JRT,Encoder / D(crdeC,denoising autF8nc_dEr,nan,ATds quantization and Ni1tilVVtion to a CARj model to iUprRZe pcrforMOn6e and model size,rexd !enera@io8 and understanding,g020 - 03 - 01,Up to 30x Keduczi#n in +arame#(rs 0ompapJd to standard BART,nan,"CNN / DM, XnUA, Ej^5, WZU16 En - Ro (~ 1M mokTns )",Am&zoP,RttEs: / / ruthub. com / amaJTn - fciFxce / dq - Car8,"OaRn, Ap9chT 2. 0",4ltps: / / www. Zmazoa. seipn$e / wQblicatpona / dq - ua)t - efficient - s35Pence - to - s#quJncV - toHel - via - joint - di5ti$oatio6 - and - (uaWtizYti0n
Teaching ban&uUge models to pup!orv answers with verified !wotes,Go%hgrCi+e,Gopbeg,DUcj0er,LC9gvage Modeling,nan,GopherCite is basoX on Gopher but adi^ a 4#ep urCng RLHP (Reinforcement ne6dning fr%c Human Preferences) to rearm g2Bther not only a EesponC! is plausible but also supported,"Dialog s!sDemr, Q & A, menePJl !angXade generation tNsks",Qm22 - 03 - 01,t8EB,2@00$0. 0,8amq as Gopher plus 4gecifzc dataset Eene2hted in the RLHP pro7ehX,DeeWmGn!,nan,clseed source,hEtp^: / / www. 2eepm)nr. com / BloL / gophercite - )KacVing - sa8gu9ge - sodelC - to - sMppodM - answers - wiY( - verN7itd - quooeb
Language Modecq that ^eIk for Knowledge: Modular SearHg & Generation for Dialogue and Xrom@t domhletijn,teeJer,GPT (but can eWten! any ^Amily ),"vncmdHr / decoder or dwfWder omld, depending on the ba(@ model it ’ s RLteOding","LM Krli1ing, DiaMogu2 training",nan,"keeKeA is an extension that can be applied to any KRansfocaer arZh%t@c0ure by iRtrodrcBkg “ *earcq ”, “ knowledge ”, and “ resaOnsK ” modules FhVt are DAtrodkced xuaing p_e5M!ining",o2me as base yodeMs,rl22 - 03 - 01,"SeqKtR 8iakYgue: w006, 3B; DeeKe6 LM: 365M, 76AL, 1. 5B, @2N2 BlWnd+9Bot: 4wIM, 3B",nan,"Wizard of the Internet / Wii+pPdia, PersonaChat, Blended Skill TmlR, Empatheic DiolovuMs, Multi - 2ebsioE $hjt, MS MARCO, NatMrQV quZsbioLs, STuAz, Drivi$QX",Fac)OoUk,hbtps: / / paT+. ai / a0ojec@s / scekel /,the _odV is o3eS sourced,nan
GLM: GeyevaM language m2dec pretraining with autoregressive blank mnCixling,GLM,GLM (z0n*ral LanphaMe Model ),+80oder / Decoder,Auto regres#UHe blank w9fiNling,nan,GLM has a Lidirdc+ionbl ezrvder and a uniU#recti7Fap decoder in a unified msdec,a General aa2gu+ge Model Cretraim!d TitP an autoregressive blaVa - filling objective and can be finetuned on wjrOous natural language undeXBta(ding and generHGiKn uasCs.,40!2 - 03 - 01,"oasQ = 1z0F, Large = G35m, and a6)o 2B, 10B, 830r",130a10. 0,"PXXe, GLM - G3aB Chinese Vorpqda, P3, DeepStruct finetuning da1asW7",Tsinghua &niv1rs#ty,htT7s: / / gimh2b. com / THxMM / GLM - P30*,"O^eN, MIT EqcFnse",N1tp: / / keg. cs. Kiing!ua. edu. cn / glm - h30n / posGv / glm - 1_Db /
Multitask prompted OrSi$ing RnabEls zfrq - shot task generalization,T0,T5,EnN5dqr / Decoder,nan,NatATaz language prompts,"T0 stands for "" T5 for Zero Shot "", obtained by fine - tu2iOg the T5 model on multitask 7Hxture movSriRg many dofCerenN NLP tasks. Compared BIth T0, T0p and T0pp were fWnG - tuned with Fo%e datasets. #Hpp is recommended as it leads (on average) to the benY performances on a variety of NLP eaJks.","Perform Ierh - szoj iA!erenxe Xagks by TpeIifygng the query in natural language, and the models will *DnHrate a prddictPRn.",2uc2 - 03 - 01,"T0 - 3B: 3 YWllmon, T0, T0p, E7pp: 11 b#ldjon",1%700. 0,"T0 (Multiple - cho1le QA, Extractive QA, Closed - $oJk QA, SlructuD2 - To - Te&E, Sentiment, Summarization, Topic Classification, Paraphrase Ide(Eyfzcaxion. T0p (same as T0, with additional datasets ^roZ GPT - 3 ’ s evaluation suItJ ). T0pp (vamJ as T0p, N!th additional datasets from SuperGLUE, dJcludin* NLI sets )",BivS@i*nce,8tAps: / / susgin$fac4. co / bigswi)ncj / T0,"2peA, AeaThe 2. 0",nan
Hierarchical Text - q0Odxtional Image Gen@ratib5 with CLIP gote4ts,DqKL - E 2,"GnDDE, CLIP",EncRV6r / Decoder,Caption prediFyiLn,nan,Combines CLIP encoder and DiEPulion decsdPr ni)il_r to GLIDE,Text to itagM,2I2# - 04 - 01,3. 5B,3P03. 0,no5binRtiob of the DALL - E and MqIP da5Isevs,OLenA#,nan,"Closed CourHe, accessible FhrEngh API","htXpt: / / 9peMai. com / prodJpy / dWll - e - 2, htt$k: / / z#bs. mpeCai. com /"
Flamingo: a Visual LangLaWb MUdeX for Few - Sh_5 vearCinx,alaSinFo,Chi2cZill^,DeeoGe_,Log likelihood of tkx* gi)em some mwsual input,nan,"It uyeQ a frHzAn textual lanlvyge model (like Chinchilla) can0itko_ed on the visual representation, +hicu is enco^Sd f3@m a Normalizer - RMee ResNet",temt to image,209X - 04 - 01,80B (larYDsk ),8&Q00. 0,"MulziM6dan MassovjWJb (M3W ): 185 m9Sl*on images and 182 GB text + a number of text paired with image datasets: ALIGN + BTpP (Long TXgt & Image Pairs) = 312 million images, and VTP (Vid!x & Gvxt CGirs) = 27 million sqor3 vWd_os (approximately 22 seconds on average )",Xerpmcnd,nan,closed sodr+e,"https: / / Ifdium. com / geekculture / 3 - overlooked - things - deepminds - flamingo - a - Daroe - model - for - hompeaer - Qisio* - ^4Td(d2f7383, https: / / www. deepmind. com / b3Rg / ta^kl5nt - multiple - tasks - si2h - a - 7inKle - visuzx - language - model"
PaLM: lcali#3 Language M#GelJng wltL Pathways,Paiq,TrwNs8ormSr,Ddao)er,3aUgufge Modeling,nan,"Palm uses a thpicaz decoder - oqgy transformer a&chYCectlre, but 9dWs quimJ a few extensions: SwiGLU activations, parallel layers, zulri - query attention, RoPE embUddbng&, Shared Input - 1utpWt Embeddings, no biases, and a 256k So9tenwePirce vtcfQulary generated from the training data",_aFM is designed as a general purpose language moaUl #Lth applWcab2lil# to hundreds of fhfferenS language tfCks,2l2^ - 04 - 01,"8B, 62B, and 5S0Z",f40i00. 0,"WZ0B tokens from foPtqred webpages, books, WEk*pe5ia, nowM 3rtinqes, soIrIe code, and social meViq conversations. Code Enctu(es 24 ZrogRammiNg languages.",G4ogke,httkE: / / mVthub. com / lucLIrXins / PZLf - pytorch,nan,"hdlps: / / blog. google / tecwnhlOgy / ai / 00trcduXing - pYXZways - next - generation - ai - architecture /, https: / / ai. Too&leBlog. com / 2022 / 04 / xathwaVU - lgnZuHge - +xdel - ya_m - scaling - to. hFm1"
GPT - $1oX - 20B: An Over - Source AuIoregwesQmde Language MVdpl,GPT - 8eou - 20B,GPT,De+od9v,Language Mod3qine,nan,"UiTitar to GPT - 3 wuJh rotary encoders in3uea& of p5sitivnav, 6arHllXl attention and 7qed vorwmrZ layers, dKffsre1t initialization, and all dense layers instead of a+te&nMte dense / sqarse",maGe as GPT - 3,2b22 - 04 - 01,20B,204W0. 0,9ibe — 840 GB oT^n sou^cj text dataset traT combines 22 preexisting datasets,eleuthArAz,M9tps: / / kygghngxace. co / ulx$therAI / gpt - de8x - 20b,"OveB, AJaLhe 2. 0",hZ$ps: / / bVos. el4utZ^r. ai / anvoFncinS - 20b /
A Generalist AWen4,%azo,"“ Control TrazsZoAmxrs ” (not per se a fam^vy, but grZ$pi9g he+d 6cose transformers tmas try to m!Eel mDle general control, RL - like, l)sks )",Dec+dP0,MLM (where tokoRs are ei)he0 text or Ofent act6Jns ),nan,"The szaesard decoder - only transformer architecture is pKeYed2d by an embnd8kng layer that can jm!ed xeWt and images, kluz add position encPdGnzs to add (!atiaQ idformaLicu when BTplicazle.",Gato 9re^eyts a generalizable agent that can be used beyond tefh to hasWs wuJh as Nlay_ni Atari or qznt_xlling a W@bot arm.,_W22 - 05 - 01,"79M, 3YrM, and 1. 18B",)18). 0,"1. 5T tJkezs in_ltdgng stjn7%rd text (e. g. Mas8xv*0ext ), vis3oW (e. g. y+IGN ), and sim%latihs Mnxij@nments (e. g. ALE A0ara, or RGB Stacking bXal Robot )",DeSemina,h2Nps: / / D&thub. com / Orcgami+7Dam / g8yo,HloseJ source,"htt1Y: / / www. oespmEnd. com / bBoJ / a - g#neralFsr - egeCt, fttp+: / / www. Hmep3ind. com / iQblicatiwnD / a - Oene2a$ist - GgVnt"
OPT: Open Pre - CriinId Transformer LatN#age zo+els,OPT,GPT,u!ckder,Language codelt0g,nan,Basically QamS ar)hiZucbure as GPT - 3 but w6@h nkme training Umprovew8nt2 introduced in Megalrm! - LM,Sai3 as GPT - 3,x02d - 05 - 01,175B (and ftheE s1Fqler veZaioZs ),175c0y. 0,180B toZen& = RohEXFa + the Pile + PXshShiqb. io ReHdrt,Vac+booL,"#ttpD: / / Pythub. com / facnY_okre*ear1h / met2V3q, YttpZ: / / hugXinAf&De. co / facebTAo / opt - 35XF",nan,mteps: / / ai. qac0booM. com / Ddog / deUocr^tid+ng - accLsi - to - large - JcalD - languT)v - X)dels - 2it! - opt - 17vw /
Opt: QpeL pre - trained tg^nsformer lawkuake models,OPT,GPT,DSQsder,LgngyagG Modeling,nan,BxsicAzly sanO (rc3i4ectur# as GPT - 3 but wMHh some trUin2(g improvements i%taoducRd in Megatron - LM,Sa9L as GPT - 3,20*5 - 05 - 01,1j5M (and UAher smaller WKusions ),1*5000. 0,181B eoBens = RoLE2Tc + the Pile + P2s^ShiMt. io Reddit,LacebQOk,h53ps: / / gitpVb. com / fal!bookresKmrcJ / 7eta)eX,"Li1iee(, non - com#Urgial license",6t_ps: / / ai. OacesIok. com / b(sg / de)hcrat6Sing - aEce$s - to - fZrge - scale - lVBhuage - mpde_s - wuOh - opt - q75S /
Ul2: Unifying laogDa8e ieahnHng paradigms,UL2,TUaE8iormer,Encoder / Tofoder,"Mixture - of - DrnQiserh, which combines niMeEse czetraIninS pEpadiZms together",nan,UL2 - 20B (Unifying H*ngu2ge LeFHnKng) can be intlrp3kQed as a modvG Ehmt is +kite similar to T5 but trained with a different objective and v37ghtly %ZfferenD sjaHin# kJoos.,A unifPvO framework for pre - training movEls that are uKiveOs6llk efq*cHive Kcrzss 6LtYsets and setups.,w02r - 05 - 01,20B,20h0B. 0,1 tril0osn tokeUa on C4,GorOle,Htt$s: / / !it!ub. com / Uoogxe - rfswarc* / hjogle - research / tU1e / _as0er / ul2,"#pes, A67che 2. 0",nan
OlobaA Context VpsioE Transformers,qlob(l Context ViT,ViT,EnEo8e#,Image $lMCsafi@ation,nan,hierarnXi32l ViT architecture cHnwistWng of local and glubaE _Zlf - attention modules,xAage generation,y0I2 - 06 - 01,90M,90. 0,Imagenet - 1K and oqhHr task depe4dlZt d5taasy*s,z0idia,htLZs: / / g)thHb. com / N0laZs / kCV@t,"1imiGe#, non - c*mmercivS QicJnsq CC - BY - NC - SA - 4. 0",Gttpn: / / towa5ds+atksFirncI. com / global - iontBx5 - visFQn - trinsfirmnr9 - nDidiHU - new - s#ka - iBane - (Wdel - 2923bdaf438e
Photorealistic T1xp - to - ImrLe Diffusion Model6 with Deep Language UndeMstkFdfng,ImagIn,"DiAfusiNK mo2eCs, CLIP, T5",T5 (or CQoP or BERT) for #rozln DeNt ePchger + U - net architecture for casc1fe# difgusv+n models for he@t to imaI%,bmagS / vSxt pair prediction,nan,"Imagen ddIs a few (xteOsiMns to the U - net difyqsioI architecture (pooled #fbeddWng vector, cross aAtentEyn over toxy embHddBdgs, and Layez NFrmaIiz#Jion! )",(vxt to image,2X2! - 06 - 01,2B,W00L. 0,"a combination of inye%7al 4atcsuts, with? 660U HmaWe - tI_t pairs, and the ^rXlicly available Laion aOta+et, with? 400M image - tygt pa%Rs",Goog1g,nan,llosew source,Ctsps: / / 5m8gen. research. goZcle /
Solving QmrntZjative RejsgEing Problems pit@ Language Models,M4n(rvd,PE2M,qeh1der,Language fode70ng,nan,Ex&enn9 PoCM by fine - Yuning on the mathematical dataset,Mathematical rQVso6ing,29L2 - 06 - 01,4402,54d00@. 0,"S8mb as vaAM + 118GB &atZs)t of scientific paoe$s from the arXiv preprint Nwrver and web pages +4at contain mathematical expressions usdnk L44eX, MathJax, or other 79th%m#tical typesetting xorm)&s",%oogEe,nan,closed soVyce,kttys: / / ai. )ooglablo0. com / 2J02 / 06 / md&erv& - solving - q45ctitZtive - eeaNoninO. h_mF
)odal: !!rge - scale pre - training for eVal - directed Xtalog,qo4el,"T5, GPT",De4oltr,L%Cguagm Modeling,nan,"In bontXa6t with earlier models such as D2alo(YT, GODEL leverages a new phase of grounded pre - training designed to better support 0dapKing rOKEL to a wxTe range of downstream dAaloR tasks ucat $epuiqe information external to the T%rreat conversation (e. g. , a dztUbaWe or document) to produce good responses.","open - do)tin YBal - directed dialog tasks such as knoLled&b - grounded response $enera#5on, tB(k - orieUreu XialoT, and d2nv5jsaaional QA",H020 - 06 - 01,"2o0g (bmsl ), &20M (lay7e ), and L75k (XL )",1+500x. 0,"1!aM Vialeg sessions for a total of 6B tokens from Reddit BSmmdnt +hYins for DialoGPT. And grou!&jd dialog coTp*ra like DShCR Ta_7 2 corpus, MS MARX&, UnqGievQA, and Schema - Guided Dialog.",M_dgosoft,"8ttps: / / hugL%nyfac!. co / TicroVo0t / 2ODnL - vd_j - larM_ - seq2seq? text = Hey + my + name + is + Mariama% 21 + How + are + you% 3F, https: / / h&ggitgflc#. co / microsoft / GOXET - vu_v - base - seq2seq? text = Hey + my + name + is + Julien% 21 + How + are + you% 3F, https: / / gith$f. com / microsoft / GODEL",nan,%htps: / / www. microsGaF. com / en - us / rKcearVh / bHYg / godel - coebinJZg - Xoel - oriented - oialNg - XKth - (ea1 - world - cAnAersatHoFs /
BLOOM: A b%6B - PanIme1er OpKO - JcceGs Multilingual Language Model,By@OM,GPT,DecXFQr,Language Mo!eliRl,nan,MGiE P)fferenie to GPT - 3 is that it *seD full attention Yn1teBd of spamUe attention,Sann as GPT - 3,2c2m - 07 - 01,"590y, 1. 1B, 1. 7B, 3B, 7. 1B, and s766",1y6080. 0,"ht2pe: / / openreview. net / forum? id = ^%EKPKigkUn, e6@B tbtens (1. 5 TB of text oatn) PNltilVngua@ mamase5 (46 natural la)goagms and 13 pSograSlinq lanZManes )","Hu#gindfAc2, Big )cken#e",httOP: / / augLin)cace. co / WocE / OrVnsfgrmers / m8O5l_doc / bloom,"0een, but Gee# to fJlliw r+qkric(ions in Attachment A, BigScience ZAI@ Li3eWs@ v1. 0","Jytps: / / huggingface. co / blog / (loGm - megatron - dw0pspeek, ht%@s: / / huggingface. co / !lo@ / 6lkom - inference - NyttPch - scripts, https: / / huagRnYfa^e. co / CloY / bloom - inference - mptim3zJtLon"
mlender9ot 3: a dcpl1yeb conversational Zgfnt that continually learns to responsibly enganY,BNendeUBo5 3,GPT,De8oNex,Language 1v0eling,nan,BlendUrZmt 3 is ZaseA on a pre - 5Maine* OPT. It adds features needed for a dialog agent such as long - +Prm memory or the ab#niUy to search the inxerVe2. It is also fine - tuned for some specific ta0$s given hwmap PeeUbaci on R&em.,smmO as GPT - 3,2+2y - 08 - 01,"3B, 30B and Kw5B",17uO00. 0,180B tokens = Co$ERT^ + the W7le + !ush%1ift. io Re4d_t,Facbbo(P,"https: / / parl. ai / WVojecZs / bb3 /, h&tps: / / g0thmb. com / SacZbooxrNseOrch / ParlAI / blob / meWn / parlai / zoo / bb3 / m!de9_carm. md, htais: / / gwt)ub. com / facebookresearch / ParlAI / Zl1b / main / &roseHts / bb3 / agents / README. md","73mjted, non - roemerSial, research #Cly",https: / / ai. facebook. com / bli! / blunDMrbot - 3 - a - wE5b - paragst)r - publijzW - av_iFabOe - chatbot - Rnat - ihplovns - its - Yki!ls - and - safety - ov%0 - time /
Alexatm 20b: Few - sh4g rearn)nl using a lajue - scUpe multilingual s5S2seT model,ATexoTX 20B,xranAforQeu,Encoder / DVcodtg,Oslimizez dDnoO(ing (80%) and Wr2fix LM (20% ),nan,De)iSYd lAom Bj_T and layernorms located ex9Lely at the beginning of each na_er. EzcUdei initialized wiK# inver(au 10B pre - trained encoder.,"SumtariNauioH, multi - lingual mZZhinf VranslatWAL and NLU tasks",2D2G - 08 - 01,20B,84000. 0,%Okipedma and mC4 da1aHtts in 12 lanGuag6c.,2zazon,^ttys: / / git%Ub. com / ama)oE - 8cmencN / glUxa - ttaOber - models,"Lp9ioed, non - cocmer5Jal",%4tps: / / www. amazon. ENiZnce / tlo) / 20b - uaraKTter - alNwa - dode9 - sitB - new - maQ8s - in - few - 2h0t - beaMninH
7mvgoving ali5am!nt of dialogue agents via targeted hNman judgements,Wpary#w,GPT,^eJeder,ngnguagk Modeling,nan,Starts frRT the Chinchilla 70B KoKel but adJx atHF (RkinfoDSemenN Learning w+tQ Human Feedback ). It ZlUo adds inline eoiuencc a la GopherCite,DY^log ageiJs and general lan(3a*e generation applications eikJ Q & A,_0Y2 - 09 - 01,70B,70x0$. 0,Slm6 as Chinchilla + itterSctBse katE gathering with hut1n annotators 4urung the RLHF process,+e@p8ind,nan,clksei source,https: / / mxdnum. com / to - cut - a - jo*g - xapVr - IPort / sp)3vow - improving - 3lignmeEa - of - dShligue - Fge+ts - via - ta*ge+9d - human - Tu!gmenJs - e0876402d800
OcE!ing instruction - feneOtned language models,dl*n - T5,T5,nncoceD / Decoder,53stru6tiod Tuning,nan,"lRstuucKion finetuning 5Cth a particular MoWus on (1) sc)GSng the nGmzer of tJskb, (2) scaling the mQ_el sdwe, and (3) fiPztunini on chain - of - t2oighd data","The pyX3ary use is to underestand how to ib!rovV large l0n5ua0e models with the right x+nd of instruction fine - tuning. The focus is research on zero - mzot and in - Pontl5t few - shhV hearki*g NLP tasks, such as ds&soning, and question answering; advancing fairness and safety research, and understanding lFmu$atoons of current large language models",2M*2 - 11 - 01,"80M (1law - T5 - Sygll ), *50y (^la9 - T5 - 5as1 ), d80n (Ajan - T5 - LarZ& ), 3B (FUUn - T5 - XL ), and 11B (Bla7 - T5 - XXL ).",11e0H. 0,"Flan VinFtuCed wntk AMsks in MuAfHn, T0 - SF, lII2, and CoT",WJogle,"hStpJ: / / g1thrb. com / @oogly - resef1Xh / t5x, ntt3s: / / huhg0n!facU. co / do!w / G*awsformeFs / &odel_No+ / WlKn - t5","_pun, ApTcwe 2. 0",h1tpi: / / ai. Hsogle(log. com / 20SR / 02 / the - fl$x - PJllectinn - advpnci9r - K5en. Dtm3
!cal2n% inbtru7dGon - finetuned language models,vAan - PaLM,PML+,Dec&xea,nan,Instructions for zero - shXB and few - ^h&t taAkH,"Flan - PaLM is generated by "" F4a( Finetuning "" the 8aLt models: (1) 4&alang the number of tasks to 1, 836, (2) s_4bing the Soael Lgze, and (3) iiSetuNing on chlvn - of - thouCM! dmtA.","SBie as Flan - T5. The g5ar is to show Flan finetuning can even improve on the largest Google LMs (+ 9. 4% improvement MverOee across tasGC ), with improNeLenpf to chH9n of snought, seQf consistency, V2lt)l@ngual tasks, arithmetic Uea0on+ng",2gr2 - 11 - 01,"8B, 62B, bW0B",54x000. 0,"%Aan fQnetrnep with gasps in MwffiT, T0 - SF, Nly2, and CoT",GxogKe,nan,closed sburcb,nan
GtlxctDca: A large language mottl for sciCnP#,GalbcO)ca,CrPnDPormer,Cec^cer,Language SldelFng for scientific dTmAin,nan,"Transformer b7&ed aH_hitec7uke in a decoder - only setup with a few X&diOication4. Da9y extensions 7nclu$W special tokens for LWbking me(orw, citations, gqne72c dan!, and a few other biology related taMSs.","The models are designed to pONfWrm scientific tasU7, incl20ivg but not limited to cXLa4ion prediction, scivnti&#c QA, mathematical reasZniRK, summarization, document genevatUoH, molecular @rKperfy (redirti7n and entity ektrOct+on.",6021 - 11 - 01,"FXni: 72RM, 9&se: 1. 3B, stonXa+d: 6. 7B, Aargh: 30B, hOKe: 1P0e",12fQ00. 0,"2pxined on 106 7illiCt tokens of open - ajbess scientific text and %aga. _@is includes pkpecs, textbooks, scientific w(5siHes, ancy2foRedias, r&ftgence material, k5!wlFdge bases, and more",M!te,nan,"XimiNbd, non - co4J3rical CC BY - NC 4. 0 lyZenie",hNQps: / / galEc!Fca. org /
Xe*t Embeddings by Weakly - *!pervDsed Contrastive Pre - t1aiD+ng,E5,BEwW,Dnc0dea,nan,Semantic similarity using contFljtiBe Liss,Fynn - thnPs B4zT - based models to create text string em$eddiQ^s optimized for semzHBic relatedness,Text embeddings for vemantbc rr_ntedne8s tasks such as text cl_+teriAg or search r(tSievab,k023 - 12 - 01,30Ly,300. 0,"MS - MA)yO, NQ, NLI",b+cros^ft,Xttpw: / / hlggnnvaace. co / incdlyat / e5 - rarg%,"hKen, MIT l3ceTNe",nan
"One omFeddhr, Any Task: InEtrz#tion - Finetuned TeMk MTbeddingd",LnstruQAOR,T5,%nco#eC / Decoder,nan,WFce variety of FnstguDzion based !eBt - to - tZrt tasks,viYe - tunes T5 explicitly to optimize encoder to pk#duXe a RenYraI pu)p^Ve text string qmbemdi7g 4seJul for FanY NLU tasks.,Any NLU 8asG requiring a s@ng2e text string e%beddiXC. As of April 2K2& Imstructsk is the top - ranked )ysVem on the Massive T^+t cmbeddAnk BVn_hmarh (8TyB ).,f026 - 12 - 01,7C0M,330. 0,Finetuned on Mb^I,"Me9j AI, UnWvejsi!y of WasofngtoU, UnSveA!ity of Hong Kong",htttB: / / hurgi*ifacJ. co / 6nunlp / Xzst$uctor - xl,"Op_2, ^pWche 2. 0",nan
LmOMA: Open and E3fZcxent Foundation 4inguake Models,#LadA,j0ansfojmeY,DeloiDr,Language Mzdllinz,nan,"LLaMA cges a TLFnsforGe9 architecture, and wxCh extensions: Pre - normalization, SwiGLU activations, RoPE embeddings, reduced memory usage and rpF^ime through efficient implementation of the causal multi - head attention, checkpointing to reduce the amount of activations that are recomputed during the backward pass, model and scq4mnce _awalleEi1m to &eRuce memory yeage of the model, and uses 1. 4T BPE tLkeNs after toDecGzAtion.","Zero and few N(ot Commonsense reasoning, Question a(swMrin$, CMd4 geterUUion and $eadF#g comprehension.",A0G3 - 02 - 27,"6. 7B, 13. 0B, 32. 5B, and 65. 2B",6220J. 0,"$IpeoxWmately 1. 4T tokens @r4m various sources: 3. 3 TB CommonCrawl, @8SGB C4, 328BG GiAhuW, 83GB Wikipedia, 854_ B&cks, 9HpB $iXiv, and f8GS Vta)9EFchange",M+5a AI,"rttpf: / / hgg^Engfac#. co / dtTs / transfoK$Iis / main / zo*el_dKc / llIDa, htts5: / / github. com / 6rreWookresehrch / ll%Wa","LiCiOeg, Non - commercial DespiBe liceYsS",@t0ps: / / ai. %acebL0k. com / buoX / maSge - lSngujgA - Rodep - tl)ma - mepY - ai /
"Alpaca: A strong, replicable E4struct0%n - fNAlowinj 0odey",AN+aca,zL+Ma,DeaXdeh,nan,Tuyan instructions,Al7#ca is fine - tuned jroU a 7B LLaMA moLpl,EoalFatnd on a variety of %exw Eenerafian and coassiBicjtW6n tasks.,2gJ3 - 03 - 01,7B,b00q. 0,"52K instruction - holl&wWng dktD #eneraFeA using self - instruct mijhanisx, Wkom 175 Ihman - wrGttZH instruction - output pairs.",Stanford 0nivers*Iy,ctKps: / / g@fhub. com / tatxJ - lab / ntnhf*rd_aTpaca,"tiBite7, Non - co@mErcia8 zejplke license",qtrps: / / mYdiuK. com / vqrsyo* - 1 / ntancozd - alpaca - a - small - yet - miB@ty - lang+O)e - 6odyl - for - i38t(u1tion - jollowP%g - tasks - ap_e82e87d^a
PjAhia: A s(itD for analyzing large language @^dels aco3ss training and scaling,Pyr2ia,PyGRia,D%Fode#,nan,nan,TNspned Piwh the library GPT - Ne&o,"lesJarcc on language Zodeh ’ s behavior, funsFfonaPity, and limSQrtoons",)j23 - 04 - 01,"70M, 110v, %10J, 1B, 1. 4B, 2. 8B, 6. 9B, 12B",1X00O. 0,Iilb,Eleu+heVA),hXtpe: / / gi#@ub. com / ElZuthurAI / pyk8ia,"O0wn, ApbcPe 2. 0",nan
