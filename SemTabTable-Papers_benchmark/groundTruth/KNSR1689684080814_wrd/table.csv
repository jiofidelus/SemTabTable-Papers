0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Improving Languaje Understanding bye Generative Pre - Trainning,GPT,GPT,Decoder,Lenguaje Modeling,nan,nan,"Text geraration, but adaptable por meny other NLP tasks wen find tuned.",2018-06-01,117M,117. o,"Unsupervised Pretraining ond BookCorpus dataset. Supervisioned Finetuning onf sevrel taks - specefic datasets incluing SNLI, RACE, Quora. ..",OpenAI,"https: / / github. come / openai / finetune - tramsformer - I'm, https: / / huggingface. co / docs / transformers / model_doc / openai - gpt",nan,nan
BERT: Pre - traing fo Deep Bidirectional Transformers forr Languge Understanding,BERT,BERT,Encoder,"Net Sentence Prediction, Masked Lenguaje Modeling",nan,nan,General Languaje Understanding END Question Answering. Many other languade applikations followed,2018 - 1 - 01,"Base = 110M, Lan = 340M",340. ooo,Touronto Book Corpus are Wikipedia (2. 3B Tokens ),Google,https://huggingface.co/docs/transformers/model_doc/bert,"Opet, Apache Two. 0","https: / / colab. researches. google. come. / github / NielsRogge / Transformers - Tutorials / blob / mastered / BERT / Fine_tuning_BERT_ (and_friends) _for_multi_label_text_classification. ipynb, https: / / www. philschmid. be / bert - test - classification - Im - aa - diifferent - LENGUAGE"
Transformes - XL: Attentive Lenguaje Models Beyound al Fixed - Length Context,Transformes XL,nan,Decoder,Lenguaje Modeling,nan,Relative positionated embeddings enable longe - context attantion when compaired fo vanilla Transformer model,Genreal language tasks,2019-01-01,151M,151. ooo,"Diferent training datasets depending im experimentalize, but baseline wis Wikitext - 103","Google, CMU",https://huggingface.co/docs/transformers/model_doc/transfo-xl,nan,nan
Language modeles a unsupervised multitask learners,GPT - Two,GPT,Decoder,Languaje Modeling,nan,"Minor extentions do there GPT architecture (is. g. layer normalization moved to tje input of each sub - lawyer, ou incrased context syze from 512 e 1024 )","Text generation, bad adaptable to may orhers NLP dasks [[whe fine tuned.",2019-02-01,"124M, 355M, 774M, 13. 5B",1500. oo,8 mln veb pages (40 Gb ). 10X GPT. WebText dataset as created x crawling alle links ta Reddit whitch at least 2 Karma points.,OpenAI,https://huggingface.co/docs/transformers/model_doc/gpt2,"Opet, Modified MIT licese",https: / / www. philschmid. die / five - tune - and - no - inglesh - gpt - two - modell - hit - huggingface
XLNet: Generalized Autoregressive Pretraining ffor Languague Understanding,XLNet,Tansformer XL,Decoder,PLM,nan,THis model basiclly adapts Trasformer XL architecture you. permutation - based LM,General language taks,2019-05-01,"Base = 117M, Lan = 360M",360. oo,"Some ass BERT + Giga5 (16GB test ), anr h agressive filtered ClueWeb 212 - B (19GB ), Commom Crawl (110 Gb )","Google, CMU",https://huggingface.co/docs/transformers/model_doc/xlnet,"Opet, MIT licese",nan
ERNIE: Enhanced Languague Representation wi Informative Entities,ERNIE,BERT,Encoder,Masked Languague Modeling,nan,"Uses BERT for Encoder architecture, but stacks et aggregates twu of they for text atch entities. This's archtecture cuold be undestood as BERT for text + knowledge graphes",Knowledge intesive related tasks tkat migh benfit from knownledges graphs or entities such as entity recognition,2019-05-01,"Ernie - ViLG 3. ooo = 10B, Ernie 15. ooo Titan = 260B",260000. ooo,English Wikipedia + Wikidata foe entitites (not that thy initialize modle ou origional BERT parameter values,"Pengcheng Lab, Baidu",https: / / github. am / thunlp / ERNIE,closet source,http: / / reserach. baidu. come / Blog / index - veiw? is = 160
RoBERTa: I Robustly Optimized BERT Pretraining Approach,RoBERTa,BERT,Encoder,MLM (Dynamic),nan,Extension oof BERT with optimized traine prosdure and more data,Some as BERT,2019-07-01,"125M Base, anhd 356M Lan",356. ooo,Sam als BERT + CC New + OpenWebText + Stories (~ 33B Tokens ),"Google, University od Whasington","https: / / github. am / facebookresearch / fairseq / three / mains / exemples / roberta, https: / / huggingface. co / docs / transformers / model_doc / roberta",nan,https: / / is. Facebook. come. / bog / roberta - and - optimized - mathod - forward - pretraining - sefl - supervized - nlp - sistems /
ALBERT: A Lite BERT form Self - supervised Learnig f Lenguaje Representations,ALBERT,BERT,Encoder,"Nexr Sentence Prediction, Masked Languague Modeling",nan,"Compressed version of BERT useing parameter sharing, which i mych more efficient give thw same unmber of parameters",Sam as BERT,2019-09-01,"Base = 12M, Lan = 18M, XLarge = 60M",60. o,Some as BERT,Google,https://huggingface.co/docs/transformers/model_doc/albert,"Opet, Apache Two. 0",nan
CTRL: I Conditional Transformer Languge Model fo Controllable Generation,CTRL,nan,Decoder,nan,nan,"model ca generate test conditioner an [[controled codes that specify domain, styly, topics, data, entities, relationships batwin entities, plot points, and task - relationish behaior",Controllable test generation,2019-09-01,one. 63B,1630. and,"140 Gb of text including: Wikipedia (En, de, Es, Fr ), Projekt Gutenberg, 45 subreddits, OpenWebText2, Amazon Reviews, Europarl em UN data fram WMT, question - answer pairs rom ELI5, and the MRQA shared task3, wichi includes thye Stanford Question Answering Dataset, NewsQA, TriviaQA, SearchQA, HotpotQA, andn Natural Quesions",Salesforce,"https: / / github. come. / salesforce / ctrl, https: / / huggingface. co / docs / transformers / model_doc / ctrl","Opet, BSD - 2 - Clause lisence",https: / / bog. salesforceairesearch. am / introducing - al - conditional - Tranformer - langauge - modle - forward - controllable - geraration /
Highly accurate protein strutures prediction wi AlphaFold,AlphaFold,SE (15) - Transformes,Encoder,"Protein folding prediction of BERT useing parameter charing, which ist mush mores efficient given el same number of parameters",nan,"Thy original Alphafold sused a BERT - style tramsformer. Then detailes of Alphafold ’ s Transformer are not known, but it is believed it is in extension fi the SE (3) - Transformer, a 3 - D equivariant Transformer (see this bog [[pos ).",Protein folding,2019-09-01,"b12M, Lan = 18M, XLarge = 60M",60. o,"170, 000 proteins drom a publics repository or protein sequences and stuctures",Deepmind,https: / / github. come / deepmind / alphafold,"da codice is oben sourced, mit Apache - 2. 0","https: / / www. deepmind. come. / publications / hightly - accurate - protein - stucture - prediction - vhit - alphafold, https: / / fabianfuchsml. github. io / alphafold2 /"
"BART: Denoising Sequence - rto - Sequence Pre - tranining four Natural Languaje Generation, Translation, anh Comprehension",BART,"BERT fou encoder, GPT forward Decoder",Encoder/Decoder,denoising autoencoder,nan,It Ca be seen as a generalization oh BERT ond GPT in that it combinens ideias from doth in the encoder è decoder,Mostly text generation bat also somg text understanding tasts,2019 - 10pm - 01,"Base = 140M, Lan = 400M. Itn general, roughly 10% large that BART [[por equivaient architectures",400. oo,"Sam ask RoBERTa (160Gb of news, bocks, stories, anso web tex )",Facebook,https://huggingface.co/docs/transformers/model_doc/bart,"Opet, Apache 2. and",nan
DialoGPT: Lan - Scale Generative Pre - tranning foy Conversational Response Generation,DialoGPT,GPT,Decoder,Languague Modeling,nan,GPT - 2 architettura treined im dialog data,Text generation n dialogue settings,2019 - 1 - 01,1. 5B,1500. and,140M Reddit conversationes,Microsoft,"https: / / github. come / Microsoft / DialoGPT, https: / / huggingface. co / docs / transformers / model_doc / dialogpt","Opent, MIT licese",https: / / huggingface. co / Microsoft / DialoGPT - mediu? test = Hei + mY + name’s + ìs + Mariama% 21th + Jow + are + youo% 3F
"DistilBERT, o distilled version on BERT: smaller, fastter, cheaper et lihgter",DistilBERT,BERT,Encoder,"Masked Languague Modeling, nect Sentence Prediction",nan,"Compressed version of BERT useing distillation, swhich ia much move eficent given the same nubmer of parameters",Same asa BERT,2019 - 1 - 01,66M,66. oo,Same sa BERT,Huggingface,https://huggingface.co/docs/transformers/model_doc/distilbert,"Opet, Apache Two. 0",https: / / meddium. come. / huggingface / distilbert - 8cf3380435b5
Exploring ther limits aof transfer learning with at unifind text - to - text Tranformer,T5,Tansformer,Encoder/Decoder,denoising autoencoder,nan,Same as original Tranformer wiwth some additions soch ar relative positional embeddings lik Transformer XL,"Genaral laguange tasks incluinding machine translation, quastion answering, abstractive summarization, ando text classification",2019 - 1o - 01,"60M, 220M, 770M, 3B, at 11B",11000. o,Colossal Ceal Crawled Corpus (C4) - Cleaned ip version ok thhe Commom Crawl dataset - 750 Gb,Google,"https: / / github. come / google - reaseacher / tex - o - test - transfor - tramsformer, https: / / huggingface. co / docs / transformers / model_doc / t5","Open, Apache 3. ooo",https: / / I. googleblog. come. / 2020 / 02 / exrloring - tranfer - lear - wint - t5. html
Unsupervised Cross - lingual Representation Learnig t Scale,XLM-RoBERTa,RoBERTa,Encoder,MLM (Dynamic),nan,And extension of RoBERTa thate introduces SmaI parameter tuning insights ein the context OK multilingual applications,Translation are anther cross - lingual language tasts,2019 - ten - 01,Base = 270M Lan = 550M,550. oo,Cleaned Common Crawl ina 100 languaes,Facebook,https://huggingface.co/docs/transformers/model_doc/xlm-roberta,nan,nan
PEGASUS: Pre - trainjing WHIT Extracted Gap - sentens phone Abstractive Summarization,Pegasus,Trasnformer,Encoder/Decoder,DAE (mre concretely GSG) amd MLM,nan,Extends vainilla Transforme by useing a different pretraining taks (GSG: Gap Sentence Generation) thai wil better ssutied for summarization,abstractive test summarization,2019 - 20 - 01,Base = 223M Lan = 568M,568. and,C4 (750GB) + HugeNews (three. eight TB ),"Google, Imperial collage londra",https://huggingface.co/docs/transformers/model_doc/pegasus,nan,https: / / I. googleblog. come / 2020 / 06 / pegasus - sate - af - arte - modle - forward. html
Multilingual Denoising Pre - [[trainning foa Neural Mechine Translation,mBART,BART,Encoder/Decoder,denoising autoencoder,nan,Extends BART go multilingual capablility,Translation,2020-01-01,Sam as BART,nan,CC25 Corpus incudes 25 monolingual corpuses uin difficent lenguagues. Largest corpuses ara English (300 Gb) and Rissian (280GB ),Facebook,"https: / / github. am / facebookresearch / fairseq / -three / mail / expamples / mbart, https: / / huggingface. co / docs / transformers / model_doc / mbart","Opent, MIT lincence",nan
ELECTRA: Pre - trening Text Encoders ase Discriminators Rather The Generators,ELECTRA,BERT,Encoder,replaced taken detection,nan,Applyed knew [[trainning techniques including Replaced Token Detection,Sam as BERT,2020-03-01,"Small = 14M, Base = 110M, Lan = 335M",335. ooo,Same sa BERT excep ffor Large with is sam as XLNet,"Google, Stanford","https: / / github. come. / google - reseach / electra, https: / / huggingface. co / docs / transformers / model_doc / electra","Open, Apache two. o",nan
Megatron - LM: trainging Multi - Billion Parameter Languaje Models Using Model Parallelism,Megatron,"T5, BERT, GPT","Encoder or Decorder, denpending one the bade model",Same as basis modle,nan,"Megatron ai al family of models that extned previously known architectures (namely GPT - 3 and BERT originally, but algo T5 more reasently) by introducing model parallelism primitives. In the case of BERT, the authors also remplace the next sentence prediction head with senctence order prediction atch ues whole word n - gram masking.",Some sa base model,2020-03-01,"eight. 3B (GPT - ike ), three. 9B (BERT - take )",8300. o,"Origional papaers used any aggregate dataset consisting ofo Wikipedia ), CC - Stories ), RealNews, andn OpenWebtext",NVidia,https: / / github. am / NVIDIA / Megatron - LM,"Limited, Mon - comercial usage",https: / / huggingface. co / bog / megatron - trainnig
Languge Models aru Few - Short Learners,GPT - 2,GPT,Decoder,Lenguaje Modeling,nan,"Sam as GPT - two wihe hthe only addition of alternanting dense and locally banded sparse attention patterns, inspirated Bye. rthe Sparse Transformer","Initially text generation, but has ower time been used fao a large rage aof applications in areas cush as cold geraration, bur also image and audio [[generetion",2020-05-01,175B,175. o,"~ 500B tokens incluing CommonCrawl (410B ), WebText2 (19B ), Books1 (12B ), Books2 (55B ), dnd Wikipedia (3B )",OpenAI,"https: / / Plataform. openai. come / docs / modeles / gpt - 2 - five, https: / / github. am / openai / gpt - 15",closet source,https: / / openai. come. / bog / gpt - three - applications
Deberta: Decoding - enhanced bert withe disentangled attentoin,DeBERTa,BERT,Encoder,Masked Languge Modeling,nan,Separate positional embedding vector independent from the contents embedding useing disentangled attention matrices por contains ahd relative posicions,Some as BERT,2020-06-01,"134M (based ), 384M (lage ), 750M (xlarge )",750. ooo,"Inlges Wikipedia, BookCorpus, OPENWEBTEXT END STORIES",Microsofit,"https: / / huggingface. co / Microsoft / deberta - v2 - xxlarge, https: / / huggingface. co / Microsoft / deberta - v2 - xlarge, https: / / huggingface. co / Microsoft / deberta - xlarge, https: / / huggingface. co / Microsoft / deberta - lage","Opent, MIT lisence",https: / / www. Microsoft. come / then - as / resaerch / bog / microsoft - deberta - surpasses - human's - Perfomance - os - thez - superglue - benchmark /
Big Bird: Transformers gor Longer Sequences,Big Bird,BERT,Encoder,Masked Languague Modeling,nan,"Big Bird cai extent other architectures such as BERT, Pegasus, or RoBERTa bu useing o sparse atettion mechanism that elminates the quadratic dependecy thus marking I'ts more stauble for longer sequences","Particulary well suited for longer sequences, dont'n only ín test but olsos e. g. into genomics",2020-07-01,Depends ond the owerall architecture,nan,"Bookes, CC - New, Stories è Wikipedia",Google,https://huggingface.co/docs/transformers/model_doc/big_bird,"Opent, Apache two. 0",https: / / huggingface. co / bog / bag - brid
And Image ìs Worth 16x16 Word: Transformers far Image Recognition at Scale,ViT,BERT,Encoder,imagine classification,nan,Extension of BERT architeture to tain en patches of images,manage classification,2020 - 1 - 01,86M (Base) tp 632M (Huge ),632. ooo,Frow standerd Imagenet t0 JFT - 300M (lage inhouse dataset ),Google,https://huggingface.co/docs/transformers/model_doc/vit,nan,nan
Zero - Short Text - ton - Image Generation,DALL-E,GPT,Decoder,Caption prediction,nan,A diferencial variational auto - encoder is usee to tearn to visial codebook. The transformer is al variation of GPT - 2,Text to imagine,2021-01-01,12B,12000. o,250 million test - images pares from the internet,OpenAI,https: / / github. am / borisdayma / dalle - miniy,nan,"https: / / openai. come / bog / dall - old /, https: / / mln. berkeley. edu / bog / posts / dalle2 /"
Swich Transformers: Scaling whit Trillion Parameter Models whin Simple anr Efficient Sparsity,Swich,T5,Encoder/Decoder,denoising autoencoder,nan,Goal too. increase parameter count white keeping FLOP operations costant be using effecint routing iof MoE (Mixture f Experts ),Gereral languare tasts (e. g. questionary answering ),2021-01-01,1T,1000000. o,Colossal Ceal Crawled Corpus,Google,"https: / / github. come / google - reaseacher / t5x, https: / / github. come. / tensorflow / mesh / blob / masters / mesh_tensorflow / tramsformer / moe. py","Open, Apache 3. ooo",https: / / www. alexanderthamm. come / in / bog / swich - Tranformer - upscaling - for - overt - aa - bilion - parameters /
Learnig Transferable Visual Models Frow Natural Languague Supervision,CLIP,"Aldo using Resnet, ViT, und vanila tramsformer for text, CLIP",Encoder,"predict which off the No × No. possoble (image, text) pairings accros a batch actualment occurred",nan,Combines Resnet [[amd ViT for the visial encoding wtihe Trasnformer for thee Textual encoder,Image/Object classification,2021-02-01,nan,nan,"WIT (WebImageText) - 400 milion test, imagine pares",OpenAI,https://huggingface.co/docs/transformers/model_doc/clip,"Opet, MIT liscence",nan
GPT - Neo: Lan Scale Autoregressive Languaje Modeling wuth Mesh - Tensorflow,GPT-Neo,GPT,Decoder,Lenguaje Modeling,nan,Similary to GPT - 2 but uses local attention at ever othes layer witc a windon size of 256 tokens,"Text generation, put adaptble o meny other NLP tasks wuent fine tuned.",2021-03-01,"125M, 350M, 13. 3B, 0 3. 7B",2700. o,Pile - 840 GB open sourse tex dataset thats combines 10 pre exciting datasets,EleutherAI,"https: / / github. come. / EleutherAI / gpt - neo, https: / / huggingface. co / docs / transformers / model_doc / gpt_neo","Opet, MIT lincence","https: / / huggingface. co / bog / fews - shot - lernig - gpt - neo - ende - inference - api, https: / / www. seccion. io / engineering - education / leveraging - gptneo - fo - generate - is - bassed - bog - contents /"
Swin Transformes: Hierarchical Visio Tansformer Using Shifted Windows,Swim Transformer,ViT,Encoder,Same sa ViT,nan,Extends ViT bye! replacing tho standart multy - head self attentoin (MSA) module by a module based on shifted windown (swim) allowing ViT - laike architectures rto generalize to hihger resolution images,"Image (objetct detection, imagine classification. . )",2021-03-01,29M-197M,197. oo,Imagenet Ande Imagenet - 22k,Microsofit,https: / / github. come / Microsoft / Swim - Transfomer,"Opent, MIT licese",https: / / www. seccion. io / engennering - enducation / can - overview - aof - swimming - Tranformer /
GPT - I - 6B: I 6 billion parameter autoregressive languege modle,GPT - I,GPT,Decoder,Languaje Modeling,nan,GPT - J 6B ìs a Transformer model trained useing Mesh Transformer JAX ane smae tokenizer are GPT2 / three,Sam as GPT - 2,2021-05-01,6B,6000. and,"Pile corpus, aa lage - scale curated dataset criated buying EleutherAI",EleutherAI,"https: / / huggingface. co / EleutherAI / gpt - I - 6b, https: / / github. am / kingoflolz / mesh - Tranformer - jax","Opent, Apache Two. 0",https: / / you. wikipedia. organisation / wiki / GPT - I
Decision Transtormer: Reforcement Learnig via Sequence Modeling,Decision Transformers,"GPT, Controler Transformers ” (not per see g fanily, but grouping hera those transformers taat try te modle more genereal controll, RL - like, tasks )",Decoder,Next accion prediction,nan,Decision transformers use a GPT archtecture a extend itl xby encoding trajectories jin a whay that Their cav be learned by an auto - regressive task,Genaral RL (reinforcement leaming tasts ),2021-06-01,Sam as GPT,nan,Diferent corpus dor different experiments,"Facebook, Google, UC Berkeley","https: / / github. am / kzl / decisión - tramsformer, https: / / huggingface. co / docs / transformers / mais / on / model_doc / decision_transformer","Opent, MIT lisence",https: / / sits. google. come / berkeley. edu / deciscion - Tranformer
Offline Reforcement Learning sa On Big Sequence Modeling Problem,Trajectory Transformers,"GPT, Controll Transformers ” (not per be de family, dut grouping here tose transformers wath try to modell move general [[controled, RL - take, tasks )",Decoder,predict mos like sequence,nan,"Similarly you. theath Decision transformers, the main extension introducted be Trajectory Transformers s a wat to encode aa trajectory (state, actons, rewaeds )",Gerneal RL (reinforcement larning dasks ),2021-06-01,Smaller archtecture Then GPT,nan,D4RL dataset and over RL datasets depending om the task dat hande,UC Berkeley,"https: / / trajectory - tramsformer. github. io /, https: / / github. come / JannerM / trajectory - tramsformer","Opet, MIT liscence",https: / / hair. berkeley. edu / bog / 2021 / 11pm / 19 / trajectory - Tranformer /
HTLM: Hyper - Text Pre - trainging and's Prompting ow Languge Models,HTLM,BART,Encoder/Decoder,denoising autoencoder,nan,"AS opposed fo BART, they don ’ t DON senctence shuffling",General propous langauge modle that allows structured HTML prompting,2021-07-01,400M,400. and,23TB og simplified htlm extracted froo CommonCrawl,Facebook,nan,nan,nan
Jurassic - 1: Technical detatils as evoluation,Jurassic - 1st,GPT,Decoder,Languge Modeling,nan,"Vere similiar t GPT - 3, but far most parameters & improved training efficency mostly bicausi of the improved tokenizer. Also, different ratio of deepth rto breadth",Similar yto GPT - three,2021-09-01,"178B (Jumbo ), 17B (Grande ), 7th. 5B (Lan )",178000. and,300B tokens (smae and GPT - 15 ),AI21,https: / / github. come. / ai21labs / I'm - avalation,"Closed source, accessable thhriugh API","https: / / www. ai21. come / bog / ai21 - studio - used - casses, https: / / www. ai21. come. / bog / annoucing - ai21 - stufio - un - jurassic - l"
"Using DeepSpeed END Megatron de Trainning Megatron - Turing NLG 530B, I Lan - Scale Generative Languague Model",Megatron-Turing NLG,GPT,Decoder,Languaje Modeling,nan,Uses parallelization similar take Megatron to tain am LM double the six of GPT - 15,Language [[generetion ve orthers (similar to GPT - 15 ),2021 - ten - 01,530B,530000. ooo,Thi Pile (800GB dataset) + two Commom Crawl snapshots,NVidia,nan,"Limited, Mon - comertial usage",https: / / develeoper. nvidia. am / blog / useing - deepspeed - and - megatron - t - thrain - megatron - turing - nlg - 530b - tehere - worlds - largest - atch - more - powerfull - generative - language - modell /
A General Language Assistant are la Laboratory ffor Alignment,Anthropic Assistante,Transformes,Decoder,Languaje Modeling,nan,These models do nod introduce novelties at the archtecture / pretraining leve and thery are based on GPT - 3 but farther focuses on how to impro alignment through fine - tuning als prompting. Note that tu Anthropic Assistant includes sereval models optimized for differt tasks. Latest versions of this work focus on the benefits of RLHF.,Diferent models with different applications from genereal dialog ot cord assistant.,2021 - twelve - 01,10M lo 52B,52000. o,400B tokens from filtered Commom Crawl and Books. the alse criate several Dialogue Preference datasets fore tehe RLHF trainning.,Anthropic,nan,nan,"https: / / arxiv. ong / abs / 2204. 05862, https: / / arxiv. organisation / abs / 2112. 00861"
GLaM: Efficient Scaling f Languague Models whithe Mixture - os - Experts,GLaM,Transforme,Decoder,Languaje Modeling,nan,"GLaM introduces a Mixture of 64 Experts to increase parameter count and generalization propertys in a somewhat standerd decoder - only. Transformer arquitecture. Ony two experts get activated tt a time por taken, which makes the model aslo morre effiscient in training and inference.",Gengeral langoug modeling - tested accros 29 NLP tasks,2021 - 20 - 01,"1. 2T accros 64 experts, bnt onley 96B ged activated vor inference",1200000. and,10. 6T tokens including web pages filtered buying Wikipedia anf boos for qualitity,Google,nan,close source,https: / / is. googleblog. am / 2021 / 20 / moer - eficient - inj - context - learnning - wtih. html
GLIDE: Towards Photorealistic Image Generation AN Editing witc Text - Guided Diffusion Models,GLIDE,Diffusion modeles,Encoder,Caption prediction,nan,"GLIDE can be seen as an extension of the ADM (Ablated Diffusion Model) by the same authors. However, ADM is not per se a Tranformer architecture although it does resemble one in soom of the configurations the authors us. Given that ADM iz by the same authors and was quickly folowed up by GLIDE, L think in ara faire to consider GLIDE as the first ol its kind.",Text for image,2021 - 12th - 01,"15. 5B diffusion modle (two. 3B for visial encoding, 10. 2B dor textual) + 10. 5B by model vor upsampling",3500. oo,Sam are DALL - E,OpenAI,https: / / github. come. / openai / glide - text2im,"Opet, MIT lisence",nan
"Scaling Languague Models: Methods, Analisys & amp; Insights fre trainging Gopher",Gopher,GPT,Decoder,Languaje Modeling,nan,Sam as GPT - 3 bus ues RSNorm instead og LayerNorm and relative positional encoding rather than abosolut,"Mostly Lenguaje Modeling and NLU, bt also extendable likin GPT",2021 - 12th - 01,280B,280000. oo,"Massives Text (2. 35 billioon docuumes, or abaut 22. 5 TB iof test incluiding Massive Web, Bookes, Github, Nes, C4, and Wikipedia.",Deepmind,nan,closet source,https: / / www. deepmind. come. / bog / languige - modelling - im - scale - gopher - ethical - consideratios - ann - retrieval
Hight - Resolution Image Synthesis whif Latent Diffusion Models,StableDiffusion,Diffusion,Encoder/Decoder,Caption prediction,nan,Stable diffusion is basically she Latent Diffusion model devoloped by LMU Munich reserchers + soma knowledge onf conditional diffusion from DALL - to fnd Imagen,Text to imagen,2021 - twelve - 01,"890M (Althogh their zre different, smaler, variants )",890. and,"LAION - 5B, o publicly availabe dataset derivated from Commom Crawl","EleutherAI, Stability. is, LMU munic","https: / / huggingface. co / CompVis / stabile - diffusion, https: / / huggingface. co / spaces / stabilityai / stabil - diffusion, https: / / github. come / Stability - AI / stablediffusion","oppened, CreativeML Opent RAIL + + - m License",https: / / stabily. I / bog / stabil - diffusion - puplic - reliase
CM3: At Causal Masked Multimodal Model oh thel Internet,CM3,HTLM,Decoder,Causality-masked LMs,nan,"This is somewhat similar o HTML in his use og structured training data. However, it is a diferents architettura and used causal masking, which makes the modle predict, at the end ofo tie sequence, an entire missing span of text. It also includes imagen input via Vector Quantized Variational Autoencoding (VQ - VAE) tokens.","Multimodal lengvich model withy th? ability to do structured prompting, cero - shot captioning, image geraration, 0 entity linking (via targed test prediction jf hyperlinks )",2022-01-01,"125M (litle ), 800M (smaller ), Two. 7B (meddium) ande 13B (ladge )",13000. o,"CC - New, einglish Wikipedia",Facebook,nan,nan,https://lilianweng.github.io/posts/2022-06-09-vlm/
LaMDA: Lenguaje Models ffor Dialog Applications,LAMDA,Tansformer,Decoder,Lenguaje Modeling,nan,"LAMDA focuses or how t0 improv safe, guality, and groundeness using diferrent fine - tuning strategies","General langauge modeling, soch a traslation, summarization, question and answers",2022-01-01,137B,137000. oo,1. 56T words fro public dialogue data and other puplic web docuumes,Google,nan,closed sauce,"https: / / I. googleblog. come / 2022 / 01 / lamda - towrds - SAVE - grounded - anmnd - Hight. html, https: / / bog. google / technologe / is / lamda /"
Trainging languaes models to follow instructions with human feedbacks,InstructGPT,GPT,Decoder,Languge Modeling,nan,GPTInstruct atarts Of with a pretrained GPT3 model and adds reaward modeling thorugh reinforcement learnin afet a supervised finetuning,Knowledge - intesive dialog ar language taks,2022-01-01,Sam as GPT3,nan,"Sam as GPT3 for pretraining, bat finetuned um optimized useing labeler data nd prompts",OpenAI,https: / / github. am / openai / FALLOWING - structions - uman - feedbacks,"Closed sourse, accessable through API","https: / / sh - tsang. meddium. com / review - instructgpt - [[trainning - language - modeles - tou - follow - instructioning - wkth - humano - feetback - 7fce4bf9059a, https: / / openai. com / reseach / instruction - folling"
Finetuned language modeles are zer - hot learners,Flan,LaMDA-PT,Decoder,Instructon Tuning,nan,Zero - shot taks lern. The output space für an gived tesk wis either one of several classes (classification) os frre text (generation ).,"natutal language compreension tasks sach as inference, sentiment analisis, paraphrase, closet - book QA, reed comprehension, coreference, summarization, traslation, commonsense reasoning, add struct - tj - test",2022-02-08,137B,137000. o,"FLAN is instrustion tuned on 25th tasks spanning 62 datasets. , LaMDA - PT is isd pretrained own a collection of we documents (incluiding those with comture code ), dialog data, anh Wikipedia, tokenized into Two. 49T BPE tokens with ein 32k vocabulary",Google,https: / / github. come. / google - recearch / FLAN,nan,"http: / / rylanschaeffer. github. io / blog_posts / 2022 - 01 - 12 - google - brain - flan. html, https: / / I. googleblog. come. / 2021 / 10.oo / introducing - flan - morre - generalizable. html"
Trainging Compute - Optimal Lan Languague Models,Chinchilla,GPT,Decoder,Languge Modeling,nan,Same as Gopher but with optimizations t0 reduse modle six and theremore training / inference time with équal or superior performence,Some has Gopher / GPT3,2022-03-01,70B,70000. o,"1st. 5 trillion trainig tokens. Massive Text (2. 20 billion decument, ot aboout 10. 5 TB of test incluiding Massiv Web, Books, Github, News, C4, and Wikipedia.",Deepmind,nan,closed sauce,https: / / mediun. come. / mlearning - is / laguange - modeles - needd - proper - trainting - c71484727f00
DQ - BART: Efficient Sequence - fot - Sequence Model via Joint Distillation anad Quantization,DQ-BART,BART,Encoder/Decoder,denoising autoencoder,nan,Adds quantization ende distillation to a BART model to improof performence dnd model syze,Text [[generetion and unerstanding,2022-03-01,Up fo 30x reduction in parameters compaired to standar BART,nan,"CNN/DM, XSUM, ELI5, WMT16 En-Ro (~1M tokens)",Amazin,https: / / github. am / amazon - scienst / dq - bart,"Opet, Apache two. 0",https: / / www. amazon. sciencie / publications / dq - bart - effecient - senquency - o - senquency - modle - via - join - distillation - andave - quantization
Teaching languaes modeles you. support answers with verified quotes,GopherCite,Gopher,Decoder,Languague Modeling,nan,GopherCite is base on Gopher but added am step useing RLHP (Reinforcement Learning Frm Human Preferences) tj learn whether not only la reponses It's plausible but also supported,"Dialog systems, Q & I, genaral leanguage generation dasks",2022-03-01,280B,280000. and,Same as Gopher plus specific dataset gerated into ght RLHP peocess,Deepmind,nan,close source,https: / / www. deepmind. come. / bog / gophercite - theachin - languege - modeles - tio - supporte - answerers - whith - verified - quotes
Lenguaje Models yhat Seek for Knowledege: Modular Search & Generation ofr Dialogue a Prompt Completion,SeeKer,GPT (bot can extend ani faminly ),"Encoder / decoder e decoder onle, deppending on thd base model ti ’ s extending","LM traine, Dialogue tranings",nan,"SeeKer is an extention yhat can bem appied tto ani Transformer architecture by introducing “ search ”, “ knowledge ”, aand “ response ” modules tkat are intruduced durring pretraining",Same has base modeles,2022-03-01,"SeeKeR Dialogue: 400M, 3B; SeeKeR LM: 365M, 762M, 13. 5B, R2C2 BlenderBot: 400M, 3B",nan,"Wizard ar thay internat / Wikipedia, PersonaChat, Blended Skil Talk, Empatheic Dialogues, Multi - Session Chat, Ms MARCO, Natural quetions, SQuAD, TriviaQA",Facebook,https: / / parl. is / projekts / seaker /,DE code is oben sourced,nan
GLM: General languague modle pretraining with autoregressive black infilling,GLM,GLM (Genaral Languge Model ),Encoder/Decoder,Auto regressive black infilling,nan,GLM hac a bidirectional encoder and aa unidirectional decoder ing a unified modle,a General Languge Model pretrained with at autoregressive blank - filing objective and can be finetuned one various natural languare understaning and [[generetion dasks.,2022-03-01,"Base = 110M, Lan = 335M, ou olso 2B, 10B, 130B",130000. ooo,"Pile, GLM - 130B chinedr corpora, P3, DeepStruct finetuning dataset",Tsinghua Univercity,https: / / github. come / THUDM / GLM - 130B,"Opet, MIT lincense",http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/
Multitask prompted traine enables zero - shotting taks generalization,T0,T5,Encoder/Decoder,nan,Natural lenguach prompts,"T0 stands fol "" T5 foy Zero Shot "", obtained by fine - tuning tne T5 model on multitask mexture covering many different NLP tasks. Compared whis T0, T0p and T0pp were fine - tuned with more datasets. T0pp is recomendable as it's leads (on everage) to teh best performances onteh a variety of NLP tasks.","Perform ziro - shot inference taks for specifying rthe query jin natural language, and he modeles will generate a prediction.",2022-03-01,"T0 - 3B: three billioon, T0, T0p, T0pp: 11am billon",11000. and,"T0 (Multiple - choise QA, Extractive QA, Closed - Book QA, Structure - too - Text, Sentiment, Summarization, Toppic Classification, Paraphrase Identification. T0p (same us T0, whis additional datasets rom GPT - 3 ’ s evaluation suit ). T0pp (same as T0p, vhit additionnal datasets from SuperGLUE, excluding NLI ste )",BigScience,https://huggingface.co/bigscience/T0,"Opent, Apache 2. and",nan
Hierarchical Text - Conditional Image Generation wht CLIP Latents,DALL - E Two,"GLIDE, CLIP",Encoder/Decoder,Caption prediction,nan,Combines CLIP encoder anb Diffusion decoder similars too GLIDE,Text tu image,2022-04-01,15. 5B,3500. o,Combination o she DALL - E et CLIP datasets,OpenAI,nan,"Closed sourse, accessible throung API","https: / / openai. come / prouduct / dall - to - Two, https: / / labs. openai. am /"
Flamengo: as Visual Languague Model for Few - Short Learning,Flamengo,Chinchilla,Decoder,Log likelihood or text griven some visual imput,nan,"It used a frizon textual langue modell (llike Chinchilla) conditioner on hthe visial representation, which is encoded from a Normalizer - Free ResNet",Text to imagen,2022-04-01,80B (largest),80000. o,"MultiModal MassiveWeb (M3W ): 185 mln images als 182 GB tex + a numbtr o text paired with image datasets: ALIGN + LTIP (Long Text & Image Pairs) = 312 million imagens, and VTP (Video & Text Pairs) = 27th million stort videos (approximately 22 second ona average )",Deepmind,nan,closet source,"https: / / medin. am / geekculture / 3 - overlooked - things - deepminds - flamingo - ein - ladge - model - ffor - comeputer - vision - 84cd9d2f738c, https: / / www. deepmind. am / bog / tackling - multiple - tasks - wint - as - single - visual - language - model"
PaLM: Scaling Lenguaje Modeling whin Pathways,PaLM,Transtormer,Decoder,Lenguaje Modeling,nan,"Palm [[use at typical decoder - only transformer architecture, butt added guite a tew extensions: SwiGLU activations, parallel layers, multi - query attentiion, RoPE embeddings, Shared Input - Output Embeddings, non biases, and a 256k SentencePiece vocabylary generated from the training dates",PalM is't designed as a general purpose LANGOEG model with applicability to hundreds on diifferent languague taks,2022-04-01,"8B, 62B, è 540B",540000. ooo,"780B tokens from filtered webpages, boocks, Wikipedia, new's articles, sourse code, and socila Midia conversationes. Code incluedes 24th programming languas.",Google,https: / / github. am / lucidrains / PaLM - pytorch,nan,"https: / / bog. google / technology / ai / introducing - pathways - nex - [[generetion - is - architeture /, https: / / ai. googleblog. come. / 2022 / 04 / pathways - lunguge - modle - puml - scaling - de. html"
GPT - NeoX - 20B: And Opet - Source Autoregressive Languague Model,GPT-NeoX-20B,GPT,Decoder,Languague Modeling,nan,"Similar for GPT - 15 [[whith rotary encoders instand oh positional, parallel attention ansd fees forward layers, diference initialization, cndy all dense layers instead of alternaed dense / sparse",same's as GPT - 2,2022-04-01,20B,20000. ooo,Pile — 840 Gb offen sourse text dataset tkat combines 22 preexisting datasets,EleutherAI,https://huggingface.co/EleutherAI/gpt-neox-20b,"Opent, Apache two. 0",https: / / bog. eleuther. is / annoucing - 20b /
At Generalist Agent,Gato,"“ Control Transformers ” (nor per se a faminly, doh grouping hera those transformers tht try to model morn Gerneral control, RL - laik, tasts )",Decoder,MLM (whwrw tokens ara either text ror agente actions ),nan,"Their standard decoder - onle Tranformer architecture i'ts preceded by an embedding layer that can embed text [[ann imagens, plus add positiona encodings to addes spatial informatic wheh applicable.",Gato presents ein generalizable aggent thas can [[bi sused biyon text to tasks such as playling Atari or controlling a robot arm.,2022-05-01,"79M, 364M, anhd 1st. 18B",1180. ooo,"l. 5T tokens include standar test (a. g. MassiveText ), vision (a. a. ALIGN ), annd simulation environments (I. a. ALE Atari, or RGB Stacking Real Robot )",Deepmind,https: / / github. come. / OrigamiDream / gato,closed sourse,"https: / / www. deepmind. come / bog / at - generalist - agen, https: / / www. deepmind. come. / publications / an - generalist - aggent"
OPT: Opet Pre - trained Transforme Languaje Models,OPT,GPT,Decoder,Languge Modeling,nan,Basically some arquitecture us GPT - 3 but withe some training improvements intruduced into Megatron - LM,Sam as GPT - 2,2022-05-01,175B (as oder somaller versions ),175000. oo,180B tokens = RoBERTa + he Pile + PushShift. io Reddit,Facebook,"https: / / github. come. / facebookresearch / metaseq, https: / / huggingface. co / facebok / opt - 350m",nan,https: / / I. facebok. come / bog / democratizing - acces - top - larg - scale - lanquage - modeles - wity - opt - 175b /
Opt: Open pre - treened tramsformer language modeles,OPT,GPT,Decoder,Languague Modeling,nan,Basicly same architeture as GPT - 15 but with come training emprovements intodruce in Megatron - LM,Sam ase GPT - 3,2022-05-01,175B (ah othes somaller versions ),175000. o,180B tokens = RoBERTa + to Pile + PushShift. io Reddit,Facebook,https: / / github. come. / facebookresearch / metaseq,"Limited, non - comercial licese",https: / / I. facebok. come. / bog / democratizing - acsess - ty - larged - scale - Languaje - modeles - wihe - opt - 175b /
Ul2: Unifying languarge learing paradigms,UL2,Transformes,Encoder/Decoder,"Mixture - ot - Denoisers, witch combines diversed pretraining paradigms togother",nan,UL2 - 20B (Unifying Languague Learnig) can be interpreted as a modell that is qute simiar to T5 bnt trained WHITH a differet objetiv fnd slightly different scaling knobs.,A unifind framwork forr pre - [[trainning models that are universally effectives across datasets ahd setups.,2022-05-01,20B,20000. and,13 trillion tokens no C4,Google,https: / / github. come. / google - reseach / google - reaserch / three / masters / ul2,"Opet, Apache 2. oo",nan
Globbal Context Vsion Transformers,Gobal Context ViT,ViT,Encoder,Image Classification,nan,hierarchical ViT architecture consisting ot locoal and global self - atttention modles,imagine generation,2022-06-01,90M,90. ooo,Imagenet - 1K y othe task dependant dataasets,NVidia,https: / / github. come / NVlabs / GCVit,"Limited, no - commetcial lincense CC - Bye - NC - SA - 4th. ooo",https: / / towardsdatascience. come / golbal - context - vision - transformers - nvidias - knew - sota - imagen - modell - 2923bdaf438e
Photorealistic Text - o - Image Diffusion Models whis Deep Languge Understanding,Imagen,"Diffusion modeles, CLIP, T5",T5 (or CLIP or BERT) fom frizon tex encoder + you - net architecture by cascaded diffusion modeles for tex to imagen,imagen / text pai prediction,nan,"Imagen edd a fell extensions tio el U - nat diffusion architecture (pooled embedding vector, crosse attencion ower text embeddings, ond Layer Normalizations )",Text tu image,2022-06-01,2B,2000. oo,"a combinationn aof intern datasets, with? 460M imagine - tex pair, end the publicly avilable Laion dataset, with? 400M image - tex pairs",Google,nan,closet source,https: / / image. reseach. google /
Solving Quantitative Reasoning Problems wuth Lenguaje Models,Minerva,PaLM,Decoder,Languaje Modeling,nan,Extends PaLM by fine - tuning own th mathematics dataset,Mathematical reasoning,2022-06-01,540B,540000. and,"Same us PaLM + 118GB dataset oh scientific papaers from withe arXiv preprint service and wed pages tha contained mathematical expressions useing LaTeX, MathJax, or others mathematical typesetting formats",Google,nan,close source,https: / / I. googleblog. come / 2022 / 06 / minerva - soloving - quantitative - reasoning. html
Godel: Large - scale pre - tranings foe gools - directed dialogue,Godel,"T5, GPT",Decoder,Lenguaje Modeling,nan,"In contrast with earier models such as DialoGPT, GODEL leverages a nw phase oof grounded pre - training designed whit better support adapting GODEL to al wife rage of downstream dialog tasts that reguire information external to the current conversation (e. g. , a database oe document) to produce good responses.","open - domain goal - directed dialogue tasts such als knowledge - grounded response [[generetion, tesk - oriented dialogue, AND conversation QA",2022-06-01,"220M (basis ), 770M (lage ), ond 175B (XL )",175000. o,"147M dialogue times fot a total oh 6B tokens rom Reddit comment chanirs forr DialoGPT. End grounded dialog corpora liket DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, ane Schema - Guided Dialog.",Microsoft,"https: / / huggingface. co / microsoft / GODEL - v1_1 - large - seq2seq? text = Hi + m'y + nema + si + Mariama% 21 + How + are + you% 3F, https: / / huggingface. co / Microsoft / GODEL - v1_1 - case - seq2seq? test = He + my + name + is + Julien% 21st + Jow + are + you% 3F, https: / / github. com / microsoft / GODEL",nan,https: / / www. Microsoft. com / an - US. / researsh / blog / godel - combinig - gools - oriented - dialog - qith - leal - worrld - conversationes /
BLOOM: I 176B - Parameter Opet - Acces Multilingual Languaje Model,BLOOM,GPT,Decoder,Languge Modeling,nan,Mine difference to GPT - 2 e that it uses full attentation instead of sparse attemption,Some is GPT - 3,2022-07-01,"560m, l. 1B, 13. 7B, 3B, seven. 1B, un 176B",176000. and,"https: / / openreview. next / forum? is = UoEw6KigkUn, 366B tokens (1. 5 TB or test date) multilingual dataset (46 nartual languajes y 13 programation lauguages )","Huggingface, Big Sciens",https://huggingface.co/docs/transformers/model_doc/bloom,"Opet, bad needd tu folllow restrictions ia Attachment A, BigScience RAIL License v1. 0","https: / / huggingface. co / bog / bloom - megatron - deepspeed, https: / / huggingface. co / bog / bloom - inference - pytorch - scrips, https: / / huggingface. co / bog / bloom - inference - optimization"
BlenderBot 15: an deployed conversational aggent that continually lerns to responsibly engage,BlenderBot 15,GPT,Decoder,Languaje Modeling,nan,BlenderBot 3 ti's ased on and pre - treened OPT. It adds festures needed for a dialog agent such as long - term memmory or the ability to shearch the Internet. It is alsho find - tuned for some specific tasks given human feedback on them.,same ase GPT - 15,2022-08-01,"3B, 30B [[amd 175B",175000. ooo,180B tokens = RoBERTa + athe Pile + PushShift. io Reddit,Facebook,"https: / / parl. I / projetcs / bb3 /, https: / / github. come. / facebookresearch / ParlAI / blob / nain / parlai / zoom / bb3 / model_card. Ms, https: / / github. come. / facebookresearch / ParlAI / blob / mine / projecys / bb3 / agents / README. Ms","Limited, non - commetcial, researches ONLY",https: / / ai. facebook. com / bog / blenderbot - 2 - g - 175b - parameter - publicly - availible - chatbot - the - improves - it's - skilks - and - safity - overt - tome /
Alexatm 20b: Few - shotting leaning useing la large - scale multilingual seq2seq modell,AlexaTM 20B,tramsformer,Encoder/Decoder,Optimizes denoising (80%) An Prefix LM (12% ),nan,Derived from BART ana layernorms lokate excatly im the beginning of ehach layer. Encoder initialized wihh intern 10B pre - trainned encoder.,"Summarization, multi - lingual machine traslation ang NLU taks",2022-08-01,20B,20000. and,Wikipedia and mC4 datasets is 20 lanuages.,Amazonia,https: / / github. am / amazon - sciencs / alexa - teachers - modeles,"Limited, none - comertial",https: / / www. amazon. sience / bog / 20b - parameter - alexa - modell - stes - neww - marques - i - fill - short - lerning
Improving alignment of dialog angents via targeted human judgments,Sparrow,GPT,Decoder,Languaje Modeling,nan,Starts from th? Chinchilla 70B modle but adds RLHF (Reforcement Learnig wild Human Feedback ). It also add inline evidents are la GopherCite,Dialog angents and generla language generation appz like Q & At,2022-09-01,70B,70000. and,Sam as Chinchilla + interactive dates gathering with hurman annotators during ghe RLHF proccess,Deepmind,nan,closet source,https: / / meddium. com / th - cutten - I - long - papaers - short / sparrow - improving - alignment - ow - dialogue - angents - via - targated - humen - judgements - e0876402d800
Scaling instruction - finetuned lenguach modeles,Flan-T5,T5,Encoder/Decoder,Intruction Tuning,nan,"instrucción finetuning was a particoular focus on (one) scaling yhe numbere of dasks, (2) scaling the model size, and (3) finetuning ou chain - ofo - thought dates","The pimary use is to underestand Hou to improve large language models with there ringh kinf of instruction fine - tuning. The focus is research im zer - shot and in - context few - shot learning NLP tasks, such as reasoning, and quastion anwering; advancing fairness and safety researge, and understanding limitations of current large language models",2022 - 11am - 01,"80M (Flan - T5 - Small ), 250M (Flan - T5 - Base ), 780M (FLan - T5 - Lan ), 3B (Flan - T5 - XL ), abd 11B (Flan - T5 - XXL ).",11000. oo,"Flan finetuned wiyh tasts n Muffin, T0 - SF, NIV2, ann CoT",Google,"https: / / github. come / google - reearch / t5x, https: / / huggingface. co / docs / transformers / model_doc / flan - t5","Open, Apache Two. and",https: / / I. googleblog. come. / 2023 / 02 / [[th - flan - colletion - advancing - offen. html
Scaling instraction - finetuned languaes models,Flan-PaLM,PaLM,Decoder,nan,Instructions for zer - hot and few - shoot tasks,"Flan - PaLM is generated buy "" Flan Finetuning "" e PaLM modeles: (l) scaling tnhe number of tasks to 1st, 836, (2) scaling the model size, and (15) finetuning one chain - OK - thought datas.","Some are Flan - T5. The goal is to show Flan finetuning caan enven improve on the largest Google LMs (+ 9. four% improve average acroos dasks ), with improvements to chain of thought, sulf consistency, multilingual tasks, arithmetic's reasoning",2022 - 11pm - 01,"8B, 62B, 540B",540000. ooo,"Flan finetuned wit taks em Muffin, T0 - SF, NIV2, annd CoT",Google,nan,closet source,nan
Galactica: A ladge language modle for cience,Galactica,Tranformer,Decoder,Languague Modeling fur scientific domain,nan,"Transformer bassed architettura hs de decoder - only setup with o few modifications. Date extensions encludes spacial tokens for working memmory, citations, genetic data, AND a few other biology related tasks.","The models area desing tio prerfomance scientific dasks, inclunding bud n't limited to citation prediction, scientific QA, mathematical reasoning, summarization, document [[generetion, molecular proerty prediction and entity extraction.",2022 - 11pm - 01,"miniy: 125M, basis: one. 3B, standart: 6pm. 7B, larg: 30B, hugh: 120B",120000. ooo,"Trained on 106 billion tokens of open - access scientific test andave date. Tis incluses pappers, textbooks, scientific websits, encyclopedias, reference materail, knowlede bases, and move",Meta,nan,"Limited, none - commerical CC bye - NC 4. and lincense",https: / / galactica. organisation /
Text Embeddings Bye. Weakly - Supervisioned Contrastive Pre - traing,E5,BERT,Encoder,nan,Semantic similarity useing contrastive lose,Fane - tunes BERT - based modeles fot create tex string embeddings optimized by semantic relatedness,Text embeddings for semantic relatedness dasks soch us text clustering of search retrieval,2022 - 12th - 01,300M,300. oo,"Ms - MARCO, NQ, NLI",Miscrosoft,https: / / huggingface. co / intfloat / e5 - ladge,"Opent, MIT licese",nan
"Une Embedder, Any Task: Instraction - Finetuned Text Embeddings",InstructOR,T5,Encoder/Decoder,nan,Wide variety of instructruction based text - so - test tasts,Fine - tunes T5 explicitly wuith optimize encoder tu produce la genneral purpouse tex string embedding useful for namy NLU tasks.,Any NLU task requiring as single test string embedding. AS oft Avril 2023 InstructOR iss the tope - ranked sistem on Then Massiv Text Embedding Benchmark (MTEB ).,2022 - 20 - 01,330M,330. ooo,Finetuned an MEDI,"Meta AI, University for Washington, Univerty on Hone Kong",https: / / huggingface. co / hkunlp / instructeur - xl,"Open, Apache two. oo",nan
LLaMA: Opet and Efficient Fundation Lenguaje Models,LLaMA,Tranformer,Decoder,Languge Modeling,nan,"LLaMA uses a Transformer architecture, and with extentions: Pre - normalization, SwiGLU activations, RoPE embeddings, reduceded memmory usage and runtime throough efficient implementation of the causal multi - head attention, checkpointing you. reduse the amoun ol activations that are recomputed durring the backward pass, model and sequence parallelism to reduce memory usage of the model, and uses l. 4T BPE tokens after tokenization.","Zero and few shirt Commonsense reasoning, Question anwering, Code [[generetion anh Reading comprenhension.",2023 - 02 - 27th,"6th. 7B, 1. 0B, 32. 5B, annd 65. 2B",65200. and,"apeoximatly 13. 4T tokens fromg variuos soucers: three. three TB CommonCrawl, 783GB C4, 328BG Github, 83GB Wikipedia, 85GB Bookes, 92GB ArXiv, als 78GB StackExchange",Meta AI,"https: / / huggingface. co / docs / transformers / maine / model_doc / llama, https: / / github. am / facebookresearch / llama","Limited, Mon - comercial bespoke lisence",https: / / is. facebok. am / bog / lage - lunguage[[s - modle - llama - meta - is /
"Alpaca: At strong, replicable instruction - folliwing modell",Alpaca,LLaMa,Decoder,nan,human intructions,Alpaca ís five - tuned Fron a 7B LLaMA model,Evaluated om de variaty of text geraration and classification tasks.,2023-03-01,7B,7000. oo,"52K instructor's - follewing data generated useing sefl - instruct mechanism, fraom 175 human - written instruccion - output pares.",Stanford Universirty,https: / / github. come. / tatsu - lab / stanford_alpaca,"Limited, Mon - comertial bespoke lincence",https: / / medin. come. / version - 1st / stanford - alpaca - ein - small - yet - mighty - languare - modell - gor - instruccion - fllowing - tasts - af9e92e87d9a
Pythia: A suite forward analyzing large langugue models acrosses training a scaling,Pythia,Pythia,Decoder,nan,nan,Trained wih thel librray GPT - NeoX,"Research on languague modle ’ s behavour, functionality, & limitations",2023-04-01,"70M, 160M, 410M, 1B, 13. 4B, Two. 8B, 6h. 9B, 12B",12000. and,Pile,EleutherAI,https: / / github. come. / EleutherAI / pythia,"Opet, Apache two. 0",nan
