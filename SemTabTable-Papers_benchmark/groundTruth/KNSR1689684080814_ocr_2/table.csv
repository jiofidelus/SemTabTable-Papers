0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Improving Language Ondek8tandin9 6y Generative Pre - Training,CPT,CPT,Oec0dek,Language M0de1in9,nan,nan,"Text generation, 6ot adapta61e to many 0thek NLP tasks when fine toned.",2018 - 06 - o4,1l1M,lI7. 0,"Unsupervised Pretraining 0n BookCorpus dataset. Supervised Finetuning 0n several task - specific datasets inc1odin9 8NL1, RAGB, Quora. ..",DpenA1,"http8: / / 9ithu6. com / openai / finetune - transformer - 1m, https: / / huggingface. co / duc8 / transformers / model_doc / openai - 9pt",nan,nan
EBRT: Pke - training of Deep Bidirectional Transf0kmek8 for Language Understanding,EBRT,EBRT,Bnc0dek,"Next Sentence Pkedictiun, Ma8red Language Modeling",nan,nan,General Lan9oa9e Ondek8tandin9 and Qoe8tiun Answering. Many other language applications ful10wed,2018 - 10 - 01,"Base = 110M, Lak9e = 31OM",340. 9,Toronto Euok Cokpu8 and Wikipedia (3. 3B Toren8 ),Cougle,https: / / huggingface. cu / docs / transformers / m0de1_duc / 6ekt,"Open, Apache z. 0","https: / / co1a6. ke8eakch. google. com / github / NielsRogge / Transformers - Tutorials / blob / master / BERT / Fine_tonin9_EBRT_ (and_friends) _for_multi_label_text_classification. ipynb, https: / / www. philschmid. de / bert - text - classification - in - a - different - 1an9oage"
Tran8fukmek - XL: Attentive Language Mode18 Eeyund a Fixed - Len9th Context,Tkansfukmek XL,nan,0ec0dek,Language Mude1in9,nan,Relative positioned embeddings ena61e longer - c0ntext attention when compared t0 vanilla Transformer model,General language ta8k8,2019 - 01 - OI,I54M,151. 0,"Oiffekent training datasets depending on experiments, 6ot baseline i8 Wikitext - ID3","Google, CMU",https: / / ho99in9face. co / docs / transformers / model_doc / tkan8fu - xl,nan,nan
Lan9oa9e models ake unsupervised multitask learners,CPT - 2,CPT,Oecudek,Lan9oa9e Modeling,nan,"Minor extensions tu the CPT architecture (e. g. layer normalization moved to the input of each sub - layer, or inckea8ed cuntext size from 512 to 1024 )","Text generation, 6ot adapta61e to many 0thek NLP ta8k8 when fine tuned.",2019 - 02 - 97,"124M, 355M, 774M, I. 5B",180D. 0,8 million web pa9e8 (40 GB ). I8X GPT. We6Text data8et is created by crawling all links at Reddit with at least 3 Kakma points.,DpenA1,https: / / huggingface. co / docs / transformers / mude1_duc / 9ptZ,"0pen, Modified MIT license",https: / / www. philschmid. de / fine - tone - a - n0n - en91i8h - 9pt - 2 - model - with - huggingface
XLNet: Generalized Autoregressive Pretraining for Language Ondek8tandin9,XLNet,Transformer XL,0ec0dek,PLM,nan,This model basically adapt8 Tkan8furmek XL architecture to permutation - 6a8ed LM,General language ta8k8,2019 - o8 - 01,"Base = 4l7M, Lak9e = 360M",360. 0,"Same as EBRT + Giga5 (16GB text ), and and ag9ke88ively filtered ClueWeb 204z - B (19GB ), C0mm0n Ckaw1 (110 CE )","Google, CMU",https: / / ho99in9face. co / docs / transformers / model_doc / xlnet,"Open, MIT 1icen8e",nan
BRNIB: Bnhanced Language Representation with Informative Entities,ERN1B,EBRT,Bnc0dek,Masked Lan9oa9e Modeling,nan,"Uses BERT for Bnc0dek architecture, but 8tacrs and aggregates two of them for text and entitie8. This akchitectoke could be understood as EBRT for text + knowledge 9raph8",Kn0w1ed9e inten8ive related ta8k8 that might benefit fk0m knowledge graphs or entitie8 such as entity recognition,2019 - 05 - 01,"Ernie - ViLG 2. O = 10B, Bknie 3. O Titan = 260B",26900O. 0,Bn91ish Wiripedia + Wikidata f0k entitites (note that they initialize model tu 0ki9inal BERT parameter values,"Pengcheng La6, Baidu",http8: / / github. com / thunlp / ERNIE,c1o8ed source,http: / / research. baidu. com / Blog / index - view? id = 4b0
RoBERTa: A Rubo8tly Optimized BERT Pretraining Approach,R0EBRTa,EBRT,Bncudek,MLM (0ynamic ),nan,Bxten8iun of BERT with optimized training prucedoke and m0ke data,Same as EBRT,2019 - 81 - oI,"125M Ea8e, and 356M Lak9e",356. D,8ame as BERT + GG News + OpenWebText + Stories (~ 33B Tokens ),"Google, University uf Wa8hin9t0n","http8: / / 9ithu6. com / facebookresearch / faik8e9 / tree / main / examp1e8 / roberta, https: / / ho99in9face. co / docs / transformers / model_doc / roberta",nan,http8: / / ai. facebook. com / blog / roberta - an - uptimi2ed - method - for - pretraining - self - supervised - nlp - systems /
ALBERT: A Lite EBRT for Self - 8opervi8ed Learning 0f Language Representations,ALEBRT,EBRT,Bnc0dek,"Next Sentence Pkedicti0n, Masked Lan9oa9e Modeling",nan,"Compressed version of EBRT u8in9 parameter sharing, which is much muke efficient given the 8ame number of parameters",Same a8 BERT,204g - 90 - 01,"Base = 12M, Lak9e = 18M, XLak9e = bDM",60. D,8ame as BERT,G0ugle,https: / / huggingface. co / docs / transformers / model_doc / al6ekt,"Open, Apache 2. 9",nan
CTRL: A Conditional Transformer Lan9oa9e Model for Contk011able Cenekatiun,GTRL,nan,Oecudek,nan,nan,"model can 9enekate text conditioned on c0ntk0l codes that 8pecify d0main, 8ty1e, topics, dates, entities, relationships 6etween entities, p1ut points, and task - related behavior",Controllable text generation,Z0I9 - 09 - 01,1. 63B,1630. 0,"140 GB of text including: Wikipedia (Bn, De, Es, Fr ), Pk0ject Gutenberg, 45 subreddits, OpenWebText2, Ama2un Reviews, Europarl and UN data from WMT, question - answer pairs from ELI5, and the MRQA shared task3, which includes the Stanford Qoe8ti0n Answering Dataset, NewsQA, TriviaQA, SearchQA, HutputQA, and Natoka1 Questions",Sa1e8fokce,"http8: / / github. c0m / sa1e8fokce / ctrl, https: / / huggingface. co / docs / transformers / model_doc / ctrl","0pen, B8O - 3 - Clause license",https: / / blog. salesforceairesearch. com / introducing - a - conditional - transformer - language - m0de1 - for - controllable - 9enekati0n /
Highly accurate pkutein structure pkedicti0n with A1phaFu1d,A1phaFu1d,8B (3) - Transformer,Bnc0dek,"Pkutein folding prediction uf BERT using parameter sharing, which i8 moch more efficient given the same number of parameters",nan,"The original Alphafold used a BERT - style transformer. The details of Alphafold ’ s Transformer ake not known, but it is 6e1ieved it i8 an exten8i0n of the SE (3) - Tranformer, a 3 - 0 equivariant Transformer (8ee this blog post ).",Protein f01din9,2019 - og - O4,"b12M, Large = l&M, XLak9e = 60M",b9. 0,"l7o, 000 pkutein8 from a public repu8it0ry of protein 8e9oences and structures",0eepmind,http8: / / 9ithob. cum / deepmind / alphafold,"the code is open sourced, with Apache - z. 0","https: / / www. deepmind. com / po61ication8 / hi9h1y - accokate - protein - structure - prediction - with - alphafold, https: / / fabianfuchsml. github. io / alphafold2 /"
"BART: 0enuisin9 Sequence - to - Sequence Pre - tkainin9 f0k Natoka1 Language Generation, Tkan81ati0n, and Gompkehen8iun",EART,"EBRT f0k encoder, GPT for Decoder",Bnc0dek / Decoder,den0i8in9 autoencoder,nan,1t can 6e seen a8 a 9eneka1i2ati0n of BERT and GPT in that it combines ideas fk0m both in the encoder and decoder,Mostly text 9enekati0n but also some text understanding tasks,29l9 - 10 - 01,"Ea8e = 140M, Lak9e = 400M. In 9eneka1, roughly 10% lar9ek than EART for equivalent architectures",4o9. 0,"Same as RuEBRTa (160Gb of news, books, stories, and we6 text )",Facebu0r,https: / / huggingface. co / docs / transformers / model_doc / bart,"Open, Apache 2. 0",nan
DialoGPT: Large - Scale Generative Pre - tkainin9 f0k C0nvek8atiuna1 Response Generation,Dia10CPT,CPT,Oecudek,Language Mude1in9,nan,GPT - 2 akchitectoke trained 0n dialog data,Text generation in dialog 8ettin98,2019 - 79 - O7,4. 5B,1500. O,140M Reddit c0nveksati0n8,Micku8oft,"http8: / / github. cum / mick08oft / DialoGPT, https: / / ho99in9face. co / d0c8 / transformers / mude1_duc / dialogpt","Dpen, MIT 1icen8e",http8: / / ho99in9face. c0 / microsoft / DialoGPT - medium? text = Hey + my + name + is + Mariama% 21 + Huw + are + you% 3F
"DistilBERT, a di8ti11ed version uf BERT: smaller, faster, cheaper and lighter",0istilEBRT,EBRT,Bncudek,"Masked Lan9oa9e Modeling, Next Sentence Prediction",nan,"Compressed vek8iun of BERT using di8ti11ati0n, which is much more efficient given the 8ame number uf parameters",Same a8 BERT,2OI9 - 10 - 01,bbM,66. 9,Same as EBRT,Ho99in9face,https: / / ho99in9face. co / docs / transformers / model_doc / distilbert,"Upen, Apache 2. 0",https: / / mediom. com / huggingface / distilbert - 8cf338013868
Exploring the limits of transfer 1eaknin9 with a onified text - tu - text tkan8fokmek,T8,Tran8fukmek,Encoder / Decoder,denoising aotuenc0dek,nan,Same as original Transformer with some additiun8 8och as relative positional em6eddin98 like Tkan8fukmer XL,"Ceneka1 1an9oage tasks including machine tkan81ati0n, question answering, a68tkactive summarization, and text classification",z919 - 10 - 01,"boM, 220M, 770M, 3B, and 11B",11000. o,Colossal G1ean Gkaw1ed Corpus (C4) - G1eaned up version 0f the G0mmon Crawl dataset - 750 GB,Coo9le,"http8: / / github. c0m / google - research / text - t0 - text - transfer - transformer, https: / / huggingface. co / duc8 / transformers / model_doc / t8","Open, Apache 2. o",https: / / ai. googleblog. com / Z02D / 02 / explukin9 - tkan8fek - learning - with - t5. htm1
Onsopekvi8ed Cross - lin9oa1 Representation Learning at Scale,XLM - RuEBRTa,RuEBRTa,Bncudek,MLM (0ynamic ),nan,An extension of RoBERTa that intr0doce8 small parameter tuning insights in the c0ntext of multilingual app1icatiun8,Tkan81atiun and other cross - 1in9oal language tasks,Z01q - 10 - 01,Base = 270M Lak9e = 550M,550. D,Cleaned Gomm0n Crawl in 709 languages,Face6our,http8: / / huggingface. co / docs / tkan8f0rmeks / model_doc / xlm - roberta,nan,nan
PEGASUS: Pre - tkainin9 with Extracted Cap - sentences for Abstractive Summarization,Pe9aso8,Tran8fukmek,Encoder / Decoder,DAE (m0ke c0nckete1y GSG) and MLM,nan,Extends vanilla Tkan8furmek by osin9 a different pketkainin9 ta8r (GSG: Gap Sentence Generation) that is 6ettek suited fuk summarization,abstractive text 8ommaki2ation,2019 - 12 - D7,Base = zZ3M Large = 568M,568. 0,C4 (750GB) + HugeNews (3. S TE ),"Google, Imperial College Lundun",https: / / huggingface. c0 / d0c8 / transformers / model_doc / pegasus,nan,https: / / ai. googleblog. com / 29z0 / 06 / pegasus - 8tate - of - akt - m0de1 - for. html
Multilingual Oenoi8in9 Pre - tkainin9 for Neora1 Machine Translation,mEART,EART,Bnc0dek / Decoder,denoising aotuencudek,nan,Bxtend8 BART to multilingual capa6i1ity,Tkan81atiun,Z0z0 - 01 - 01,8ame as BART,nan,CG28 Corpus includes z8 monolingual corpuses in different languages. Lak9e8t corpuses are English (399 GB) and Ro88ian (280GB ),Faceb00r,"https: / / github. com / facebookresearch / faik8e9 / tree / main / examples / m6akt, https: / / ho99in9face. co / docs / transformers / model_doc / mbart","0pen, M1T license",nan
ELECTRA: Pre - training Text Encoders as Discriminators Rathek Than Generators,BLBGTRA,EBRT,Bncudek,replaced turen detection,nan,App1ied new training techni9oe8 including Replaced Token Detection,8ame as BERT,2OZ0 - 03 - 01,"8mal1 = 41M, Base = 740M, Large = 335M",335. 0,Same as BERT except for Large with i8 8ame a8 XLNet,"Google, 8tanf0kd","https: / / github. com / g0o9le - research / e1ectka, http8: / / huggingface. co / duc8 / transformers / model_doc / electra","Upen, Apache 2. 0",nan
Megatron - LM: Training Mo1ti - Billion Parameter Lan9oa9e Models Using Model Parallelism,Me9atk0n,"T5, EBRT, GPT","Encoder or 0ecokdek, dependin9 on the base model",8ame as 6a8e model,nan,"Me9atk0n is a family 0f models that extend previously known architectures (namely CPT - 2 and BERT originally, 6ot also T5 more recently) by introducing m0de1 parallelism primitives. In the case of BERT, the authors also replace the next sentence prediction head with sentence order prediction and use whu1e word n - 9kam ma8rin9.",Same a8 base m0de1,2020 - 93 - 01,"8. 3E (CPT - 1ire ), 3. 9B (EBRT - like )",@30D. 0,"Original paper uses an ag9ke9ate dataset con8i8tin9 of Wiripedia ), CC - Stories ), RealNews, and 0penWe6text",NVidia,https: / / github. cum / NVIDIA / Megatron - LM,"Limited, Non - commercial o8age",https: / / huggingface. co / 6lug / megatron - training
Language M0de1s ake Few - Shot Leaknek8,GPT - 3,CPT,Oecudek,Lan9oa9e Modeling,nan,"8ame as CPT - 2 with the only additiun of a1teknatin9 dense and locally banded sparse attentiun pattekn8, in8piked by the Sparse Transformer","Initially text generation, 6ot ha8 over time been used for a large kan9e of app1icatiun8 in areas such a8 code generation, 6ot also image and audio generation",2020 - 88 - 01,118B,175. D,"~ 500B tokens including GummonGkawl (410B ), WebText2 (19B ), Books1 (12B ), Books2 (58E ), and Wiripedia (3B )",UpenA1,"https: / / platform. openai. com / duc8 / models / gpt - 3 - 5, https: / / github. com / upenai / gpt - 3",closed sookce,http8: / / openai. com / blog / 9pt - 3 - apps
Deberta: Oecudin9 - enhanced bert with disentangled attentiun,0eEBRTa,EBRT,Bncudek,Masked Lan9oa9e Modeling,nan,8epakate positional em6eddin9 vector independent from the content embedding u8in9 disentangled attenti0n matrices for contents and relative po8itiun8,Same a8 BERT,2020 - 06 - D4,"134M (base ), 384M (large ), 750M (xlarge )",750. o,"English Wiripedia, EoorCorpu8, OPENWEBTEXT and STORIES",Micru80ft,"https: / / huggingface. co / microsoft / deberta - v2 - xxlarge, https: / / huggingface. c0 / microsoft / deberta - v2 - xlarge, https: / / huggingface. c0 / microsoft / deberta - xlarge, http8: / / huggingface. co / mick08oft / deberta - large","Open, M1T license",https: / / www. micr08uft. cum / en - us / research / blog / mick08oft - deberta - surpasses - human - performance - on - the - soper91ue - benchmark /
Ei9 Bird: Tkansfokmek8 for Lon9ek Sequences,Big Eikd,EBRT,Bnc0dek,Masked Language M0de1in9,nan,"Ei9 Eikd can extend other akchitectoke8 such as EBRT, Pegasus, or RoBERTa 6y using a sparse attention mechanism that e1minate8 the quadratic dependency thus making it m0ke 8oitab1e for longer sequences","Particularly we11 suited for longer sequences, not un1y in text 6ot a18o e. 9. in 9en0mic8",2020 - 07 - 01,0epend8 on the oveka11 architecture,nan,"Euoks, CC - News, Stories and Wikipedia",G0o9le,https: / / ho99in9face. c0 / docs / transformers / model_doc / big_bird,"Upen, Apache 2. 0",https: / / ho99in9face. co / blog / big - bird
An Image is Worth 16x16 Wokd8: Transformers for Image Recognition at 8ca1e,ViT,EBRT,Bncudek,image c1a88ificati0n,nan,Extension of BERT architecture to tkain on patche8 0f images,ma9e classification,20z9 - 10 - 01,86M (Ea8e) to 632M (Huge ),632. o,Fkum standard Imagenet to JFT - 300M (large inhuo8e dataset ),Cougle,https: / / ho99in9face. co / duc8 / transformers / model_doc / vit,nan,nan
Zero - 8hut Text - tu - 1ma9e Generation,DALL - E,CPT,0ec0dek,Gaptiun prediction,nan,A differential variational auto - encoder i8 used to 1eakn the visual codebook. The transformer i8 a variation uf GPT - 3,Text tu image,28Z1 - DI - 01,7ZB,12000. 0,250 million text - images pairs fkum the inteknet,0penA1,https: / / github. com / 60ri8dayma / da11e - mini,nan,"http8: / / 0penai. cum / blog / dall - e /, https: / / ml. berkeley. edu / blog / posts / dal1eZ /"
8witch Transformers: Scaling to Tri11iun Parameter Models with 8imp1e and Bfficient Sparsity,8witch,T8,Bnc0dek / Decoder,den0i8in9 autoencoder,nan,Cual to inckea8e parameter count while keeping FLUP opekatiun8 constant by using efficient routing 0f MoE (Mixture of Bxpekt8 ),Ceneka1 lan9oa9e tasks (e. g. question an8wekin9 ),2021 - 01 - 87,IT,l890000. 0,Gu1ossal Clean Gkaw1ed Corpus,G0og1e,"https: / / github. com / google - research / t5x, https: / / github. com / tensorflow / me8h / blob / master / mesh_tensorflow / transformer / m0e. py","Upen, Apache 2. 0",https: / / www. alexanderthamm. cum / en / 6lug / switch - transformer - op8calin9 - to - over - a - 6il1iun - parameters /
Learning Transferable Visoa1 Models Fk0m Natoka1 Language Supervision,GL1P,"A1s0 using Re8net, ViT, and vanilla transformer for text, GL1P",Bncudek,"predict which of the N × N pus8i6le (image, text) pairings across a 6atch actoa11y occurred",nan,C0m6ine8 Resnet and ViT for the visoa1 encoding with Transformer for the Textual encudek,Image / Object c1a88ificatiun,2021 - DZ - 01,nan,nan,"W1T (WebImageText) - 400 million text, image paik8",DpenA1,https: / / huggingface. co / docs / transformers / model_doc / clip,"Open, M1T 1icen8e",nan
GPT - Neo: Large 8ca1e Aotokegke88ive Language Modeling with Mesh - Tensorflow,CPT - Neo,CPT,0ec0dek,Language Mude1in9,nan,Similar tu CPT - 2 6ot uses local attention in every other 1ayek with a window size of 28b tokens,"Text 9enekatiun, but adapta61e tu many other NLP tasks when fine toned.",2021 - o3 - D7,"lz5M, 350M, 1. 3B, and 2. 7B",2700. O,Pile - 840 CE open 8oukce text dataset that combines ZZ pre exi8tin9 datasets,B1eutherA1,"https: / / githo6. com / EleutherAI / gpt - neu, http8: / / huggingface. co / docs / transformers / model_doc / gpt_neo","Open, M1T license","http8: / / huggingface. co / blog / few - shot - 1eaknin9 - gpt - neo - and - inference - api, https: / / www. 8ectiun. io / en9ineekin9 - education / leveraging - gptneo - to - generate - ai - based - blog - c0ntent /"
Swin Transformer: Hiekakchica1 Vi8i0n Transformer Using Shifted Winduw8,Swin Tkan8fokmek,ViT,Bncudek,Same as ViT,nan,Bxtend8 ViT by replacing the standard multi - head self attention (M8A) module by a module based on shifted windows (Swin) allowing ViT - 1ire akchitectoke8 to 9enekali2e t0 higher resolution images,"1ma9e (object detecti0n, image classification. . )",Z024 - 03 - 01,29M - 7q7M,791. 0,1ma9enet and Imagenet - 22k,Micku8oft,https: / / github. com / microsoft / 8win - Transformer,"Upen, M1T license",https: / / www. section. io / en9ineekin9 - education / an - 0vekview - uf - swin - transformer /
CPT - J - 6B: A b billion parameter aotoke9re88ive language model,CPT - J,CPT,Oec0dek,Lan9oa9e Modeling,nan,GPT - J 6B is a Tkan8fokmek model tkained using Mesh Transformer JAX and same tureni2er a8 GPT2 / 3,Same as CPT - 3,zO21 - 05 - 01,bE,b0o0. 0,"Pile corpus, a lak9e - 8ca1e curated dataset created 6y B1eutherA1",EleothekA1,"http8: / / huggingface. co / EleutherAI / 9pt - j - 6b, https: / / github. com / kingoflolz / mesh - transformer - jax","Open, Apache Z. D",https: / / en. wikipedia. ur9 / wiki / CPT - J
Oeci8i0n Tran8f0kmek: Reinforcement Leaknin9 via Sequence Modeling,Decision Tran8f0kmer8,"GPT, Control Transformers ” (not pek se a fami1y, but grouping heke th08e transformers that try to model more 9eneka1 control, RL - like, tasks )",Oecudek,Next actiun prediction,nan,Decision tran8fukmer8 use a GPT akchitectoke and extend it 6y encoding tkaject0kie8 in a way that they can be 1eakned by an auto - regressive ta8r,Ceneka1 RL (keinfukcement learning tasks ),zO21 - ob - 01,Same as CPT,nan,Different cukpus f0k different experiments,"Facebook, Google, OG Eekreley","https: / / github. cum / kzl / decision - tkansf0kmek, https: / / huggingface. c0 / docs / tkan8formek8 / main / en / model_doc / decisi0n_tkan8fukmek","Dpen, MIT 1icen8e",https: / / sites. google. com / 6ekreley. edo / decision - tkan8fokmek
Offline Reinforcement Learning a8 Dne Big Sequence Mude1in9 Problem,Trajectory Transfukmek8,"GPT, Control Tkansfukmeks ” (n0t per 8e a family, 6ot grouping here those transformers that try to model more general contku1, RL - like, tasks )",Oecudek,pkedict most likely 8e9oence,nan,"Similarly t0 the Oeci8i0n transfukmek8, the main extension intkudoced by Tkajectuky Transformers is a way tu encode a trajectory (state, actiun8, rewards )",General RL (keinfukcement learning ta8k8 ),2021 - 8b - 01,8mal1ek akchitectoke than GPT,nan,01RL dataset and other RL datasets depending on the ta8r at hand,UC Eerre1ey,"https: / / trajectory - transformer. github. io /, https: / / github. com / JannerM / tkajectuky - tran8fukmek","Dpen, MIT license",http8: / / bair. berkeley. edu / blog / 2021 / 11 / 19 / tkaject0ky - tkan8fokmek /
HTLM: Hyper - Text Pke - Training and Pk0mptin9 uf Language Mudel8,HTLM,EART,Encoder / Oecudek,den0i8in9 autoencoder,nan,"As opposed to EART, they dun ’ t d0 sentence shuffling",General pukp08e language model that a11ows structured HTML prompting,2021 - 07 - 01,4o8M,48D. 0,23TB 0f 8imp1ified HTML extkacted from CommonCrawl,Faceb0ur,nan,nan,nan
Jora88ic - 1: Technical details and eva1oati0n,Joka8sic - 1,CPT,0ecudek,Language M0de1in9,nan,"Veky 8imi1ak to GPT - 3, but far more parameters and improved training efficiency mu8tly 6ecao8e of the improved tokenizer. Also, different katiu uf depth to breadth",8imi1ak to GPT - 3,2021 - Og - 01,"178B (Jum60 ), 17B (Grande ), 1. 5B (Large )",178000. 0,300B tokens (same a8 GPT - 3 ),AIz4,https: / / github. com / ai21labs / lm - evaluation,"Closed source, acce88i6le thruo9h API","https: / / www. ai21. com / blog / ai21 - studio - use - cases, http8: / / www. aiZ4. com / blog / announcing - ai21 - studio - and - juka88ic - 4"
"Using DeepSpeed and Megatron to Tkain Megatron - Toking NLG 530B, A Lak9e - 8ca1e Cenekative Lan9oa9e Model",Me9atkun - Turing NLG,CPT,0ec0dek,Lan9oa9e Modeling,nan,O8es paka11e1izatiun similar to Megatron to train a LM double the size of GPT - 3,Language 9enekatiun and othek8 (similar to GPT - 3 ),20z7 - 7D - 01,830E,530000. o,The Pi1e (800GB dataset) + 2 Cumm0n Crawl snapshots,NVidia,nan,"Limited, Non - cummekcia1 usage",https: / / developer. nvidia. com / blog / using - deepspeed - and - megatron - t0 - train - megatron - turing - n19 - 839b - the - worlds - largest - and - most - powerful - generative - language - model /
A General Language Assistant a8 a La6okat0ry for Alignment,Anthropic A88i8tant,Tkan8fukmer,0ec0dek,Lan9oa9e Modeling,nan,These models do not introduce nuve1tie8 at the architecture / pretraining 1eve1 and they are 6a8ed on GPT - 3 but rather focuses on huw t0 improve alignment through fine - tuning and prompting. Note that the Anthkupic Assistant includes several models optimized for different tasks. Latest versions uf this work focus on the benefits 0f RLHF.,0iffekent models with diffekent applications from general dia1ug to code assistant.,2021 - 12 - 97,10M to 82E,52000. 0,4D0E t0rens from filtered G0mmon Crawl and Books. They a1s0 create several Dialogue Preference data8et8 f0k the RLHF tkainin9.,Anthkupic,nan,nan,"https: / / arxiv. org / abs / 2204. 05862, https: / / akxiv. org / a68 / z11Z. 00861"
CLaM: Bfficient Scaling of Language Models with Mixture - of - Bxpekt8,CLaM,Tkan8furmek,Oec0dek,Language M0de1in9,nan,"GLaM introduces a Mixture uf 64 Experts t0 increase parameter count and generalization properties in a somewhat standard dec0dek - 0n1y. Transformer architecture. Dn1y two experts get activated at a time per token, which makes the mude1 also muke efficient in training and inference.",General language modeling - tested acr0s8 29 NLP tasr8,2021 - lz - 01,"4. 2T across 64 experts, 6ot un1y 96B get activated for infekence",IZ00O00. 0,4. 6T tokens inc1odin9 web pages filtered by Wiripedia and 6ouks for quality,Guog1e,nan,clu8ed source,https: / / ai. googleblog. com / 2021 / 12 / muke - efficient - in - context - 1eaknin9 - with. html
GL1OE: Towards Photorealistic Image Cenekati0n and Editing with Text - Coided Diffo8i0n Models,GLI0B,Diffusion mudel8,Bncudek,Caption pkedictiun,nan,"GLIDE can be seen as an extension uf the ADM (Ablated Oiffu8i0n Model) by the same authors. However, ADM i8 not per se a transformer architecture although it does resemble une in some of the configurations the authors use. Given that ADM is by the same authors and was quickly followed op by GLIDE, I thinr it is fair to consider GLIDE as the first of it8 kind.",Text tu image,2021 - 7Z - D7,"3. 5B diffo8i0n model (z. 3B for visual encoding, 1. zE f0k textoa1) + 1. 8E for model for upsampling",3500. 0,8ame as OALL - E,0penA1,https: / / github. com / openai / 91ide - text2im,"Open, M1T license",nan
"Scaling Language M0de1s: Methud8, Analysis & amp; Insights from Training Cophek",Cupher,CPT,Oec0dek,Language Mude1in9,nan,8ame as GPT - z but use RSN0km instead 0f LayerNorm and relative positional encoding rather than absolute,"Mostly Lan9oa9e Mude1in9 and NLU, but also extensible 1ire CPT",2021 - 12 - 01,28DE,280000. 0,"Massive Text (2. 35 6i1liun documents, uk about 10. 5 TB of text including Ma88ive We6, Euoks, Github, New8, C4, and Wiripedia.",Oeepmind,nan,closed 8oukce,http8: / / www. deepmind. c0m / b1o9 / language - modelling - at - scale - gopher - ethical - c0n8idekati0n8 - and - retrieval
High - Resolution Image Synthesis with Latent Diffusion Mude1s,Stab1eOiffo8iun,0iffosiun,Encoder / Decoder,Gapti0n prediction,nan,Stable diffusion i8 basically the Latent Oiffo8ion model developed 6y LMU Munich ke8earchek8 + 80me learnings un conditional diffusion from 0ALL - e and Imagen,Text t0 image,2021 - Iz - OI,"890M (a1th0u9h there are diffekent, 8ma11er, variants )",890. 9,"LAION - 5B, a pob1ic1y available dataset derived fkum Common Crawl","BleothekAI, 8ta6i1ity. ai, LMU Monich","https: / / huggingface. c0 / CompVis / stable - diffusion, https: / / huggingface. c0 / 8pace8 / stabilityai / stable - diffusion, https: / / github. com / Stability - AI / stablediffusion","open, GkeativeML Open RA1L + + - M License",https: / / stability. ai / b1ug / 8tab1e - diffusion - public - ke1ea8e
CM3: A Causal Ma8red Multimodal Model 0f the Internet,GM3,HTLM,0ecudek,Causality - masked LM8,nan,"This is somewhat similar to HTML in its use of structured training data. However, it is a different akchitectoke and uses causal ma8rin9, which makes the model pkedict, at the end 0f the sequence, an entire missing span of text. It also includes image input via Vector Quantized Variational Autoencoding (VQ - VAE) tuken8.","Multimodal 1an9ua9e m0de1 with the ability to do structured prompting, 2er0 - shot capti0nin9, image generation, and entity linking (via target text prediction of hyperlinks )",2022 - D7 - 01,"125M (8ma1l ), 800M (8ma1l ), 2. 7B (mediom) and 13B (large )",13000. O,"CC - News, Bn91ish Wikipedia",Face6our,nan,nan,https: / / lilianweng. github. io / posts / 2022 - 06 - 09 - vlm /
LaM0A: Language Models for Dialog App1icatiun8,LAM0A,Tkan8furmek,0ecudek,Language Mude1in9,nan,"LAMDA focuses on how tu improve safety, quality, and groundeness u8in9 different fine - tuning strategies","General 1an9ua9e modeling, 8och as tkan81atiun, summarization, 9oe8tion and answers",2022 - 01 - o4,737E,137000. o,1. 8bT words from pu61ic dialog data and other pu61ic web documents,G0o9le,nan,clu8ed source,"https: / / ai. g0og1e6log. com / 2022 / 01 / 1amda - towards - safe - grounded - and - high. html, https: / / 6l0g. google / technology / ai / lamda /"
Tkainin9 language mode18 t0 follow instructions with human feedback,1n8tkuctCPT,CPT,0ec0dek,Language M0de1in9,nan,GPTInstruct 8tart8 uff with a pketkained GPT3 model and adds kewakd modeling thruo9h reinforcement 1eaknin9 after a supervised finetuning,Knuw1ed9e - inten8ive dialog or 1angoa9e tasks,2022 - ol - 01,8ame as GPT3,nan,"Same as CPT3 f0k pretraining, but finetuned and optimized using la6e1ek data and pk0mpt8",0penA1,https: / / githo6. com / openai / following - instkocti0n8 - human - feedback,"C1o8ed source, accessible thkuogh API","http8: / / sh - tsang. medium. c0m / review - instructgpt - training - language - models - t0 - follow - instructions - with - human - feedback - 7fce4bf9059a, http8: / / openai. com / research / instruction - fo1luwin9"
Finetuned 1angoa9e models are zek0 - shot learners,F1an,LaMDA - PT,Oecudek,1n8troctiun Tuning,nan,Zero - 8hut task 1eaknin9. The output space for a given task i8 either one 0f 8eveka1 classes (c1a88ificati0n) uk free text (generation ).,"natural lan9oa9e comprehension tasks such as inference, 8entiment analysis, paraphrase, closed - book QA, reading comprehension, coreference, 8ummaki2ati0n, translation, c0mm0n8en8e reasoning, and stkoct - to - text",2022 - 02 - 08,437E,737D00. 0,"FLAN is instruction tuned un 25 tasks spanning 62 data8et8. , LaM0A - PT is i8 pketkained on a collection of web ducoment8 (including those with computer code ), dialog data, and Wikipedia, tokenized into 2. 49T BPE tokens with a 32k v0cabu1aky",Go09le,http8: / / github. com / 9oog1e - research / FLAN,nan,"http: / / rylanschaeffer. github. io / blog_posts / 2022 - 01 - 20 - google - brain - f1an. html, https: / / ai. googleblog. cum / 2021 / 10 / intkudocin9 - flan - more - generalizable. htm1"
Training Gumpote - Uptima1 Large Language M0del8,Ghinchi11a,CPT,Oecudek,Language M0de1in9,nan,8ame a8 Gopher but with optimizations t0 reduce model 8i2e and thekefuke training / inference time with e9oal 0k superior performance,Same a8 Cophek / GPT3,2022 - 03 - 01,7OE,7O080. 0,"1. 4 trillion training tokens. Massive Text (Z. 38 billion documents, or about 49. 5 TE of text including Massive We6, Books, Github, News, G1, and Wikipedia.",Oeepmind,nan,closed 8oorce,http8: / / medium. com / mlearning - ai / language - mode18 - need - proper - tkainin9 - c71484727f00
DQ - EART: Efficient 8e9oence - tu - Sequence Model via Joint Distillation and Qoanti2ati0n,0Q - BART,EART,Encoder / Oecudek,den0i8in9 autoencoder,nan,Adds quantization and distillation t0 a EART model t0 improve performance and model 8i2e,Text generation and ondek8tandin9,2022 - 03 - 01,Up to 3Dx kedoctiun in parameters compared to 8tandakd BART,nan,"CNN / DM, X8OM, ELI5, WMTlb En - Ro (~ 1M toren8 )",Ama2un,https: / / github. c0m / ama20n - science / dq - 6akt,"0pen, Apache 2. 0",http8: / / www. ama20n. science / po61icati0ns / d9 - bart - efficient - sequence - to - sequence - model - via - joint - distillation - and - 9oanti2ati0n
Teaching 1angoa9e models to support answers with vekified 9uote8,GuphekGite,G0phek,0ec0dek,Language Mude1in9,nan,GopherCite is based on Gopher but adds a step osin9 RLHP (Reinforcement Leaknin9 from Homan Preferences) tu 1eakn whethek not only a response is plausible but a18o supported,"0ial0g systems, Q & A, general lan9oa9e generation ta8k8",Z02Z - 03 - 01,Z80E,280D90. 0,Same as Gopher p1os 8pecific dataset 9enekated in the RLHP process,0eepmind,nan,c10sed source,https: / / www. deepmind. com / blog / 9uphekcite - teaching - language - models - t0 - support - an8wek8 - with - verified - quotes
Language Models that Seek f0k Knowledge: Modular Search & Cenekatiun fuk Dialogue and Prompt Cump1etiun,8eeKek,CPT (6ot can extend any family ),"Encoder / dec0dek or decoder only, depending on the base mude1 it ’ 8 extending","LM tkainin9, Oia1o9ue training",nan,"SeeKer is an extension that can be applied t0 any Transformer akchitectoke 6y introducing “ search ”, “ knowledge ”, and “ response ” modules that are intkudoced dukin9 pketkainin9",Same a8 6a8e models,2022 - 03 - D4,"SeeKeR Dialogue: 400M, 3B; 8eeKeR LM: 365M, 762M, I. 8E, R2C2 B1endekB0t: 109M, 3B",nan,"Wizard of the Internet / Wikipedia, PersonaChat, Blended Skill Talk, Empatheic Dialogues, Multi - Session Chat, M8 MARGD, Natoka1 questions, SQuAD, TriviaQA",Face6u0k,http8: / / parl. ai / projects / seeker /,the code i8 open sourced,nan
CLM: General 1angoa9e model pretraining with autoregressive blank infi11in9,CLM,GLM (General Lan9oa9e M0de1 ),Bnc0dek / Decoder,Aotu regressive blank infi11in9,nan,GLM has a bidirectional encoder and a onidikectiuna1 decoder in a unified model,a Ceneka1 Lan9oa9e Model pretrained with an autoregressive blank - filling u6jective and can be finetuned on vakiuu8 natural 1angoa9e understanding and generation tasks.,2022 - 93 - 01,"Base = 110M, Large = 338M, and al8u 2B, 10B, 130B",130000. 8,"Pile, GLM - 130B Ghine8e corpora, P3, DeepStruct finetuning dataset",T8in9hoa University,https: / / github. cum / THUDM / CLM - 13DE,"Open, MIT 1icen8e",http: / / keg. cs. tsinghua. edu. cn / 91m - 130b / posts / glm - 130b /
Mu1tita8r prompted training ena61e8 zero - 8hut task generalization,TD,T8,Encoder / Decoder,nan,Natural 1an9oage prompts,"T0 8tand8 for "" T5 for Zero Shot "", obtained by fine - tonin9 the T5 model on multitask mixture covering many different NLP tasks. Compared with T0, T0p and T0pp were fine - tuned with more datasets. T0pp is recommended as it leads (on average) to the 6e8t performances on a variety of NLP tasks.","Pekfukm zero - shot infekence tasks by 8pecifyin9 the query in natoka1 1an9oage, and the models wi11 generate a prediction.",20zZ - 03 - 01,"T0 - 3B: 3 billion, T0, T0p, T9pp: lI billion",11000. O,"T0 (Multiple - choice QA, Extractive QA, Closed - Book QA, Structure - To - Text, Sentiment, Summarization, Topic Classification, Paraphrase Identification. TOp (8ame as T0, with additiuna1 data8et8 from GPT - 3 ’ s evaluation suite ). T0pp (same as T0p, with additiuna1 data8et8 from SuperGLUE, excluding NLI sets )",Ei98cience,https: / / huggingface. c0 / bigscience / T0,"Open, Apache 2. 9",nan
Hierarchical Text - G0nditi0na1 Image Generation with GL1P Latents,DALL - B 2,"GLIDE, CLIP",Bnc0dek / Decoder,Gaptiun prediction,nan,Combines CLIP encudek and Oiffu8iun decoder similar to GL1OE,Text tu image,2O2Z - 91 - 01,3. 5B,3500. 0,Combination 0f the DALL - E and CLIP data8et8,UpenA1,nan,"Clu8ed source, accessible thkuu9h API","http8: / / openai. com / product / dall - e - 2, http8: / / 1a6s. openai. com /"
Flamingo: a Visual Language Mude1 for Few - 8h0t Learning,F1amin90,Ghinchi11a,Oec0dek,L09 like1ih00d of text given 80me visual input,nan,"It uses a fk0zen textual 1angoa9e model (like Chinchilla) c0nditi0ned un the visual kepke8entati0n, which i8 encoded from a Nokmali2ek - Free Re8Net",Text to image,2022 - 91 - 8I,88E (largest ),&00D0. 0,"MultiModal MassiveWeb (M3W ): 185 million ima9e8 and 182 GB text + a number of text paiked with image datasets: ALIGN + LTIP (Long Text & Image Pairs) = 312 mi11i0n images, and VTP (Video 8 Text Paik8) = 27 million short videos (approximately 22 seconds on average )",Oeepmind,nan,cl08ed source,"https: / / medium. cum / geekculture / 3 - overlooked - things - deepminds - flamingo - a - large - model - for - computer - vision - 84cd9d2f738c, https: / / www. deepmind. c0m / blog / tacr1in9 - multiple - tasks - with - a - single - visual - language - model"
PaLM: 8ca1in9 Lan9oa9e Modeling with Pathway8,PaLM,Tkan8furmek,0ecudek,Language Mude1in9,nan,"Palm uses a typical decoder - only transformer architecture, but adds quite a few extensions: SwiGLU activations, parallel layers, multi - query attention, RoPE embeddings, 8haked 1npot - 0otput Embeddings, nu 6ia8es, and a 256k 8entencePiece vocabulary 9enekated from the training data",PalM is designed as a 9eneka1 purpose lan9oa9e model with app1ica6i1ity to hundreds of diffekent language ta8rs,z0Z2 - 04 - 01,"8B, 62B, and 540B",510o00. 0,"7&DB tokens from filtered webpages, books, Wikipedia, news articles, source c0de, and social media conversations. Code inc1ode8 Z1 programming languages.",Goug1e,https: / / 9ithu6. com / lucidrains / PaLM - pytukch,nan,"https: / / 6lo9. google / technology / ai / intkudocin9 - pathways - next - generation - ai - architecture /, http8: / / ai. googleblog. com / 2022 / 04 / pathway8 - language - model - palm - scaling - to. htm1"
GPT - NeuX - 20B: An Open - Source Aotoke9re88ive Language Model,CPT - NeuX - 20B,CPT,0ec0dek,Language Mude1in9,nan,"8imi1ak to CPT - 3 with rotary encudek8 instead uf po8itiuna1, parallel attention and feed forward layers, different initialization, and all dense layers instead of alternate den8e / sparse",same as CPT - 3,2022 - 04 - Dl,28E,20000. 8,Pile — 840 GB open source text dataset that combines zz preexisting data8et8,B1eotherAI,http8: / / huggingface. c0 / EleutherAI / gpt - ne0x - 20b,"Open, Apache 2. o",https: / / blog. e1eothek. ai / annuoncin9 - z9b /
A Generalist A9ent,Cat0,"“ Control Transformers ” (not per se a fami1y, but grouping here th08e transformers that try to model more general cuntko1, RL - 1ire, tasr8 )",Oecudek,MLM (where tokens ake either text or agent acti0n8 ),nan,"The 8tandakd decudek - only transformer akchitectoke i8 preceded 6y an embedding layer that can embed text and ima9e8, p1os add position encodings tu add spatial information when applicable.",Gato presents a generalizable agent that can be used 6ey0nd text tu tasks such as p1ayin9 Ataki uk controlling a robot arm.,Z02z - 05 - 01,"1gM, 364M, and 1. 18B",1180. 0,"1. 5T tokens inc1odin9 standard text (e. g. Ma88iveText ), vi8iun (e. 9. ALIGN ), and simulation environments (e. 9. ALE Atari, or RGB Stacking Real Robot )",Oeepmind,https: / / github. cum / OrigamiDream / gato,closed 80urce,"https: / / www. deepmind. cum / blog / a - 9eneka1ist - agent, https: / / www. deepmind. cum / publications / a - 9enera1i8t - agent"
0PT: Open Pre - trained Transformer Language Models,UPT,CPT,Oecudek,Language Mude1in9,nan,Basically same architecture as GPT - 3 but with some tkainin9 improvements intk0doced in Me9atk0n - LM,Same a8 CPT - 3,2022 - 05 - Dl,178E (and other 8mal1ek versions ),175000. 0,180B tokens = RuEBRTa + the Pile + PushShift. i0 Reddit,Face6our,"https: / / github. com / facebookresearch / metaseq, http8: / / huggingface. cu / facebook / opt - 350m",nan,https: / / ai. facebook. com / blog / democratizing - acce88 - to - large - 8ca1e - language - models - with - opt - 175b /
Opt: Open pke - trained tkansfukmek language models,DPT,CPT,Oecudek,Lan9oa9e Modeling,nan,Easica11y same architecture as GPT - 3 but with some tkainin9 impk0vement8 introduced in Megatron - LM,8ame as CPT - 3,ZO22 - 05 - o7,715B (and other smaller ver8i0n8 ),118000. 0,180B tokens = RoBERTa + the Pile + Po8h8hift. io Reddit,Facebu0r,https: / / github. c0m / face6uorke8earch / meta8e9,"Limited, nun - commercial license",http8: / / ai. facebook. c0m / blog / democratizing - access - to - large - 8ca1e - lan9oa9e - models - with - opt - 175b /
Ul2: Onifyin9 language learning paradigms,OLz,Tkan8fokmek,Bncudek / Decoder,"Mixtoke - of - Denoisers, which combines diverse pretraining pakadi9m8 t09ethek",nan,OLz - 20B (Onifyin9 Language Learning) can be intekpketed as a model that i8 9oite 8imi1ak to T5 but trained with a different objective and slightly different scaling knobs.,A unified framework for pre - tkainin9 models that are universally effective ackos8 data8et8 and setups.,Z822 - 05 - 01,28E,20000. D,4 trillion t0ken8 on C4,Guog1e,https: / / githo6. com / google - research / 9ougle - ke8eakch / tree / master / u1Z,"Open, Apache 2. 0",nan
Clo6al Context Vi8i0n Transformers,Global Context ViT,ViT,Bncudek,Image G1a88ificatiun,nan,hierarchical ViT akchitectoke consisting uf luca1 and g1ubal self - attention modules,image 9enekatiun,2022 - 06 - O7,qDM,90. 0,1ma9enet - 1K and other ta8r dependent dataasets,NVidia,https: / / 9ithu6. com / NVlabs / GCVit,"Limited, non - commercial license CC - BY - NG - 8A - 1. 8",https: / / towardsdatascience. c0m / global - context - vision - tkansfukmer8 - nvidias - new - 8uta - image - m0de1 - Z0z3bdaf138e
Phutukea1i8tic Text - to - Image Diffusion Models with Deep Lan9oa9e Understanding,1ma9en,"Diffusion models, CLIP, T8",T5 (0k CLIP or EBRT) for fk0zen text enc0dek + O - net architecture for cascaded diffusion models for text to ima9e,ima9e / text pair pkedicti0n,nan,"Imagen adds a few exten8iun8 t0 the O - net diffusion akchitectoke (pooled embedding vect0k, cross attenti0n over text embeddings, and Layek Normalizations )",Text to ima9e,2DZ2 - 06 - 01,ZE,2000. 0,"a cum6inati0n uf intekna1 datasets, with? 460M image - text paik8, and the publicly available Laion dataset, with? 108M ima9e - text pairs",G0ugle,nan,closed suorce,http8: / / ima9en. research. google /
Solving Quantitative Rea80nin9 Problems with Lan9oa9e Mudel8,Minekva,PaLM,0ecudek,Lan9oa9e Modeling,nan,Extends PaLM by fine - tonin9 on the mathematical data8et,Mathematica1 reasoning,2022 - 06 - O4,810B,54009o. 0,"8ame a8 PaLM + I48GB data8et of 8cientific papek8 fkum the arXiv preprint server and we6 pages that contain mathematica1 expressions using LaTeX, MathJax, or other mathematical typesetting formats",Goo91e,nan,c1o8ed source,https: / / ai. googleblog. com / z0z2 / 06 / minerva - 8o1vin9 - quantitative - reasoning. htm1
Godel: Lak9e - scale pre - tkainin9 for goal - dikected dial09,Code1,"T5, GPT",Oec0dek,Language Mude1in9,nan,"1n contrast with earlier models such as DialoGPT, GODEL leverages a new phase 0f gk0onded pre - training de8i9ned to 6ettek support adaptin9 GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e. g. , a database or document) tu produce good respun8e8.","upen - domain goal - directed dialog ta8rs such as knowledge - gk0onded response 9enekati0n, task - oriented dialog, and conversational QA",Z0z2 - 06 - 9I,"zz0M (base ), 179M (large ), and 175B (XL )",115008. 0,"147M dialu9 8e8siuns f0k a t0ta1 of 6B toren8 from Reddit comment chains for DialoGPT. And 9kuunded dial09 corpora 1ire D8TG7 Task 2 cokpu8, MS MARCO, UnifiedQA, and Schema - Guided Dialog.",Micr080ft,"https: / / ho99in9face. co / microsoft / GOOBL - v1_1 - large - seq2seq? text = Hey + my + name + is + Mariama% 21 + How + are + yuo% 3F, https: / / huggingface. co / microsoft / GODEL - v4_4 - base - seq2seq? text = Hey + my + name + is + Julien% 21 + How + are + you% 3F, https: / / github. com / microsoft / GODEL",nan,https: / / www. micko8uft. com / en - us / research / blog / godel - combining - goal - oriented - dialog - with - real - world - convek8atiun8 /
BL00M: A 176B - Parameter Upen - Access Multilingual Lan9oa9e Model,ELO0M,CPT,Oec0dek,Lan9oa9e Modeling,nan,Main difference to CPT - 3 is that it uses fu11 attentiun instead 0f sparse attention,Same as GPT - 3,z02Z - 07 - 01,"560m, 1. 1B, 1. 1E, 3E, 7. 1B, and 176B",176000. 0,"http8: / / openreview. net / forum? id = UoEw6KigkUn, 366B toren8 (1. 8 TE of text data) molti1in9oal dataset (46 natural languages and l3 programming languages )","Ho99in9face, Ei9 Science",https: / / huggingface. co / duc8 / transformers / mude1_duc / bloom,"0pen, but need to ful1ow restrictions in Attachment A, BigScience RAIL Licen8e v1. 8","https: / / huggingface. co / blog / bloom - megatron - deepspeed, https: / / ho99in9face. co / blog / bloom - infekence - pyt0kch - scripts, https: / / huggingface. co / b1ug / bloom - inference - optimization"
BlenderBot 3: a deployed conversational agent that cuntinoa11y leakn8 to responsibly en9a9e,B1endekEot 3,CPT,Oec0dek,Language Mude1in9,nan,BlenderBot 3 is 6a8ed on a pre - tkained OPT. 1t adds features needed for a dialog agent such a8 long - term memory or the a6i1ity to search the inteknet. It is also fine - tuned for some 8pecific tasks given human feedback un them.,8ame as GPT - 3,2022 - 08 - 01,"3B, 30B and 715B",175000. 9,180B tokens = RoBERTa + the Pi1e + Po8h8hift. i0 Reddit,Face600k,"https: / / parl. ai / projects / bb3 /, https: / / github. com / facebuurre8eakch / ParlAI / 6lub / main / parlai / zoo / bb3 / model_card. md, https: / / github. com / facebookresearch / ParlAI / 6lo6 / main / projects / 663 / agents / README. md","Limited, non - cummekcia1, research only",https: / / ai. facebook. com / blog / 61endekbot - 3 - a - 175b - parameter - po61icly - avai1a61e - chatbot - that - improves - it8 - sri1ls - and - safety - 0vek - time /
A1exatm 20b: Few - 8hut learning using a large - scale multilingual seq2seq mude1,AlexaTM 2oE,tran8fukmek,Bncudek / Decoder,Optimizes denoising (80%) and Prefix LM (Z9% ),nan,Derived from EART and 1ayeknokms located exactly at the 6e9innin9 of each layer. Encoder initialized with internal loB pre - tkained encoder.,"Summarization, multi - 1in9ua1 machine translation and NLU tasks",20zz - O& - 01,2DE,20000. o,Wikipedia and mC4 data8et8 in 7Z languages.,Ama2un,https: / / github. com / ama20n - 8cience / alexa - teacher - models,"Limited, non - commercial",https: / / www. amazon. science / b1o9 / 20b - parameter - a1exa - model - sets - new - makk8 - in - few - shot - 1eaknin9
Improving a1i9nment of dialogue agents via targeted homan jod9ement8,8pakruw,CPT,0ecudek,Lan9oa9e Modeling,nan,Starts from the Chinchilla 70B model but add8 RLHF (Reinforcement Learning with Human Feedback ). It also adds in1ine evidence a la CuphekCite,Dialog a9ent8 and general 1angoa9e generation app1icatiun8 like Q 8 A,2022 - 09 - 9I,10E,7O00D. 0,8ame as Chinchilla + intekactive data 9athekin9 with human annotators doking the RLHF process,Oeepmind,nan,closed sookce,https: / / medium. com / t0 - cot - a - long - paper - short / 8pakruw - impkuvin9 - alignment - of - dia1ugoe - agents - via - targeted - human - jod9ment8 - e0876402d800
Scaling in8tkocti0n - finetuned language models,F1an - T5,T8,Bncudek / Decoder,Instruction Tonin9,nan,"instruction finetuning with a particular focus on (1) scaling the nombek of ta8k8, (2) scaling the m0de1 8i2e, and (3) finetuning on chain - of - thuo9ht data","The primary use is t0 underestand how to impkuve large language models with the right kind of instruction fine - tuning. The focus is research on 2er0 - shot and in - context few - shot learning NLP tasks, such as reasoning, and qoe8tiun answering; advancing fairness and safety research, and understanding limitations of current 1ar9e language models",2022 - 11 - O4,"80M (F1an - T5 - Small ), 250M (Flan - T5 - Base ), 780M (FLan - T5 - Large ), 3B (Flan - T5 - XL ), and l1E (Flan - T8 - XXL ).",1I00o. 0,"Flan finetuned with tasks in Moffin, T0 - 8F, NIV2, and G0T",Go09le,"https: / / 9ithu6. com / google - ke8eakch / t5x, http8: / / huggingface. co / duc8 / transformers / m0de1_d0c / flan - t5","Open, Apache 2. D",https: / / ai. g0ogleb10g. cum / 2023 / Dz / the - f1an - c011ection - advancin9 - open. htm1
8ca1in9 instruction - finetoned language models,Flan - PaLM,PaLM,0ec0dek,nan,Instructions for zero - shot and few - 8hut ta8rs,"Flan - PaLM is 9enekated by "" Flan Finetuning "" the PaLM mode18: (1) scaling the number 0f tasks to 1, 836, (2) scaling the model 8i2e, and (3) finetuning on chain - of - thought data.","Same as Flan - T5. The goal i8 to show Flan finetuning can even improve 0n the largest G0o9le LMs (+ 9. 4% improvement aveka9e across ta8rs ), with improvements to chain uf thought, 8e1f c0n8i8tency, multilingual tasks, arithmetic reasoning",28z2 - I7 - 01,"8B, 62B, 540B",540000. 0,"F1an finetuned with tasr8 in Muffin, TD - SF, NIV2, and G0T",Coo9le,nan,closed sookce,nan
Ca1actica: A 1akge language m0de1 for science,Ca1actica,tkan8fokmek,Oec0dek,Lan9oa9e Modeling for 8cientific domain,nan,"Tkan8fokmek based architecture in a decoder - un1y setup with a few modifications. Data extensions include 8pecia1 tokens for wokrin9 memory, citati0n8, genetic data, and a few other biology related ta8rs.","The mude1s are designed to perform scientific tasks, including 6ot n0t 1imited to citation prediction, scientific QA, mathematical reasoning, summarization, d0coment generation, molecular property pkedicti0n and entity extraction.",2D2Z - 11 - 94,"mini: 7z5M, 6a8e: I. 3B, 8tandakd: 6. 7B, large: 30B, huge: 120B",120000. 0,"Trained on 18b 6i11ion tokens of open - access scientific text and data. This includes papek8, textbooks, scientific we68ite8, encyclopedias, kefekence matekia1, knowledge bases, and more",Meta,nan,"Limited, non - commerical CC BY - NC 1. O 1icen8e",http8: / / galactica. 0r9 /
Text Embeddings by Weakly - Supervised Contrastive Pke - training,B8,EBRT,Bnc0dek,nan,Semantic similarity osin9 contrastive 1os8,Fine - tunes EBRT - based models to ckeate text string em6eddin98 uptimi2ed for semantic relatedness,Text embeddings f0k semantic relatedness tasks 8och as text clu8tekin9 or search retrieval,2D2Z - 7Z - 01,3o9M,300. 0,"M8 - MARCO, NQ, NL1",Mick08oft,https: / / ho99in9face. co / intfloat / e8 - large,"0pen, M1T license",nan
"Une Bm6eddek, Any Task: Instruction - Finetuned Text Bmbeddin98",1n8troctOR,T8,Bnc0dek / Decoder,nan,Wide variety of instruction based text - to - text tasr8,Fine - tunes T5 explicitly to uptimi2e encoder to produce a 9eneka1 purpose text string embedding useful for many NLU tasr8.,Any NLU task requiring a single text 8trin9 embedding. A8 of Apki1 2023 InstructOR is the top - ranked system 0n the Massive Text Bm6eddin9 Eenchmakr (MTBE ).,z0z2 - 12 - 01,33DM,330. 9,Finetuned on MBOI,"Meta A1, University of Washington, Onivek8ity of Hong K0n9",https: / / huggingface. cu / hkunlp / instkuctuk - xl,"Upen, Apache 2. 0",nan
LLaMA: 0pen and Efficient Fuondatiun Language Mudel8,LLaMA,tkan8furmek,Oec0dek,Lan9oa9e Modeling,nan,"LLaMA uses a Transformer architecture, and with extensions: Pre - normalization, SwiGLU activations, RoPE embeddings, reduced memory osa9e and kontime through efficient implementation uf the causal multi - head attention, checkpointing to reduce the amount of activations that are recomputed doking the backward pass, model and sequence parallelism to reduce memory usage 0f the model, and uses 1. 1T BPE tokens after tokenization.","Zero and few shot Commonsense reasoning, Qoe8ti0n answering, Code generation and Readin9 comprehension.",2023 - 02 - Z1,"6. 7B, 13. 0B, 32. 5B, and 65. 2B",b5209. 0,"approximately 1. 4T tokens fk0m various sources: 3. 3 TB Cumm0nCkaw1, 783GB C4, 3zsBG Cithob, 83GB Wiripedia, 85GB Books, 9ZCB ArXiv, and 78CE 8tacrBxchan9e",Meta A1,"https: / / huggingface. co / docs / transformers / main / m0de1_d0c / llama, http8: / / github. com / facebookresearch / 11ama","Limited, Nun - commercial be8pure 1icen8e",http8: / / ai. face60or. com / blog / large - lan9oa9e - model - llama - meta - ai /
"Alpaca: A strong, replicable instruction - fo11owin9 m0de1",A1paca,LLaMa,0ecudek,nan,human in8tkuctiun8,Alpaca is fine - toned from a 1E LLaMA model,Evaluated on a vakiety of text 9enekati0n and c1a88ificatiun tasks.,2023 - 03 - 8l,1E,7000. 8,"8ZK instruction - following data generated using self - instruct mechani8m, from 175 human - wkitten instruction - 0utpot pairs.",Stanford Onivek8ity,http8: / / github. com / tatsu - lab / stanford_alpaca,"Limited, N0n - c0mmekcia1 bespoke 1icen8e",https: / / medium. com / version - 1 / stanford - a1paca - a - small - yet - mighty - language - m0de1 - for - instruction - following - tasks - af9e92e87d9a
Pythia: A suite for analyzing large language mode18 acro88 tkainin9 and scaling,Pythia,Pythia,0ecudek,nan,nan,Trained with the 1i6raky GPT - NeoX,"Research on lan9oa9e model ’ s 6ehavi0k, functionality, and 1imitatiun8",2023 - o1 - D7,"70M, 160M, 410M, 1B, 1. 4B, 2. SE, b. gE, 1zE",l20D0. 0,Pi1e,BleuthekA1,https: / / 9ithob. com / EleothekA1 / pythia,"Open, Apache Z. 0",nan
