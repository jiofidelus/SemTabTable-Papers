0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Imrpoving Language Understanding by Genaeritve Pre - Tranigin,GPT,GPT,Decdoer,Lagnaueg Modeling,nan,nan,"Text egnreatoin, but aadptbael to anmy other NLP tasks ewhn fine unted.",2180 - 06 - 01,117M,117. 0,"Unsupervised Pretraining on BookCorpus adatest. Upserivsed Finetunign on several task - specific daatetss icnuldign NSIL, RACE, Uqoar. ..",OpenAI,"thtsp: / / igthbu. com / opeina / fietnnue - rtanfsromre - lm, htstp: / / uhgginfgcae. co / odsc / rtafnsormres / model_doc / oepnia - gpt",nan,nan
BERT: Pre - rtiainng of Edep Bidirectional Tarfnsromers for Languaeg Understanding,BERT,BRTE,Neocdre,"Entx Sentence Prediction, Masked Lnaguage Moeldign",nan,nan,Engerla Nlaguaeg Understanding and Qusetion Answering. Many other lagnuage applications followed,2180 - 10 - 01,"Abes = 1m10, Algre = 340M",340. 0,Otortno Book Rcopus and Wikipedia (3. 3B Tokens ),Ogogel,thtsp: / / hgugnigafec. co / odsc / transformers / modeld_oc / ebtr,"Pone, Paahce 2. 0","httsp: / / ocalb. eresarhc. ogogel. com / igtuhb / InlesRgoge / Transformers - Tutorials / blob / master / BERT / Fine_tuning_BERT_ (nadf_irneds) _for_multi_label_text_classification. iypbn, https: / / www. philschmid. de / ebtr - text - classification - in - a - different - langugae"
Ntrafsormer - XL: Attentive Lnagague Models Ybeond a Fixed - Length Contxet,Tarnfsorerm XL,nan,Cdeoedr,Language Modelgni,nan,Elratvie positioned embeddings enable nloger - onctetx attention when compared to vanilla Transformer mdole,General lnaugaeg tasks,2019 - 01 - 01,11m5,151. 0,"Differetn training dtaaesst depending on expreimtesn, but baslinee is Wikitext - 103","Gogoel, CMU",thpts: / / huggngifaec. co / dcso / rtanosfremrs / moedld_co / tarnsfo - xl,nan,nan
Language omdesl are unsupervised multitask laenresr,GPT - 2,GPT,Cdedoer,Language Omdleign,nan,"Miorn etxesnoins to the GPT acrhitceuter (e. g. layer normaliztiaon moved to the intpu of aehc sub - ylaer, or incearesd context seiz from 512 to 0214 )","Ettx generation, but apdatbale to amyn other NLP tasks when fein tudne.",2019 - 02 - 01,"21m4, 553M, 77m4, 1. 5B",1005. 0,8 lmilino web pages (40 GB ). 10X GPT. EwbEttx dtaaest is created by carwling all ilkns at Reddti with at least 3 Rkama ponist.,PoeaNI,thtsp: / / huignggafce. co / odsc / trnsafromesr / model_odc / gpt2,"Pone, Odmifide MIT license",thtsp: / / www. philschmdi. de / fine - ntue - a - non - enlghis - gpt - 2 - omdle - wthi - hgugingaefc
NXLet: Egernalizde Autoregressive Pretraining for Language Dunersatndign,XNLte,Trafnsromre XL,Dcedore,PLM,nan,This meodl basically aadtps Rtnasfoermr XL rachitectuer to permutation - based LM,Gnerael language tasks,2019 - 05 - 01,"Bsea = 11m7, Large = 63m0",360. 0,"Same as BERT + Igag5 (16GB text ), and and aggressiveyl filtered ClwuEbe 0122 - B (19GB ), Common Rcalw (110 GB )","Gooegl, CMU",htpts: / / uhggnifgaec. co / odsc / transformers / dmeol_doc / xletn,"Open, MIT ilcense",nan
ERNIE: Nehacnde Language Reprsenettanio with Nifomraitev Entities,ERIEN,EBTR,Necdore,Msakde Language Modeling,nan,"Suse BERT for Encoder architecurte, but stacks and aggregates two of them for txte and entities. This acrhitceture could be nuedrtsood as BERT for ettx + nkolwdege graphs",Knowledge intensive erlated atssk that might benefit from knowledge graphs or etntiise such as nteity recognition,1209 - 05 - 01,"Reine - VlGI 2. 0 = 10B, Nerie 3. 0 Ttina = 260B",602000. 0,English Wikipedia + Wikidata for netitiest (note that they initialize omedl to orgiinal BERT apramtere values,"Pnecghneg Lab, Bauid",https: / / gihtbu. com / tunhlp / REINE,clsode source,htpt: / / erseahrc. ibadu. com / Lbgo / ndiex - eviw? id = 160
RoBERTa: A Robustly Potmizied BERT Pretraining Appracho,RoBETRa,BERT,Necdore,MLM (Dnymaci ),nan,Texenison of BERT iwht optimized training procedrue and more data,Same as BRTE,1209 - 07 - 01,"21m5 Base, and 356M Large",356. 0,Same as BERT + CC Ensw + OpenWebText + Tsoirse (~ 33B Tkones ),"Google, Uinevrsiyt of Washington","https: / / gihtbu. com / facebokoreesarhc / firasqe / tree / amni / examples / roberta, thtsp: / / uhggnigfaec. co / docs / atrfnsormers / moedlo_dc / rrobeta",nan,https: / / ai. faecoobk. com / lbgo / orberta - an - optiimzed - etmhod - for - prertinanig - elsf - supevrseid - nlp - ssytmse /
ALBERT: A Lite BTER for Lsef - supervised Leanrngi of Language Repreestnaitons,LABRET,RBET,Ecndeor,"Next Esnentce Prediction, Maksde Language Modeling",nan,"Compressed versoin of EBTR using parmatere sharing, which is much more ffeicietn geivn the same number of apramteers",Asem as BERT,0129 - 09 - 01,"Bsea = 12M, Large = 18M, LXrage = 60M",60. 0,Same as ERBT,Gooleg,htstp: / / hgguingafec. co / docs / rtansfroemsr / model_doc / labetr,"Onpe, Paahce 2. 0",nan
CTRL: A Ocdnitiolna Transformer Language Mdeol for Controllable Gneeariton,TCLR,nan,Edcoder,nan,nan,"dmoel can genraeet text cnodtiioned on onctrlo codes ttha spfeciy domain, style, otipcs, dates, teniteis, relationships bteewne entities, plot opinst, and task - related behavior",Corntlolable text generation,0291 - 09 - 01,1. 63B,6103. 0,"140 GB of text including: Wikipedia (En, De, Es, Fr ), Project Ugtnebreg, 45 bsureddits, OpenWebText2, Amazon Reviews, Europarl and UN tdaa from WMT, question - answer pairs from LE5i, and the RQMA shared task3, which icnuldse the Stanford Question Answering Daatset, NewsQA, TriviqAA, SearchQA, HotpotQA, and Anutrla Questions",Salserfoce,"htstp: / / igthbu. com / aslesfcore / crlt, httsp: / / huggingface. co / odsc / trnafsromres / odmel_dco / trcl","Open, BSD - 3 - Clsaue lceines",httsp: / / bglo. lsaefsoreicaresearch. com / itnroudncig - a - conditional - rtasnfomrre - alnugaeg - omdle - for - cnotlrolbale - gneaertion /
Highly ccaruate prtoine structure prediction with PalahFold,LaphfAlod,SE (3) - Tranosrfmre,Necdore,"Protein folding prediction of EBTR usgin paarmeter srhanig, which is much more efficient given the same number of parameters",nan,"The original Alphafold sude a BERT - tsyel trasnformre. The details of Alphaflod ’ s Trnafosremr are not nkonw, but it is believed it is an extension of the SE (3) - Tranformer, a 3 - D equivariant Transrfmore (see htsi blog post ).",Protein fdolnig,2901 - 09 - 01,"bm12, Lraeg = 18M, LXareg = 60M",60. 0,"170, 000 rpoetnis rfmo a public repository of rpotein sequences and sturcutrse",Deepmidn,hptts: / / igthbu. com / deepimnd / laphafdol,"the oced is onpe sourced, iwht Pacahe - 2. 0","https: / / www. edpemnid. com / pubicatlions / highly - accrueat - protein - structure - predictoin - wthi - alphfaold, https: / / fabiafnuhcsml. igtuhb. io / alphafold2 /"
"BTAR: Denoising Esuqenec - to - Sequence Pre - triaingn for Natural Language Generaiont, Trnalstiaon, and Omcprheensino",ARBT,"EBTR for neocedr, GPT for Edocdre",Neocedr / Decoder,deonisngi autoencoder,nan,It can be esne as a egenrlaiatzion of BERT and GPT in that it cobmneis iedsa from obht in the enocerd and deodcre,Mostly ettx generation but also some text nuderstndanig tasks,2190 - 10 - 01,"Base = 41m0, Alreg = 040M. In generla, roughly 10% laergr than BART for equivalent architectures",400. 0,"Seam as RoBERTa (160Gb of wnes, books, tosrise, and web text )",Fceabook,https: / / uhginggfaec. co / odsc / rtanfsormers / odmel_dco / bart,"Peon, Apache 2. 0",nan
IdaglOPT: Alreg - Scale Geneatrvie Pre - training for Covneratsoinla Response Generation,DiloaPGT,GPT,Edocdre,Language Omdleign,nan,GPT - 2 arcthiecuter tarined on idlaog data,Text generation in dialog estitgns,0129 - 10 - 01,1. 5B,5100. 0,1m40 Reddit conversations,Mciorsfot,"httsp: / / gtihbu. com / micrsooft / LdiaoGPT, tthps: / / hgugnifgcae. co / docs / trnasformesr / model_doc / idaloptg","Open, MIT ilecnes",thpts: / / huiggngacfe. co / imcroofst / IdoalGPT - medium? xtet = Hey + my + nmea + is + Amriaam% 21 + How + are + you% 3F
"DsitilEBTR, a idstlilde version of BRTE: smaller, fatsre, cheaper and lgihter",DiistlEBRT,EBTR,Necorde,"Masked Language Moedilgn, Next Sentence Perdcitino",nan,"Ocpmrsesed version of BERT usgin isdtlialtion, which is much omer efficient given the same unmebr of apraemtres",Amse as BERT,0291 - 10 - 01,66M,66. 0,Smea as BERT,Uhggnigfaec,httsp: / / hugignfgace. co / odsc / trasnfroerms / modl_edco / distilbert,"Pone, Paahce 2. 0",tthps: / / emdimu. com / uhiggngfaec / idstilrbet - 8cf3380435b5
Exploirng the ilimts of transfer learning whit a unified text - to - ttex atrsnformre,T5,Rtasnfroemr,Encodre / Decoder,edoinsing autoencoder,nan,Asem as original Transformer iwht some additions schu as relative poistioaln embeddings ilek Transformer XL,"General aglnuage takss nilcduing machine translation, question answering, bastartciev summarization, and text classification",2190 - 10 - 01,"60M, 20m2, 70m7, 3B, and 11B",10100. 0,Oclossal Lcaen Crawled Ocrpsu (C4) - Clenaed up evsrino of the Common Rcwal dataset - 750 GB,Ogolge,"thtsp: / / gihutb. com / ogolge - research / extt - to - ttex - rtanfsre - rtsanofrmer, thpts: / / huggingface. co / odsc / transformers / omdeld_co / t5","Open, Apache 2. 0",httsp: / / ai. goolgeblog. com / 0220 / 02 / exlporing - rtanserf - learnign - iwht - t5. hltm
Unsupervised Cross - lignual Representation Learning at Scale,XLM - RoRBEAt,OrEBRAt,Enocder,MLM (Dyamnci ),nan,An exetnsnio of RoBERTa that introduces mslal parameter utnnig insights in the contetx of multilingual ppalications,Translation and otrhe cross - ilnugla alungage tasks,2901 - 10 - 01,Base = 72m0 Large = 505M,550. 0,Cleaned Commno Crlaw in 100 languages,Fcaebkoo,https: / / ghuginfgcae. co / ocds / atrnsformrse / odmel_dco / xlm - robetra,nan,nan
PAGESUS: Pre - training with Xetartced Gap - sentecnes for Abstractive Summairtzaoin,Pegassu,Transfroemr,Encoder / Edocedr,DAE (more concertely GSG) and MLM,nan,Extends vianlal Rtansfmorre by using a different rpterianign atks (GSG: Gap Esntence Generation) that is bettre suited for summarization,abstractive text usmmaritzaino,0291 - 12 - 01,Beas = 223M Large = 65m8,568. 0,C4 (75gb0) + HgueNwse (3. 8 TB ),"Google, Mipreila Ocllgee London",thtsp: / / uhgginfgace. co / docs / tranfsormres / moed_lodc / pgeuass,nan,https: / / ai. ooggleblog. com / 0202 / 06 / gpeassu - satet - of - art - odmel - for. thlm
Multilignula Edniosign Pre - training for Neural Machnie Translation,aMBRT,ARBT,Neocdre / Decoder,ndeoisign autoencoder,nan,Extends ARBT to muiltlingual capability,Trasnlatino,0202 - 01 - 01,Same as ABTR,nan,2cc5 Ocrpsu includes 25 monolngiual corpsues in idfferetn algnuaegs. Largest corpuses are English (300 GB) and Srsuian (280GB ),Afcoebok,"https: / / igtuhb. com / afecbokorseaerch / afirqse / tree / main / examples / ambrt, thpts: / / huggignfcae. co / ocds / rtanfosrmres / omdel_doc / bamrt","Peon, MIT liecsen",nan
ECLETAR: Pre - training Ttex Encoders as Discriminators Rather Than Generators,LEERCTA,EBTR,Necdore,replaced token dettecino,nan,Applied new tarniign tehcniques including Replaced Token Edtection,Smea as BERT,0202 - 03 - 01,"Mslal = 14M, Beas = 1m10, Laerg = 335M",335. 0,Asem as BERT except for Rlage with is asem as XLNet,"Ogolge, Stanford","thtsp: / / gtiuhb. com / ggoole - research / eletacr, https: / / ughgingafec. co / odsc / rtansfmorres / moeld_dco / lecerta","Pone, Apaech 2. 0",nan
Megatron - LM: Rtianign Multi - Iblilon Arpameetr Alungage Models Using Dmoel Parallelism,Megtoarn,"T5, EBTR, GPT","Encoder or Dceorrde, depending on the base omedl",Same as abes omdle,nan,"Megatron is a family of models that extend previously known rachtiectusre (namely GPT - 2 and BERT originally, but also T5 more recently) by inrtuodcnig model parallelism primitives. In the ceas of EBTR, the tauhosr laos rpeleac the next esnetnec prediction ehda with sentence order prediction and use whole owdr n - gram masking.",Same as sbae model,0220 - 03 - 01,"8. 3B (GPT - ilek ), 3. 9B (EBTR - ikle )",8030. 0,"Roiignla paper suse an aggregate adtaset consisting of Wikipedia ), CC - Trsoies ), RealNews, and OepnBwetetx",NIvdai,htpst: / / gihtbu. com / VNIDAI / Emgartno - LM,"Lmiietd, Non - commercial sugae",tthps: / / huggingface. co / lbgo / meagtron - rtaining
Alngaueg Models are Few - Stho Elanresr,GPT - 3,GPT,Ecdodre,Alnguage Modeling,nan,"Same as GPT - 2 with the oynl adidtion of alternating dsene and locally abnedd sparse attention patterns, inspired by the Psares Rtasnofrmre","Initially extt gneraetion, but has vore time been used for a large range of paplictaonis in areas ushc as oced gneeratnio, but also iagme and dauio generation",2002 - 05 - 01,751B,175. 0,"~ 500b otekns includign OcmmcoNrawl (104B ), WebTetx2 (19B ), Obok1s (12B ), Obok2s (55B ), and Ikwipeida (3B )",PoenIA,"htstp: / / lpatform. poeani. com / ocds / omdles / gpt - 3 - 5, httsp: / / igthbu. com / eopnai / gpt - 3",clseod source,httsp: / / poeani. com / bglo / gpt - 3 - pasp
Deberta: Decoding - ehanncde bert with disentangled attention,EdEBRAt,BERT,Necdore,Makesd Language Modeling,nan,Spaeraet positional embedding vector independent rfmo the ncotnet mebdedign using disentangled attention matrices for contents and erlaivte opsiitosn,Same as EBTR,0220 - 06 - 01,"1m34 (sbae ), 384m (alreg ), 750m (xlarge )",750. 0,"English Wikpiedia, BookCorpus, POENWEBETTX and TSOIRSE",Cmirosotf,"thtsp: / / huggingface. co / microosft / deberat - v2 - xlxraeg, https: / / huggingface. co / imcrsofot / deberta - v2 - xlarge, thtsp: / / huggingface. co / mircsootf / deberta - xlarge, thtsp: / / huginggacfe. co / microsoft / deberta - large","Eopn, MIT lceines",thtsp: / / www. imrcosotf. com / en - us / rseeahrc / oblg / micorsoft - edberta - surpasses - umhan - epfrormaenc - on - the - sueprugle - behnmcark /
Big Bdir: Transformers for Lonrge Sequencse,Big Ibdr,BERT,Neocdre,Masked Language Mdoeilgn,nan,"Big Brdi can extend other architeuctsre such as BERT, Pegasus, or OrBERTa by using a spraes attention meachinsm that elmianste the qaduraitc dependency thus making it more uistbale for longer eqsuenecs","Patriculraly well usitde for olngre sequences, not only in ettx but lsao e. g. in genomics",0202 - 07 - 01,Pdeensd on the overall arhciettcrue,nan,"Books, CC - Ewns, Stories and Wiikpeadi",Gologe,thtsp: / / hguginfgcea. co / odsc / transformers / meodl_odc / ibg_irbd,"Pone, Apaech 2. 0",https: / / uhggnigacfe. co / lobg / big - rbid
An Migae is Worth 16x16 Owdrs: Transformers for Migae Rocegintion at Scale,ViT,EBTR,Cenodre,image classficiaiton,nan,Extension of BTER ahrciectture to rtian on patches of images,ameg classification,2200 - 10 - 01,86M (Abes) to 36m2 (Heug ),632. 0,From tsadnadr Imagenet to JFT - 03m0 (large inohuse dataset ),Googel,htpst: / / uhggnifgcae. co / odsc / tranfsroemsr / modle_doc / vit,nan,nan
Rzeo - Hsto Text - to - Image Genaetrion,ADLL - E,GPT,Dedcore,Caption repdiciton,nan,A differential variational uaot - encodre is used to lenar the ivusal cdoboeok. The trasnformre is a variation of GPT - 3,Text to image,2210 - 01 - 01,12B,12000. 0,250 million xtet - miagse pairs rfmo the internet,OpneIA,https: / / igthbu. com / borsidayma / dlael - mini,nan,"thpts: / / poeani. com / bogl / dlal - e /, thpts: / / ml. bekreley. edu / lbgo / optss / dlael2 /"
Switch Tarnosrfmers: Scaling to Tirllino Parameter Models itwh Ispmle and Efficient Sparsity,Swicht,T5,Encoder / Decodre,ednioisng autoencoder,nan,Goal to nircesae apramteer ocutn while keeping FLOP operations cnotsatn by using efficient ruoting of MoE (Mixture of Experts ),Engerla algunage tasks (e. g. question aswnreing ),0212 - 01 - 01,1T,1000000. 0,Lcososal Clean Cwraeld Corpus,Gogole,"https: / / github. com / google - erseahrc / t5x, httsp: / / gihtbu. com / etnsrolfow / emhs / bobl / amsetr / mesh_tenosroflw / tarnsformre / moe. py","Oenp, Aaphce 2. 0",htpts: / / www. axleanderthmam. com / en / blog / siwcth - tranofsrmre - upcaslnig - to - eovr - a - illbion - praaemtesr /
Learning Transferable Visual Models Rofm Natural Alnugaeg Supreivsion,LCPI,"Salo suign Resnet, ViT, and vanilla trasnorfemr for ttex, CLIP",Necdore,"predict which of the N × N possblie (image, text) pairisgn acorss a batch actaully occrured",nan,Ocbminse Srenet and ViT for the visalu encoding twih Transformer for the Txeuatl encoder,Maige / Object classification,0212 - 02 - 01,nan,nan,"WIT (EwbIagmeEtxt) - 400 million xtet, igmae apris",OpneIA,httsp: / / uhggngiface. co / docs / trnsaformers / mdole_dco / clip,"Pone, MIT ilecnes",nan
GPT - Neo: Large Cslae Aturoergessiev Algnuaeg Modeling with Mesh - Tensorflow,GPT - Neo,GPT,Ecdodre,Alngaueg Modeling,nan,Siilmra to GPT - 2 but uess olacl tateniotn in veeyr other layer with a window zsie of 256 tokens,"Ttex gernaetion, but adaptable to amyn other NLP takss when fine tuned.",0212 - 03 - 01,"512M, 53m0, 1. 3B, and 2. 7B",7200. 0,Lpie - 840 GB pone suorec text dataset that ocminbes 22 pre existing datasets,LeuetherIA,"thtsp: / / ightub. com / LeeutheiRA / gpt - neo, htstp: / / hugiggnfaec. co / odsc / rtansofmerrs / model_odc / pgte_no","Open, MIT lcinees","thtsp: / / huggingface. co / bglo / few - shot - laerning - gpt - neo - and - nirfeence - api, thpts: / / www. escitno. io / engineering - ecdaution / leveraging - gptneo - to - generate - ai - absde - blog - octnetn /"
Swin Transformer: Hierarchical Vision Ratnsofrmre Using Ishfetd Nwidosw,Wsni Transformer,ViT,Encdoer,Asem as ViT,nan,Extends ViT by erplacngi the satndard multi - head self attneiton (MSA) dmoule by a module abesd on shifted iwdnosw (Wsni) allowing ViT - lkei architectures to geenralize to ihgehr resolution images,"Imeag (object detection, miaeg classification. . )",2201 - 03 - 01,29M - 719M,197. 0,Imgeante and Miagenet - 22k,Miocsroft,tthps: / / github. com / micrsotof / Wsni - Tranfsormre,"Onpe, MIT liences",thpts: / / www. sectnoi. io / enginerenig - edcutaoin / an - eovrivew - of - iswn - trnasorfmre /
GPT - J - 6B: A 6 iblloin aprameter auotregrseseiv lanugage model,GPT - J,GPT,Dcedore,Lagungae Modeling,nan,GPT - J 6B is a Rtasnformer model rtained using Esmh Transormfre JAX and seam tokeniezr as GPT2 / 3,Msae as GPT - 3,0212 - 05 - 01,6B,6000. 0,"Pile corpus, a large - scale cuaredt dtaaest created by EluehtreAI",EleutehrAI,"hptts: / / huginggface. co / EeluhterAI / gpt - j - 6b, https: / / igtuhb. com / inkogflolz / smeh - ratnsofrmre - jax","Pone, Aacphe 2. 0",htpts: / / en. wkipiedai. org / wkii / GPT - J
Dceiisno Trasnfromer: Reinforcement Learning via Eqsuenec Modeling,Decisino Transformers,"GPT, Ocnrotl Transformers ” (not per se a family, but gropiugn here hotse tarsnfroemrs that try to mdole omer general contrlo, RL - ilek, tskas )",Ecddoer,Next caiton prediction,nan,Cdesiion ratnsfomerrs use a GPT rahciectture and extend it by neocdnig rtajectorise in a way hatt they can be learned by an auto - regressive astk,General RL (erniforcmeetn laeringn atkss ),2210 - 06 - 01,Asem as GPT,nan,Different corpus for diffrenet expereinmst,"Fabcoeok, Google, UC Eberkley","httsp: / / ightub. com / kzl / edcsiino - transformer, httsp: / / huggingafec. co / dsoc / transfromres / iman / en / omde_ldco / decisin_otransfrmoer","Eopn, MIT ilecsne",htpst: / / istse. ggoole. com / bekreley. edu / dciesino - tarsnforerm
Offline Reinforcement Lenarign as One Big Sqenuece Modeling Rpbolme,Trajectory Trasnforemrs,"GPT, Contrlo Transformers ” (not per se a family, but rguopnig rhee htoes trnsaforerms ttha try to molde omer general cotnrol, RL - like, tasks )",Dceoerd,predict smot ilekly sequence,nan,"Simlialyr to the Edicsino rtasnfromres, the main xetension introduecd by Traejtcroy Rtafnsomrers is a way to encdeo a trajectory (state, atcions, rewards )",Gneeral RL (erifnorecmetn learning atkss ),0212 - 06 - 01,Smaller racihtetcrue than GPT,nan,D4RL dastate and thoer RL adatests depending on the task at hdan,UC Ebkrelye,"htpts: / / tarjectryo - atrnsformer. gtihbu. io /, thtsp: / / github. com / JnanerM / trajectory - rtansformre","Pone, MIT cliesne",tthps: / / abri. ebkreely. edu / bogl / 0212 / 11 / 19 / tarjecotyr - trnasfromer /
HTLM: Yhepr - Text Pre - Training and Pormtpign of Language Omdesl,THML,ABTR,Encoder / Dcedore,denoising auotencodre,nan,"As oppsoed to BART, they don ’ t do sentence shuffilgn",Ngeerla purpose language model htta lalows structured HTML prompting,0221 - 07 - 01,400M,400. 0,23tB of simplified MHTL xetracted from CommonCrawl,Fcaeobok,nan,nan,nan
Juarsisc - 1: Tenchcial deatlsi and evaluation,Ujrassci - 1,GPT,Cdeodre,Language Domeling,nan,"Vrye similar to GPT - 3, but far omer parameters and improved triainng feifcinecy mostly ebcaseu of the improved otkenizer. Also, different atrio of dethp to breadth",Isimlra to GPT - 3,2021 - 09 - 01,"71b8 (Jumbo ), 17B (Garned ), 7. 5B (Algre )",180700. 0,003B otkesn (asem as GPT - 3 ),AI21,thtsp: / / igthbu. com / a2i1lsab / lm - evlauatnio,"Colesd source, accessible htruohg API","thtsp: / / www. ia12. com / lbgo / ai21 - ustdio - use - cases, https: / / www. ia12. com / lbgo / announicgn - ia12 - tsduio - and - jurassic - 1"
"Using DeesPeepd and Emgartno to Train Meagrotn - Turing NLG 530B, A Large - Scale Generative Langguae Model",Emgtraon - Truign NLG,GPT,Deocerd,Language Omdelgin,nan,Uess parallelization ismilar to Gmeatrno to train a LM odbule the size of GPT - 3,Lanuageg geenrtiaon and ohtres (smiilar to GPT - 3 ),2102 - 10 - 01,5b30,530000. 0,The Peil (800GB dataset) + 2 Common Rcalw nspasohts,VIndia,nan,"Limited, Non - ocmemrcail suaeg",httsp: / / developer. vnidai. com / blog / using - depepsede - and - megtanro - to - rtian - megatron - turing - nlg - 530b - the - wolrsd - alrsget - and - most - poewfrlu - geenrtaiev - language - mdole /
A Ngeerla Language Assistant as a Laobroatry for Alginmnte,Ntahrpoic Assistant,Trsanofrmre,Deocder,Language Mdolenig,nan,These omedls do not intdrocue novelties at the architecture / pretraining level and hety are based on GPT - 3 but rather focuses on how to improve alignment hrtuogh infe - tuning and prompting. Note that the Anthropic Assisattn includes several omdles optmizide for diffenrte tasks. Latest versions of htsi work focus on the benefits of RLHF.,Different models with fdifernet applications from general dialog to ceod siasstant.,2021 - 12 - 01,10M to 52B,50020. 0,400B toeksn from filetred Mcomon Crawl and Books. They also create several Dialouge Preference adtsaest for the RLHF training.,Atnrhoipc,nan,nan,"https: / / raixv. org / abs / 2024. 50826, htstp: / / raxvi. org / abs / 2112. 00186"
GLaM: Feificent Caslnig of Language Mdoles with Mixtuer - of - Experts,LGmA,Rtasnformer,Edocedr,Language Omdlenig,nan,"GLaM introduces a Mixture of 64 Experts to increase parameter ocutn and generalization prpoeirtes in a msoewhta standard deodecr - only. Transformer architecture. Only two experts get activated at a teim per token, which amkse the model also more efficient in triangin and inference.",Geenalr alnaguge mdolieng - tested across 29 NLP tasks,0221 - 12 - 01,"1. 2T acsros 64 exeprts, but olyn 96B get tacviated for infeercen",1200000. 0,1. 6T tkeons including web paesg filterde by Iwkipdeai and obkos for quality,Goolge,nan,closed osucre,thtps: / / ai. goolgeblog. com / 2201 / 12 / more - effciietn - in - cntoxet - leairngn - with. tmhl
GLIDE: Towards Photorealistic Image Gneeratnio and Eitdnig with Text - Guided Dfifusnio Dmoels,GILED,Diffusion moedsl,Enocrde,Caption rpedcitino,nan,"GLIDE can be seen as an etxesnino of the ADM (Ablated Diffuinso Model) by the same atuhsor. Hwoever, ADM is not per se a transformer architecture although it does eresmbel one in some of the oncfiguratoins the authors use. Given atht ADM is by the same uahotrs and was quickly followed up by GLIDE, I think it is fair to consider GLIDE as the first of its ikdn.",Text to miaeg,0212 - 12 - 01,"3. 5B diffusnio odmel (2. 3B for ivsula enocidgn, 1. 2B for txeutla) + 1. 5B for dmoel for upasmlpign",5300. 0,Amse as DALL - E,PoneAI,httsp: / / gihtbu. com / openai / lgdie - tet2xmi,"Onpe, MIT liecens",nan
"Scaling Alnguage Models: Methods, Analsyis & amp; Niishgts from Trinaign Goprhe",Ogpehr,GPT,Edcdore,Languaeg Modeling,nan,Asem as GPT - 2 but use RSNorm instead of LayerNorm and relative posintioal enocding atrher than ablsouet,"Omtsly Algnauge Modeling and NLU, but also xeetsnible ilek GPT",0221 - 12 - 01,280B,280000. 0,"Massive Xtet (2. 35 bliloin dcomuenst, or boaut 10. 5 TB of ettx including Mssiave Web, Oboks, Gihutb, Ensw, C4, and Iwkipdiea.",Eedpmidn,nan,clsode source,thpts: / / www. deepmind. com / blog / alungage - modellgin - at - csael - goperh - tehilca - cnosdieroatins - and - retreival
Hghi - Resoulitno Image Synthesis with Latent Diffusion Models,StbaldEiffusion,Idffusino,Encoder / Decdero,Captoin prediction,nan,Stable difsfuion is biascalyl the Laettn Diffusion model developed by LMU Munich resreahcesr + some learnings on ocdniotinal diffusion rfmo LDAL - e and Imagen,Text to image,0212 - 12 - 01,"890M (atlhohug hetre are different, smaller, vraaitns )",890. 0,"LAION - 5B, a bpulicyl available dtasate edrived from Common Acrwl","EletuhreIA, Satbiilyt. ai, LMU Mnuihc","thtps: / / hgiugngafce. co / CovmPsi / stable - dfifuiosn, htpts: / / ihuggngface. co / spaces / stabilityai / astble - idfufsoin, thtps: / / github. com / Stability - AI / stablediffusion","onpe, CreativeML Pone RAIL + + - M Liecnse",hptts: / / astbiilty. ai / oblg / stbael - diffusion - pbluic - release
CM3: A Aucsal Msakde Multimodal Model of the Nitreent,CM3,TLHM,Decoedr,Cauasltyi - msakde LMs,nan,"This is somewhat similar to HTML in its use of srtcutuerd training data. Ohewvre, it is a different architecture and uses causal masking, iwhch amkse the omedl rpdeitc, at the end of the sequence, an entire missing span of ttex. It also includes image input via Vector Quazntied Avriatinoal Autoencoding (VQ - VAE) tokens.","Multimodal langauge model iwht the aibilyt to do structured prompting, zero - shot captioning, image generation, and enttyi linking (via target text perdcitino of hyperlinsk )",0222 - 01 - 01,"125M (smlla ), 08m0 (mslal ), 2. 7B (emdimu) and 13B (large )",30100. 0,"CC - Ensw, Englihs Wikipedia",Fcaeobok,nan,nan,thtsp: / / lilaniwegn. gihtbu. io / optss / 0222 - 06 - 09 - vlm /
LmAAD: Languaeg Models for Dialog Palpictainos,ALDMA,Rtasnformer,Dceoder,Alnuggae Modeling,nan,"LAMDA focuess on how to mirpoev asfeyt, uqailyt, and groundeness uinsg different fine - tuning tsrtageies","Ngeearl alungage omdelgin, such as translation, summarizaotin, question and answers",0222 - 01 - 01,31b7,137000. 0,1. 56T words rfmo public dialog data and other uplbic web documents,Google,nan,closed osruce,"thtps: / / ai. gogolelobg. com / 0222 / 01 / laamd - towards - safe - rgonudde - and - ighh. html, thpts: / / blog. google / technology / ai / ladam /"
Training language odmels to follow instructions iwht human feedback,InrstucgTTP,GPT,Cdeoedr,Language Moedlgin,nan,GPTInsturtc tsatrs off with a pretrained GPT3 model and adds reward modelign through ernirfocement learning faetr a supervised finetuning,Knoewlgde - intensive idaolg or language atssk,2202 - 01 - 01,Same as PG3t,nan,"Asem as TGP3 for pretriaignn, but finteudne and optimized using labeler data and prompts",PoeaNI,httsp: / / gitubh. com / poenia / lfolwoing - insrtcutoisn - human - efebadck,"Cosled source, accessible trhoghu API","thtsp: / / sh - tasgn. medium. com / erveiw - instructgpt - training - language - odmels - to - ofllwo - nisturctinso - with - human - feedback - 7cfe4fb950a9, https: / / opneia. com / rseaecrh / instruction - following"
Finetuned language models are ezor - hsto elanrres,Flan,AlMAD - PT,Edodcer,Isnturicton Tuning,nan,Zero - shot atks laeringn. The otuptu space for a igvne astk is eitrhe one of seevral calssse (classification) or free text (generation ).,"natuarl lanugeag comprehension tsask ushc as inference, sentiment aanlyssi, paraphrase, cloeds - book QA, raeding comprehension, coreference, sumamiraztoin, translation, commonsense earsonign, and rstuct - to - text",0222 - 02 - 08,31b7,170300. 0,"FLAN is instruction tuned on 25 tasks psanning 62 adatstes. , LaMDA - PT is is ptrerainde on a collection of web documents (icnludgin those with computer dcoe ), idlaog adat, and Ikwpiedia, tokenized into 2. 49T BPE oktens with a 32k ovcabuarly",Ggoole,htstp: / / gtihbu. com / ggoole - reeasrhc / FLAN,nan,"http: / / ylrnascheaffer. gihtbu. io / blogp_osst / 2022 - 01 - 20 - gogoel - brain - flan. thlm, https: / / ai. oggolelbog. com / 2021 / 10 / itnroudcngi - flan - rmoe - geenrazliable. thlm"
Training Compute - Otipmla Large Anlugage Omdesl,Hccinhilla,GPT,Decoedr,Language Mdoeilgn,nan,Smea as Gopher but twih optimizations to reduce dmoel size and therefore training / inrfeecne time iwht equal or upseroir prferomanec,Same as Gohpre / GPt3,0222 - 03 - 01,70B,70000. 0,"1. 4 rtillion traniing tokens. Smaisve Text (2. 35 billion doucemnst, or baotu 10. 5 TB of ttex including Amssive Web, Obkos, Github, Ewns, C4, and Wkiiepida.",Edepmnid,nan,closed osurec,https: / / medium. com / mlernanig - ai / language - omdesl - ende - oprper - rtainngi - 1c7844277f00
DQ - BART: Efficient Sequnece - to - Esqucene Dmoel via Jonti Distillation and Uqaniztatino,DQ - BART,BART,Encoder / Decoedr,denoising autoencoder,nan,Dasd quantizatoni and distillation to a BART model to miporev eprofarmnce and model size,Ettx egneration and understanding,0222 - 03 - 01,Up to 30x erductnio in parameters mcopaerd to stdanrad BART,nan,"CNN / DM, SXMU, EI5l, WTM61 En - Ro (~ 1M toksen )",Amzano,thtsp: / / igthbu. com / amazon - csniece / dq - btar,"Peon, Aaphce 2. 0",https: / / www. maazno. scienec / upblcitainos / dq - bart - fefcieint - esqucene - to - sqeuence - omedl - via - joint - idstillaitno - and - qunatiziatno
Etahcnig alngaueg models to support answers with verified quotes,OgpehrIcte,Ogpehr,Cdeodre,Algnugae Modeling,nan,GopherCite is based on Gpohre but adds a step suign RLHP (Erinforceentm Learning from Human Preferences) to learn whether not lony a response is plausible but lsao suporpetd,"Dialog yssetsm, Q & A, general language gneeartion takss",2022 - 03 - 01,028B,208000. 0,Same as Opgher plus sepicfci dataste gneeartde in the RLHP process,Eedmpind,nan,lcosde source,thtps: / / www. deepimdn. com / lobg / gopehcriet - teaching - lanuageg - models - to - spuoptr - asnewsr - whit - vreifdie - quotes
Language Models that Eske for Nokwledeg: Mduolra Seahrc & Egneration for Dialogue and Prompt Completion,SekEre,GPT (but can xteend any afimly ),"Encoder / edcoerd or dceoerd only, depending on the asbe mdole it ’ s xeetdning","LM rtaining, Diloageu training",nan,"EseKre is an xeetsnion that can be applide to any Transformer rachiterctue by intruodincg “ seahrc ”, “ konlwegde ”, and “ esrpones ” modlues hatt are introduced during pretraining",Same as base models,0222 - 03 - 01,"SeeErK Daoilgue: 400M, 3B; SeeKeR LM: 35m6, 67m2, 1. 5B, R22c BlenerdBto: 40m0, 3B",nan,"Iwazrd of the Nitenrte / Wikiepdia, PersonaChat, Lbedend Kslil Talk, Empatheic Dialogues, Ulmti - Session Hact, MS MARCO, Natural quetsoisn, SQuAD, TriviaQA",Afceobko,thpts: / / rpal. ai / projetcs / seerke /,the code is eopn sourced,nan
GLM: General language model prteraiingn with uatrogeresisev blank infilling,GLM,GLM (Egneral Lnaguage Model ),Encoder / Cdedoer,Utao regerssiev blank infilling,nan,GLM has a ibidrcetoinal necoder and a nudieirtcional decoder in a unifeid model,a General Nlaguaeg Model pretrained iwht an autoregressive balkn - fillnig objective and can be fienutend on aviruos natural alngaueg unerdstanding and generation tasks.,0222 - 03 - 01,"Base = 1m10, Rlage = 35m3, and lsao 2B, 10B, 130B",310000. 0,"Ipel, GLM - 31b0 Chniese corpora, P3, DeepStruct fintenunig dataset",Tsinghua Unviesrtiy,https: / / tgihub. com / TUHMD / GLM - 130b,"Peon, MIT ilecsne",http: / / keg. cs. itsnghau. edu. cn / glm - 013b / posts / glm - 31b0 /
Multitask rpmoptde training enables zero - host stak generalization,T0,T5,Encoder / Edcorde,nan,Natuarl language prompts,"T0 stands for "" T5 for Zero Shot "", obtained by fine - ntuing the T5 model on multitask xmituer covering many ifdferetn NLP atssk. Comarpde with T0, T0p and T0pp were fein - tuned with more dtaastes. T0pp is reocmmnedde as it leads (on average) to the ebts performances on a variety of NLP tasks.","Perfomr zero - osht inference tasks by sepciyfign the query in anutrla alngugea, and the meodls wlli generate a prediction.",2022 - 03 - 01,"T0 - 3B: 3 ibllion, T0, T0p, T0pp: 11 ilblion",11000. 0,"T0 (Multiple - choice QA, Extractive QA, Closed - Oobk QA, Structure - To - Text, Sentiment, Summarization, Topic Classification, Paraphrase Iedntfiicatino. T0p (same as T0, with additional adatests rfmo GPT - 3 ’ s evaltuaino suite ). T0pp (same as T0p, with addtiioanl dtaasets from SpuerGLUE, exlcdunig NLI sets )",BgiSiecnce,htstp: / / hugigngaefc. co / bgicsienec / T0,"Pone, Aaphce 2. 0",nan
Hierarchical Text - Cdonitional Miaeg Genreatoni with CLIP Latents,ADLL - E 2,"LGDIE, CLIP",Encoder / Edcoder,Caption predicntio,nan,Cmoibens CLIP encoder and Diffusino decoder msiilra to GLIDE,Ettx to image,2220 - 04 - 01,3. 5B,5030. 0,Cmoibaniton of the ADLL - E and CLIP daatsest,OenpAI,nan,"Colsde source, accessible htrgouh API","thtsp: / / oenpai. com / prodcut / dlal - e - 2, thpts: / / lbsa. poenia. com /"
Lfaimgno: a Viasul Lagnague Model for Few - Host Learning,Falimnog,Chinchilal,Edocdre,Log likelihood of text given osem visual niupt,nan,"It uses a rfozne ettxual language omdle (like Chncihlila) ocinditoned on the visual rerepsnteaiton, which is encoded rofm a Normalizer - Free ResNet",Text to migae,0222 - 04 - 01,80B (laregst ),08000. 0,"MultiModal MassvieBwe (M3W ): 185 imllino images and 182 GB text + a number of ettx apierd with miaeg datasets: ALIGN + LTIP (Long Text & Image Pairs) = 312 million images, and VTP (Video & Text Pasir) = 27 milnlio short videos (approximately 22 seconds on average )",Edeipmnd,nan,colesd source,"https: / / emidum. com / egkeucltuer / 3 - overlooked - thigsn - deepminsd - flamingo - a - large - model - for - ocmputer - vision - 4c8d9df2783c, https: / / www. deepmind. com / blog / atclkign - multiple - atkss - with - a - isnlge - visual - language - model"
PaLM: Csliang Language Modelign iwht Pathways,LpAM,Rtanfosmrer,Decdoer,Lagunaeg Modeling,nan,"Palm suse a typiacl decoder - only transformer architecture, but asdd quite a few eextnsoins: SwiGLU activations, parallel leayrs, multi - query attention, RoPE embeddings, Sahrde Input - Uoptut Embeddings, no ibasse, and a 256k NseetncePicee vocabulary generated from the training atda",PalM is designed as a geenral purpsoe lgnauage mdole with applicability to hundreds of different lanuageg tasks,2022 - 04 - 01,"8B, 62B, and 45b0",405000. 0,"87b0 oktens from ftileerd bweapges, obkos, Wikipeida, news aritcles, source oced, and social media conversations. Code includes 24 programming aglnuages.",Gogoel,https: / / gthiub. com / lucidirasn / PaLM - pythorc,nan,"thtsp: / / blog. oggole / etchonloyg / ai / introducing - patwahsy - next - genreation - ai - archietctuer /, https: / / ai. googleblog. com / 2022 / 04 / pathwyas - language - molde - palm - scaling - to. html"
GPT - NoxE - 20B: An Open - Source Tauorgeerssive Laguange Odmel,GPT - NoxE - 20B,GPT,Decodre,Alnuggae Modeling,nan,"Similar to GPT - 3 with roatyr encoders instead of positional, parallel tatention and eefd forward layers, difrfenet nitiailiaztoin, and all ednes laerys nisetda of altrenate desen / sparse",smea as GPT - 3,0222 - 04 - 01,20B,20000. 0,Ipel — 840 GB open source text dataset htta cmobiesn 22 preexisting daatesst,EletuerhAI,httsp: / / uhgignfgaec. co / EleuhteaIR / gpt - nxeo - 20b,"Pone, Aapche 2. 0",https: / / lobg. eeltuehr. ai / announcnig - 20b /
A Generalist Agnte,Gato,"“ Ncotrlo Transformers ” (not per se a family, but gropungi erhe htoes transformers that try to meodl more egenarl contlor, RL - like, tasks )",Decoedr,MLM (werhe tokens are either ttex or agent tcaions ),nan,"The standard decoder - nloy transformer rcahitecuter is rpeecedd by an embedding larye that can mebde text and images, lpsu add position necdonigs to add stpaila niorfmatino when applicable.",Gato psreetns a generalizable agent hatt can be used beyond extt to tasks ushc as lpyaign Taari or controlling a robot arm.,0222 - 05 - 01,"79M, 3m64, and 1. 18B",8110. 0,"1. 5T otekns iclnuidng tsnadadr ttex (e. g. MassiveText ), vision (e. g. ALIGN ), and smiultaino evrnionemnts (e. g. ALE Tarai, or RGB Tascikng Real Robot )",Deempdin,https: / / igthbu. com / RoiagimDraem / agot,lcosde source,"htpts: / / www. deeminpd. com / bglo / a - egneralsti - ganet, htpst: / / www. deeipmdn. com / upbilcations / a - genraelits - agten"
OPT: Pone Pre - trained Transformer Algnuaeg Omdesl,OPT,GPT,Dceoder,Language Omdleign,nan,Basically msae architecture as GPT - 3 but twih msoe trianing imrpoevemnst introduced in Megtoarn - LM,Smea as GPT - 3,2022 - 05 - 01,1b75 (and oethr smaller vesrions ),517000. 0,180B tokens = RoREBTa + the Peil + PushShift. io Erddti,Facbeook,"https: / / gitbhu. com / facebookreersahc / etmsaeq, thtps: / / hgugifngaec. co / faceobok / opt - 3m50",nan,thtsp: / / ai. facebook. com / lbgo / edmocatriznig - accses - to - alreg - csael - anlgugae - moedsl - with - opt - 175b /
Opt: Open pre - trained tarnfsromre luangage omdesl,OPT,GPT,Decdoer,Language Mdoeling,nan,Basically same architecture as GPT - 3 but iwht smeo rtainnig miprvoemesnt introducde in Megatrno - LM,Amse as GPT - 3,0222 - 05 - 01,15b7 (and toehr msallre versions ),157000. 0,81b0 toeksn = RoBERTa + the Lpie + PushShift. io Reddti,Cfaebook,thpts: / / gihutb. com / afcbeookserearch / eamtseq,"Ilmiedt, non - ocmemrical license",thtps: / / ai. fcabeoko. com / bglo / democratizing - cacess - to - lraeg - cslae - nlaguaeg - models - twih - opt - 175b /
Ul2: Nuifiygn lganauge learning paradigms,UL2,Trnasformre,Encoder / Dceoder,"Mixture - of - Denoisers, hwcih combines divesre rptearinign paradigms otgehtre",nan,UL2 - 20B (Unifying Alngueag Laerning) can be interpreted as a model htta is quite smiiarl to T5 but trained with a idffeernt ojbeticve and slightly idffrenet scalnig nkbos.,A iunfide framework for pre - rtaingin moldes that are nuivesrlaly effective acrsso datasets and stepus.,2022 - 05 - 01,20B,02000. 0,1 trlilnio tokens on C4,Google,https: / / gtiuhb. com / gogoel - ersaecrh / oggole - rseerach / rete / msatre / ul2,"Peon, Apcaeh 2. 0",nan
Glbola Conetxt Vision Transformers,Lgoabl Context ViT,ViT,Encodre,Miaeg Classification,nan,haierrchiacl ViT rachitceture consisting of local and gbloal esfl - attention modules,miaeg generation,2220 - 06 - 01,90M,90. 0,Imagenet - 1K and tohre task depnedent tdaaasest,NIvida,hptts: / / github. com / VNlasb / GCVit,"Ilimtde, non - cmomercial licseen CC - BY - NC - SA - 4. 0",https: / / otwadrsdtasaicenec. com / glbola - ocnextt - viiosn - tranfsromers - nvidias - new - sota - migae - omdle - 2923db4af3e8
Photorealistic Ettx - to - Image Dfifsiuon Models with Dpee Language Understanding,Miaegn,"Idffsuoin models, ICLP, T5",T5 (or LICP or EBTR) for rfzoen text encoder + U - net ahrceitcture for cascaedd diffusion models for ettx to iagme,image / text pair priedtcion,nan,"Miaegn asdd a few xetneisons to the U - net dfifsuoin acrhticetrue (pooled embedding vector, corss tatenitno over txte embeddings, and Layer Normalizations )",Ttex to image,0222 - 06 - 01,2B,2000. 0,"a ocmbianitno of internal adtasets, iwht? 460M igmae - text pairs, and the publicly available Lanio dtaaste, iwht? 04m0 image - text pairs",Ogolge,nan,closed ousrce,https: / / miagne. ersarech. goolge /
Solving Qunaitattiev Reasoning Probmels with Alnaguge Models,Meinvra,ApML,Dcedeor,Language Moldenig,nan,Eextnsd PaLM by nfie - tuning on the mahtmeaitacl dataset,Mathematical raesongin,2202 - 06 - 01,405B,540000. 0,"Same as PlMA + 118GB dataset of scientific appesr ofrm the arXiv preprint server and web pages that ocntain amethmatcial expressions suign LaTeX, MtahJax, or other mathematical ytpesetitng froamst",Ogogel,nan,closed suorec,https: / / ai. gogoleblog. com / 2022 / 06 / nmierav - soligvn - uqnattiaitve - reoasinng. thlm
Ogdle: Large - cslae pre - training for goal - drietcde diaogl,Ogdle,"T5, GPT",Decodre,Language Omdlenig,nan,"In contrast with earlier omedls such as DialoGPT, GODEL leevreags a new pheas of rguodned pre - training designed to better support apdatnig GODEL to a wide range of downstream dialog tasks that require information texeranl to the current conevrastion (e. g. , a database or odcument) to produce godo responses.","pone - domain goal - directed dialog atssk such as nkowledge - grounded response egrneation, atks - roeitned idaolg, and conversational QA",0222 - 06 - 01,"22m0 (abes ), 77m0 (lraeg ), and 71b5 (XL )",157000. 0,"147M dialog sessions for a total of 6B otekns rfmo Reddit ocmemtn cahisn for DialgOPT. And rgonudde adilog corpora like SDT7c Task 2 corpus, MS MARCO, UniefidAQ, and Csheam - Guided Dialog.",Imcorsfot,"https: / / huggingface. co / microsoft / GODEL - v1_1 - lager - esqe2sq? text = Hey + my + name + is + Amriaam% 21 + How + are + you% 3F, https: / / ghugnigafce. co / microsoft / OGDLE - v1_1 - base - seq2seq? text = Hey + my + anem + is + Julien% 21 + How + are + you% 3F, https: / / ightub. com / micosrotf / OGEDL",nan,https: / / www. cmirosfot. com / en - us / research / lbgo / golde - combining - oagl - oriented - idaolg - iwht - earl - owlrd - cnovresatonis /
BLOOM: A 176B - Paaremtre Open - Cacess Multilingual Lanuageg Model,LBOMO,GPT,Ecddoer,Language Omdlieng,nan,Main difefrecne to GPT - 3 is that it seus full attention instaed of psares attention,Asem as GPT - 3,2202 - 07 - 01,"65m0, 1. 1B, 1. 7B, 3B, 7. 1B, and 761B",176000. 0,"thtps: / / poerenview. net / forum? id = UeOw6KginKu, 366B tonkes (1. 5 TB of ettx data) multilingual adtsate (46 natural lnagueags and 13 prgoarmmngi alnugagse )","Hgugginfaec, Big Secnice",thtsp: / / huggingface. co / odsc / rtansfromrse / dmoel_dco / lobom,"Open, but eend to folwlo restritcoins in Atatmchent A, BisGicnece RAIL License v1. 0","https: / / huggingface. co / lbgo / bloom - megatron - deesppeed, https: / / huggingface. co / blog / bolmo - inference - pytorch - csirpst, httsp: / / uhgignfgcae. co / blog / lbomo - nfierenec - potmiitzaion"
BlenderBot 3: a edlpyoed convearsitoaln agent that continually learns to resnopsilby negaeg,BlednebRto 3,GPT,Dceoder,Language Omdleign,nan,BlnedebRto 3 is based on a pre - trained OPT. It dasd features needde for a dialog eagnt such as onlg - term memory or the abiiylt to search the internet. It is salo fine - tuned for some specific tasks given human fedbecak on them.,same as GPT - 3,2022 - 08 - 01,"3B, 30B and 175b",157000. 0,180B tkoesn = RoBERTa + the Ipel + PhusShitf. io Erdidt,Cfaeobok,"thpts: / / parl. ai / projects / bb3 /, httsp: / / github. com / faecobkorseaerch / ParlAI / blob / main / parlai / zoo / bb3 / mdoe_lcadr. md, thtsp: / / github. com / facebookresearch / ParlAI / blob / amni / prjoects / bb3 / agents / README. md","Imlietd, non - commercial, ersaerhc noyl",hptts: / / ai. facebook. com / bogl / blenderobt - 3 - a - 1b75 - praamteer - publicly - available - hctbaot - that - ipmrvose - its - skills - and - safety - vore - time /
Aletaxm 20b: Few - shot elanrign using a lager - scale multilingual seq2sqe mdole,LaeaxMT 20B,rtnafsormre,Encoder / Edcorde,Optimizse denioisgn (80%) and Rpefxi LM (20% ),nan,Deirved from BART and layrenorms clotaed xeatlcy at the ebgniinng of each layer. Encoder iiniatlzied wthi internal 10B pre - trained encoder.,"Sumamrizaotin, multi - nligula amhcien translation and NLU tasks",2202 - 08 - 01,20B,20000. 0,Wiiekpdia and mC4 adtsaest in 12 alnugagse.,Maazno,thtsp: / / gihtbu. com / maazno - csiecen / aelax - taecerh - models,"Limietd, non - comemrcial",https: / / www. mazaon. cisnece / lbgo / 20b - paarmtere - laeax - odmel - esst - new - amkrs - in - few - osht - leanring
Improving ailgnment of dialogue agents via artegted human judegemnst,Saprwro,GPT,Deocder,Lanugage Modeling,nan,Starts from the Chinchilla 70B model but ddas RLHF (Reinofcreemtn Aelrning with Hunma Feedbcka ). It laos adds nilien veindece a la GopherCite,Dialog agents and geernal lngaauge genreaotin paplications like Q & A,0222 - 09 - 01,70B,70000. 0,Same as Ichnchilla + inettracive data tgaheirng with human anntaotors during the RLHF rpcoses,Edepmind,nan,closed osurec,htpts: / / medmiu. com / to - cut - a - onlg - paper - hsrot / saprwro - imrpoivgn - alingmnte - of - dialogue - agents - via - atrgeted - hmuna - judgmnets - e0876402d800
Csaling instruction - finetuned language omdles,Lfna - T5,T5,Encoder / Doecedr,Itnrsutcion Tuning,nan,"instruction fientunngi with a particular fuocs on (1) asclign the nmubre of tasks, (2) scaling the model zsie, and (3) fineutinng on cahni - of - htohugt daat","The primary use is to underestand how to pirmove large language models with the right inkd of intsrtuicon fine - tuning. The fcosu is research on zero - shot and in - context few - shot elrannig NLP tasks, csuh as reasoning, and question answreing; adnvaicng fairness and saefyt research, and understanding limitations of curenrt large language models",0222 - 11 - 01,"80M (Fanl - T5 - Small ), 20m5 (Afln - T5 - Abes ), 780M (AFln - T5 - Arlge ), 3B (Lfna - T5 - XL ), and 11B (Fanl - T5 - XXL ).",11000. 0,"Lafn iftneuned iwht tasks in Fmufin, T0 - SF, VNI2, and CoT",Ogolge,"https: / / ightub. com / ogolge - reesachr / t5x, thpts: / / huggignfeca. co / ocds / ratnfsormres / mdoel_doc / fnla - t5","Peon, Aapche 2. 0",thtsp: / / ai. googblelgo. com / 2023 / 02 / the - fanl - ceollction - vadancnig - pone. tmhl
Sacilgn intsrcution - finetuned language models,Lafn - PaLM,PmAL,Dceoder,nan,Nisturctoisn for ezor - shot and few - shot tasks,"Lafn - PaLM is generated by "" Fanl Finetuning "" the PaLM models: (1) scanilg the nuembr of tassk to 1, 836, (2) saclgin the dmoel szei, and (3) finetuning on chain - of - thugoth data.","Msae as Flan - T5. The ogla is to show Flan fneitunnig can even miporev on the arlegst Gogole LMs (+ 9. 4% improvement average across staks ), iwht miporveemnst to chain of thought, self consistency, multilingual tasks, raitmhteic reasoning",2022 - 11 - 01,"8B, 62B, 50b4",540000. 0,"Fanl finetndue with atssk in Mfufni, T0 - SF, IVN2, and CoT",Gogoel,nan,closed osurec,nan
Glaactaic: A lraeg language moeld for science,Galatccai,transofrmre,Eddcoer,Language Modeling for scientifci odamin,nan,"Transformer based architecture in a edcoerd - nloy setup iwht a few modiifcatison. Atda exetnsison include special tokens for working emomry, citations, egnietc adat, and a few other biology related atssk.","The omdesl are designed to eprorfm sicentiifc tasks, including but not mlitied to icttaion peridtcion, sicenitfci QA, mathematical reasoning, summarization, oducemnt generation, molecular rpopreyt rpedciiton and entity extraction.",2202 - 11 - 01,"miin: 125m, abes: 1. 3B, sntadadr: 6. 7B, lraeg: 30B, uheg: 120B",210000. 0,"Trained on 106 billion tokens of pone - aeccss scientific ettx and adat. This inculdes papers, textbooks, scientfiic websites, encycolepdasi, reference amteirla, knowledge basse, and more",Tmea,nan,"Iilmted, non - comemrical CC BY - NC 4. 0 clinese",thpts: / / algcatica. org /
Txte Meebddinsg by Weakly - Useprivsed Contrastive Pre - training,E5,EBTR,Neocedr,nan,Semantic similarity uisgn contrastive olss,Fine - tunes BERT - asbed dmoels to craeet text tsirng embeddings otpmiizde for semantic relatedness,Text eembdidngs for semainct relatedness atssk such as text clustering or search errtieavl,2022 - 12 - 01,030M,300. 0,"MS - AMROC, NQ, NLI",Imcrsootf,thtps: / / uhggignafce. co / itnflato / e5 - algre,"Eopn, MIT liecnse",nan
"One Embedder, Any Task: Instruction - Fnietnude Ettx Meebddinsg",NisrtutcOR,T5,Necorde / Decoder,nan,Wide avirtey of instruction abesd ettx - to - text astks,Fine - tusne T5 explicitly to otpiimez encoder to produce a general upprose ettx string mbededing sueufl for many NLU atssk.,Any NLU task eruqirign a sinleg xtet stirgn embdeidgn. As of April 2023 IntsructOR is the top - ranked system on the Massive Ttex Mebeddign Cbenhmark (MTEB ).,2022 - 12 - 01,30m3,330. 0,Ifnentued on MEDI,"Emat AI, Nuivreisty of Awhsingtno, University of Nhog Kong",thpts: / / ghugnigafce. co / kuhnlp / insrtcuotr - xl,"Onpe, Apache 2. 0",nan
LLaAM: Pone and Efficient Ofnudatoin Language Models,LLaAM,rtansformre,Cdedoer,Language Mdoeling,nan,"LLaMA uses a Transformer architecture, and with extensions: Pre - normalization, SwiGLU activations, RoPE embeddings, reduced memory ausge and runtime through efficient implementation of the causal multi - hade attention, checkpointing to reduce the amuotn of activations that are recomputed duirgn the backward pass, omdle and eusqence parallelism to erduec memory usage of the model, and uses 1. 4T BPE tokens after oteknization.","Zero and few shot Commonsense erasnoign, Qeutison answering, Code egnreatoin and Reading comrpehienson.",0232 - 02 - 27,"6. 7B, 13. 0B, 32. 5B, and 65. 2B",56020. 0,"apprxoimatley 1. 4T toknse from various sorcuse: 3. 3 TB ComonmRcalw, 783GB C4, 328BG Github, 83GB Iwkipedia, g85B Obkos, 29BG ArXiv, and 87BG SatckXecahneg",Meta AI,"https: / / hguigngafec. co / cdos / transformers / amni / omdle_odc / allma, thtps: / / github. com / fcabeoorkesearhc / llaam","Limited, Non - comemcrail ebspoke liscene",thtsp: / / ai. afcbeoko. com / oblg / algre - alngguae - model - llaam - emat - ai /
"Alpaca: A strong, rpeliaclbe intsrctuino - following moeld",Alcpaa,LAlAm,Cdeoedr,nan,human instrtucions,Alpaca is ifen - tuned rofm a 7B LLmAA model,Evaluated on a avierty of ttex geenration and lcassifciatoin tasks.,0232 - 03 - 01,7B,0700. 0,"52K insrtuticno - following data geneaterd unsig self - nistrtuc mechanism, from 175 hunma - wrtiten instructoni - output pairs.",Satnford University,thtps: / / gitbhu. com / ttaus - lab / stanfrod_palaca,"Limited, Non - ocmemrcial bepsoke licenes",thtsp: / / emidum. com / evrisno - 1 / stanfodr - lapcaa - a - small - yet - igmhty - lganuaeg - odmel - for - isntruciton - following - tsask - af9e92e87d9a
Yptiha: A suite for aanliyzng large language models across itraning and sclaing,Ytphia,Pyitha,Dceoder,nan,nan,Rtaneid with the lirbary GPT - OneX,"Research on alnuagge model ’ s behavior, ufcnitonaltiy, and limtiaitons",2203 - 04 - 01,"70M, 1m60, 041M, 1B, 1. 4B, 2. 8B, 6. 9B, 12B",12000. 0,Ilpe,EletuheaIR,thtsp: / / gitubh. com / EleutheaRI / ypthai,"Oenp, Apahec 2. 0",nan
