0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Imprng Language Understanding by Gneraie Pre - raing,GPT,GPT,Dcer,Lngue Modeling,nan,nan,"ex gnrtion, but adaptable to ny other NLP tasks when ne ued.",18 - 06 - 01,17,117. 0,"nsupervd Pretraining on Boorpus dataset. Supervised Finetuning on sver tk - specific datasets incing NI, RA, Quora. ..",OpnI,"tps: / / ithu. com / peai / finun - nsfrmer - lm, htp: / / huggingface. co / ds / rasfrmer / mde_oc / enai - gpt",nan,nan
ER: Pre - train of Deep Bidictinl rsformrs for Language Understanding,ET,BR,Ence,"Nt Sentence Prdctin, Masked Language deing",nan,nan,General Language Undestang and Question Anerig. an other angue applications followed,20 - 10 - 01,"Be = 1M, Lar = 340M",340. 0,Toronto Bo opus and ikipda (3. 3B okns ),Gole,https: / / hingace. co / dc / ansforrs / moddoc / br,"pe, Ache 2. 0","ttp: / / colab. eserc. oole. com / github / NesRogg / Transformers - Tutorials / lo / master / BERT / ne_tningBE_ (ndriend) _for_multi_label_text_classification. ipynb, https: / / www. plshmid. de / et - text - classification - in - a - different - aguge"
Trsormr - XL: Ateive nguag Models Beyond a xed - Length Context,ranfore XL,nan,Dcod,Lauag Modeling,nan,Relve positioned embeddings enal logr - context attention when compared to vanilla Trafome ode,General nguae tasks,19 - 01 - 01,15,151. 0,"Different rning daset epedig on experiments, but baseline is Wkitt - 103","Gole, CMU",ttp: / / gngface. co / ds / tansfmes / mdeloc / asfo - xl,nan,nan
anage models are unervied multitask learners,GPT - 2,GPT,Deor,Language Mdeng,nan,"nor extensions to the GPT achtectu (e. g. lar nomaliain moved to the nut of eh sub - ayr, or increased otex si ro 512 to 1024 )","xt gnraton, but aptble to many other NLP tasks we fn tuned.",19 - 02 - 01,"1M, 5M, 7M, 1. 5B",10. 0,8 ilon web pge (40 GB ). 10X GPT. eTex daae is cred by crawling all lnk at Reddit with at les 3 ara ints.,Open,htp: / / huggiac. co / docs / ansfomrs / ode_dc / t2,"Op, Modfd MIT license",htp: / / www. phischi. de / in - ne - a - non - enli - gpt - 2 - ode - th - ugngfce
XLNet: eneraid Autoregressive Pretraining for Lanag Undrsning,LNt,rafomer XL,coer,PLM,nan,This oel basically adapts Tansrmr XL architecture to ermtton - ase LM,Gnrl language tasks,01 - 05 - 01,"se = 11, Lre = 360M",360. 0,"Same as BT + Gi5 (6B te ), and and gressivy filtered CueW 2012 - B (GB ), Cmmn Crw (110 GB )","Gogl, CMU",hts: / / hugngae. co / os / tasfomes / model_doc / xnt,"Oe, MIT lcns",nan
ERNIE: Enhanced Lanag Repeetain with Inomtiv Entities,ERI,RT,Ecdr,Masked Lanag Modeling,nan,"Us ET for Encoder rhitecte, but stacks and aggregates two of em for text and nties. This architecture could be uerstod as BE for xt + knowledge rphs",Knowledge intensive related tasks that might befi from knowledge grah or titis sh as etiy recognition,09 - 05 - 01,"Eie - Vi 2. 0 = 10B, Eie 3. 0 Tin = 2B",2600. 0,English Wikidi + Wikidata for enties (ne th they initialize model to oiina BERT rmeter values,"ngchng Lab, Bau",hts: / / ghub. com / hunl / ENI,closed soue,ht: / / rearh. adu. com / Bo / iex - ve? id = 160
RERa: A Robustly ptized ET Pretraining Approach,oERT,BR,Ecdr,MLM (Dymc ),nan,Extension of BE with optimized training rcedue and more aa,Same as BT,20 - 07 - 01,"125M ae, and 356M Lre",356. 0,am as RT + CC ws + OpenWebText + tris (~ 33B okns ),"Google, Univrty of Wasigtn","tts: / / github. com / facbooesrch / fairseq / re / mn / exple / roberta, tps: / / hggigac. co / cs / transformers / deldoc / rora",nan,ttp: / / ai. facoo. com / bl / rbet - an - timied - thod - for - rtaiing - el - supeisd - nlp - stem /
ALER: A Lite BERT for ef - supervised arnng of Lngue Representations,LBRT,BE,Ecod,"ex Sentence redicti, aked Language Modeling",nan,"Comesed version of BERT usg preter sharing, which is uc more efficient given the se nber of parameters",me as BERT,09 - 09 - 01,"se = 12M, age = 18M, XLae = 60M",60. 0,Same as ER,Gool,hts: / / ggigfac. co / docs / rasforrs / oe_doc / aber,"Op, pace 2. 0",nan
CTRL: A Conditional Transor Language Moe for Controllable Generan,CL,nan,ecor,nan,nan,"mel can enrte text conditioned on onto codes ht pefy domain, style, tops, ats, nttie, relationships between entities, lt pois, and task - related behavior",Cntllabl text generation,21 - 09 - 01,1. 63B,16. 0,"140 GB of te ncldin: Wkipdi (En, De, Es, Fr ), Project Gutenberg, 45 subreddits, OpenWebText2, Azon Reviews, Europarl and UN data from WMT, question - answer pairs from ELI5, and the MRQA shre task3, which inuds the Stanford Question Aswrig Dset, NesA, TrvaA, SearchQA, HotpotQA, and Natural Questions",Saesfoc,"hps: / / gihu. com / asforce / ct, htp: / / ggiface. co / ds / transformers / md_doc / tr","en, BSD - 3 - Case lnse",hts: / / bo. aesoreaieearch. com / ntoducn - a - oiional - tasfmer - angag - del - for - controllable - geneain /
Highly curte protein trctur prediction ih AlphaFold,lphFol,SE (3) - raformr,Ecor,"Prti folding prediction of BERT uin aramer harn, which is much more efficient ive the me number of parameters",nan,"The oigil Alphafold used a BE - style transformer. The dets of hafold ’ s Transformer are not kwn, but it is believed it is an extion of the SE (3) - Traormr, a 3 - D equivariant Transformer (see hs bg ot ).",Pten folding,19 - 09 - 01,"b1, Lae = 18M, Lare = 60M",60. 0,"170, 000 proen fm a pblc repository of protein eqenes and structures",Depmn,tps: / / thub. com / epmin / lphold,"the ce is op srce, it Apache - 2. 0","htp: / / www. depid. com / publications / ighy - acuae - protein - scture - reictin - th - alpold, htp: / / fabiahsml. github. io / alphafold2 /"
"RT: enosin Sequence - to - Seqec Pre - training for Natural Language Genrtin, Trslton, and Comeenion",RT,"BT for enco, GPT for Dedr",Encoder / Decr,denoising autecod,nan,It can be se as a geerlizon of BERT and GPT in ht it combines dea fm both in the endr and ecor,otly xt gnation but also some text understanding tasks,29 - 10 - 01,"Be = 140M, Lar = 400M. In enel, roughly 10% larr than BT for eqivlet architectures",400. 0,"me as RoBR (10G of news, books, tres, and web et )",Faceo,https: / / hingace. co / os / tranfrme / mdel_d / rt,"Op, Aace 2. 0",nan
DialoGPT: Large - Sal Generative Pre - traig for Cnerstonl Resse Generation,DiaoP,GPT,ecer,Lnuae Modeling,nan,GPT - 2 rcitectu trne on ilog data,Text generation in dlog setig,21 - 10 - 01,1. 5B,50. 0,140M Reddit convrsons,Mcroof,"htp: / / gith. com / icosft / aoGPT, ttp: / / hgigfae. co / ds / trasfors / mol_do / dialogpt","pn, MIT lcse",ttp: / / hugngae. co / mcroso / DioGT - ediu? et = Hey + my + ae + is + Mram% 21 + How + are + you% 3F
"DstilER, a dtiled vsin of BERT: mler, faster, cheaper and lhte",DstiBER,BE,Enco,"aked Language Mding, Next ntenc Prediction",nan,"Compressed version of BE using distillation, which is uh more efficient gin the me nber of prameer",ae as BERT,21 - 10 - 01,66M,66. 0,Same as BE,Huggngc,https: / / ugginfe. co / do / tasforme / odel_c / dtibert,"pe, pche 2. 0",hps: / / medm. com / uggngce / stlbert - 8cf3380435b5
Eplori the lmit of transfer arnig with a unified te - to - ex transformer,T5,Trnsfrm,nder / Decoder,denoig autoencoder,nan,Same as rgnal Transformer with se additions sc as eltve positional embeddings ie Transformer XL,"General lange tasks incdig machine translation, uesti answering, abstractive summarzto, and text casfcaion",09 - 10 - 01,"60M, 0M, 77, 3B, and 11B",000. 0,oosal Can Crawled Cops (C4) - lane up version of the Comm Crawl data - 750 GB,ogle,"htp: / / github. com / gool - research / tt - to - tt - tasfe - tasfrmr, htp: / / gginfae. co / cs / transformers / mode_c / t5","pn, Apce 2. 0",ttp: / / ai. oogblog. com / 22 / 02 / eplrin - tsfer - rning - wt - t5. hm
nuprvsed Crs - lingual Representation Learning at cal,XLM - RoBE,BRTa,Endr,MLM (Dyna ),nan,An extension of RoBERTa ta ntroues small parameter tuning iniht in the conx of multilingual aplictos,Translation and other cross - ngul lagug ass,21 - 10 - 01,as = 7M Large = 550M,550. 0,Cene Common Caw in 100 languages,Faebk,hts: / / hugingf. co / os / ansormes / mdldoc / xlm - obet,nan,nan
PEGASUS: Pre - training it Extace Gap - sentences for Abtacie Smrizatio,egss,Transrm,Encoder / Doer,DAE (or oncrety GSG) and MLM,nan,Extn illa Transformer by ung a different pretraining ak (GSG: Gap Sentence Geeatin) that is ette suited for smmaritio,abtaive text summarization,20 - 12 - 01,se = 223M Lre = 568M,568. 0,C4 (5GB) + ugeew (3. 8 TB ),"oole, Imperial Colg London",hps: / / huggingface. co / cs / tasfoers / odl_do / pgas,nan,hps: / / ai. googlbg. com / 20 / 06 / peau - tae - of - art - odl - for. tl
Multilingual Dnisin Pre - aning for Neural Mahi Translation,ART,BR,Encoder / Decr,dosing autoencoder,nan,xtns BR to multilingual capability,alation,20 - 01 - 01,Same as BA,nan,C5 orus cudes 25 monolingual orpse in different languages. Laes corpuses are English (300 GB) and Rsan (280 ),Facbk,"https: / / gitu. com / facebookresearch / faie / ee / mi / eampe / mbart, tps: / / hugggae. co / os / ranforms / model_doc / bar","en, MIT lens",nan
LTRA: Pre - taing Text Encoders as Discriminators Rter Than eeratrs,ELTR,ET,coer,replaced token dtcton,nan,Applied new training tecniqs including Replaced Tkn eecton,me as BERT,22 - 03 - 01,"Sml = 14M, Be = 11, Large = 35",335. 0,am as BERT xcet for Lae it is same as XLNet,"Gool, Stanford","hps: / / ihub. com / oole - research / lcta, hts: / / gigface. co / do / tasfrers / mdedoc / electra","Op, Apae 2. 0",nan
Mgato - LM: Trani Multi - Billion Parameter Lngug Models ing ode Parallelism,Mgaon,"T5, ER, GPT","Encoder or Decor, depeni on the as model",Same as se mod,nan,"Megatron is a family of models that extend previously known architectures (namely GPT - 2 and BERT originally, but so T5 more recently) by introducing mde parallelism primive. In the cs of BERT, the auos also epac the nt sentence prediction ad wh sentence order prdctin and use whole word n - gram masking.",Same as se mod,00 - 03 - 01,"8. 3B (GPT - ke ), 3. 9B (ET - li )",80. 0,"Oigna apr uses an aggregate dset conistn of Wikipedia ), CC - Stoe ), Rlews, and OpenWebtext",Vida,tts: / / ithu. com / NIIA / Metro - LM,"imte, Non - ommeril usage",tts: / / hugifac. co / blog / mgton - train
Lanae Moes are Few - St Learners,GPT - 3,GPT,Decd,Language odeln,nan,"Same as GPT - 2 with the ly ition of altetng ese and locally banded arse attention ptens, ipred by the Spar Transformer","Initially text geeatin, but has ov tm been used for a rge range of appicais in areas such as code genetio, but ao iag and aui generation",22 - 05 - 01,5B,175. 0,"~ 0B okes ncludi moCrawl (4B ), eTxt2 (19B ), Book (12B ), Bos2 (55B ), and Wkipdi (3B )",OeAI,"tts: / / pltfo. opna. com / ds / mels / gpt - 3 - 5, tps: / / ihub. com / opei / gpt - 3",cloe source,tps: / / onai. com / lo / gpt - 3 - ps
Dbrt: Decoding - enhanced bert it dsetaged attention,DeEa,ET,Eode,Masked Lagua Modeling,nan,Separate positional emding vector independent fm the content emeing sng disentangled aentin matrices for onent and relative positions,Same as BT,02 - 06 - 01,"134M (as ), 84 (lrg ), 0M (larg )",750. 0,"ngis Wipeda, BokCpus, OPENWEBTEXT and STORIES",Mcroof,"hts: / / huggingface. co / microsoft / bera - v2 - xlae, tps: / / huggingface. co / microsoft / deberta - v2 - xlarge, https: / / huggice. co / mirsof / erta - xlarge, https: / / hungfac. co / mrooft / eert - large","en, MIT lene",tps: / / www. mirsot. com / en - us / esrch / bl / mcrsot - dbra - surpasses - hmn - perorac - on - the - supegl - behark /
Big id: Transformers for ongr queces,Big Bi,BE,codr,Maed Language Modeling,nan,"Big Br can etnd the arctecure such as BERT, Pegasus, or RoBERTa by using a sare tenton mechanism that emiaes the quadratic deenden th making it more suitable for onge sequences","Particularly el uitd for longer qunces, not nl in ex but also e. g. in geocs",02 - 07 - 01,Dees on the overall arhtctre,nan,"oos, CC - News, Stoi and Wipedi",ooge,tps: / / hggigac. co / oc / transformers / mel_oc / bg_br,"Oe, Aace 2. 0",htp: / / hugginc. co / og / big - id
An Ima is Worth 1x6 Wos: Transformers for mge Recognition at Scale,ViT,BE,coer,image casfiatio,nan,Extension of BR architecture to train on tchs of imas,ag classification,20 - 10 - 01,86M (as) to 3M (Hu ),632. 0,From standard Imagenet to JFT - 00 (arg inus daas ),ogle,htp: / / ugingce. co / os / trnsmers / meldoc / vit,nan,nan
ro - ho Text - to - mge Generation,LL - E,GPT,Dcod,Caption pdictio,nan,A dieenial vtional to - encoder is ue to ear the visual codebook. The sformer is a vritin of GPT - 3,Text to ige,02 - 01 - 01,12B,200. 0,250 mlin ex - images pairs from the ntnet,OpnI,https: / / gihb. com / bordaya / dal - mi,nan,"hts: / / peai. com / og / dl - e /, htp: / / ml. keley. edu / bl / pos / dle2 /"
wich Transformers: cang to Trillion Paamte Models with Smpl and Efficient Sparsity,Swch,T5,coer / Decoder,deoisn autoencoder,nan,Goal to incre pamter cut while keeping OP pratins constant by using efficient routing of MoE (itue of Eers ),Gerl language tsk (e. g. esion anwing ),22 - 01 - 01,1T,1000. 0,Colossal Clean Craw Coru,Goge,"htp: / / ghub. com / oole - rearh / t5x, hps: / / gitb. com / tensoro / mesh / bb / aste / eshtensrfo / transformer / moe. py","pn, pche 2. 0",tts: / / www. lexaneram. com / en / bg / stch - trnsore - upsaln - to - er - a - illi - armetrs /
Learning Transferable Visual Moel From Nura aguag Supervision,CL,"lo uin esnt, ViT, and vanilla transformer for text, CL",Ener,"pdit hic of the N × N possible (mag, xt) pairings crss a batch actually ccred",nan,mbine Resnet and ViT for the vsul ecoig it Transformer for the exul encoder,Iag / Ojet classification,02 - 02 - 01,nan,nan,"WIT (WebImageText) - 400 ilio tt, mag ars",penA,tps: / / hugigfe. co / docs / tansfres / modloc / ip,"On, MIT lcse",nan
GPT - Neo: Lge cae utorgrsve Language Modeling ih Mesh - Tensorflow,GPT - Neo,GPT,Dcdr,Language Moeng,nan,Siia to GPT - 2 but es lcl tetion in every other layer with a window iz of 256 tkes,"xt geeraon, but adaptable to an other NLP tas he fine tuned.",02 - 03 - 01,"12, 50, 1. 3B, and 2. 7B",20. 0,Pi - 840 GB pn source tt dataset ta combines 22 pre exstg datasets,EluterI,"hts: / / thub. com / ElethrI / gpt - neo, hps: / / ugggfac. co / dc / tnformes / moldoc / gpte","en, MIT cens","tps: / / huggingface. co / lo / few - st - learning - gpt - neo - and - ineene - api, hts: / / www. section. io / egnring - education / leveraging - gptn - to - gerte - ai - bae - blog - cnen /"
Swin Trsfrer: ieraccal Vision Transformer Using fted Windows,Sw Transformer,ViT,Enor,ae as ViT,nan,Ends ViT by relaig the andad mul - he sf aenton (MSA) module by a module based on hife windows (Swin) llwig ViT - like architectures to generalize to higher esoutin images,"Image (bjct detection, ima lsificaio. . )",01 - 03 - 01,29M - 7M,197. 0,Iaget and Imagt - 22k,Micosf,htp: / / ithu. com / microsoft / in - rasfmer,"Oe, MIT cese",hps: / / www. scio. io / engneeg - educan / an - verew - of - sn - tnormer /
GPT - J - 6B: A 6 illi parmtr autorgsiv language mol,GPT - J,GPT,cder,nguae Modeling,nan,GPT - J 6B is a rnfomer mel trained usi eh Transformer JAX and am toknze as GPT2 / 3,Sm as GPT - 3,02 - 05 - 01,6B,60. 0,"Pile cops, a rge - sae curated dataset created by lutheAI",Elether,"tts: / / hgingfa. co / EleherA / gpt - j - 6b, hts: / / gtub. com / ngofloz / es - rasfrer - jax","pe, Apac 2. 0",tps: / / en. wipeia. org / ki / GPT - J
Dcion Transformer: Rinfrceet Learning via Sequence Mdeln,Dcsin Transformers,"GPT, onrl ranorers ” (not per se a faml, but grouping here the ransfmrs th try to mod mo general control, RL - lk, tsk )",ecer,Next action pediton,nan,eciin trsormer use a GPT architecture and xtnd it by encoding trajeori in a way that he can be learned by an ut - ressive ta,Gner RL (reinforcement learn tak ),21 - 06 - 01,me as GPT,nan,Dfeent corpus for different erimets,"Facok, Google, UC Bkely","hps: / / ghub. com / kzl / ecsin - transformer, tts: / / hugiace. co / do / tanormrs / ma / en / mel_oc / decison_former","pe, MIT cene",hts: / / ite. goge. com / berle. edu / ecisn - tsfrmer
Offline Reinforcement Learning as One Big Seenc Moing Prol,Trajectory nsforers,"GPT, Cntl Transrrs ” (not per se a amil, but grouping ee tho transformers th try to mde mo geer onto, RL - like, tasks )",eodr,predict most liel eqenc,nan,"imilay to the Decision transformers, the mi xtensn inrodcd by jectory rasomers is a way to encode a tajectr (sae, actions, ewrs )",enal RL (reinforcement lenng aks ),01 - 06 - 01,Smaller archietr an GPT,nan,RL datt and other RL dasts deding on the task at hand,UC Brkel,"htp: / / trajtry - tafomer. gthb. io /, https: / / itub. com / aner / trajctr - tasfrmr","pe, MIT cese",tps: / / ai. berey. edu / bg / 02 / 11 / 19 / raecory - trasfoe /
HM: Hyper - Text Pre - raing and omptig of Language Mods,TM,BA,Encoder / ecer,enosig autoencoder,nan,"As opod to BA, they don ’ t do snnce sfling",Genr purpose language mod that allows structured HT prompting,20 - 07 - 01,4M,400. 0,23TB of smplifd HTML xtaced ro CommonCrawl,Fcebk,nan,nan,nan
Jrasc - 1: Technical dtil and valutio,Juasc - 1,GPT,Deor,Language Mdeln,nan,"Very imir to GPT - 3, but far more paramer and improved training ficieny osly baus of the improved tokenz. Also, ierent rti of ept to breadth",ilar to GPT - 3,21 - 09 - 01,"78 (umb ), 17B (ande ), 7. 5B (Lrg )",1700. 0,0B kens (ae as GPT - 3 ),A1,htp: / / ihub. com / 21abs / lm - evlaton,"Csed source, acesibl through API","https: / / www. i2. com / blog / a2 - udio - use - cae, tps: / / www. a1. com / bl / anoucng - i2 - studio - and - jrass - 1"
"Ung epSped and Megatron to Train Megatron - Trng NLG 530B, A Large - ale Geneaie Language Mde",Mgtrn - Trng NLG,GPT,Dcde,auage Modeling,nan,Uses parallelization simi to Megar to rin a LM uble the ie of GPT - 3,Lanag gneraon and thes (sila to GPT - 3 ),22 - 10 - 01,3B,3000. 0,The Pile (0GB dataset) + 2 omon awl snahot,NVii,nan,"Lmed, Non - commercial sae",https: / / developer. vidi. com / lg / using - deepspeed - and - metrn - to - tra - megtn - ting - nlg - 50 - the - worlds - gest - and - most - prful - generative - language - del /
A eerl Language sistat as a Lboraoy for Alignment,nthroi Assistant,ansfomr,eder,Lanae Modeling,nan,These models do not ntoduc novelties at the architecture / etraini evl and they are based on GPT - 3 but rather focuses on how to improve alignment through fine - tuning and prompting. Note at the Anthropic Assistant includes serl models optimized for dffern ass. Ltst versions of hs work cus on the benefits of RLHF.,Different models it difren apicatns from gnea dialog to code assistant.,22 - 12 - 01,10M to 52B,200. 0,400B toes from flter Como Cra and ook. Ty also create several Dialogue Preference dtaes for the RLHF training.,Anthri,nan,nan,"tps: / / axi. org / abs / 24. 862, hps: / / arv. org / abs / 21. 081"
GLaM: Efficient Saig of Langu Mods with xtre - of - Experts,Ga,Trasrmr,ecod,Language Moelg,nan,"GLaM intrdus a Mxtu of 64 Experts to inras parameter count and eealiztin properties in a smwha standard decoder - only. Transformer architecture. Only two eprs get activt at a time per oke, which makes the moe lo more efficient in training and inference.",General anuag modeling - tested arss 29 NLP tsk,22 - 12 - 01,"1. 2T aros 64 xrts, but oy 96B get atvaed for inerec",2000. 0,1. 6T oens including web aes filtered by iipedi and bok for qual,Gole,nan,cose source,tps: / / ai. ooleblg. com / 22 / 12 / re - icient - in - onte - eanin - it. tl
GDE: Towards Photorealistic Image Geneaio and Editing wt Text - Guided Dffuon oels,GLD,ifuion models,coer,Caption preicio,nan,"GLIDE can be seen as an extension of the ADM (Ablated Difuso oel) by the same tors. However, ADM is not per se a transformer architecture although it does resemble one in se of the configurations the thor use. ive that ADM is by the sa authors and was qicy followed up by GLIDE, I think it is fair to cosid GLIDE as the first of its ki.",Text to mae,22 - 12 - 01,"3. 5B difuio mde (2. 3B for visl enoig, 1. 2B for texa) + 1. 5B for mde for psamlig",50. 0,Se as DL - E,OpnA,https: / / gihu. com / oena / gde - tt2m,"Oe, MIT ices",nan
"cali Lagge Models: Methods, Analysis & amp; Isght om Tanng Gopher",Gher,GPT,ecdr,Language delig,nan,Sa as GPT - 2 but use RSNorm instead of LaerNr and relative ositioa encig rate ta absolute,"Mostly Lnuag Melng and NLU, but so extensible le GPT",22 - 12 - 01,80,2000. 0,"Msie Tt (2. 35 lion documents, or aou 10. 5 TB of te cludig Msiv Web, Books, Giub, ew, C4, and Wikdia.",eepmd,nan,closed sour,tps: / / www. deepmind. com / og / lauge - mellig - at - cle - gper - eica - consdeati - and - reeval
ih - esluion Image Synthesis with Latent Dfsion Models,SabDifusio,iffsin,Encoder / Dcer,Ctio prediction,nan,Stable diusin is bascly the Latent Diffusion model developed by LMU Munich eerches + se laring on cotiona diffusion om AL - e and Imagen,Text to ima,22 - 12 - 01,"890M (although tre are difren, salr, aants )",890. 0,"LAION - 5B, a blicy available taet derived ro Cmmo Crawl","EleuhrI, Staiit. ai, LMU Munc","https: / / uggnfce. co / CompVis / sble - dffsio, htp: / / uinface. co / sces / statyai / able - ffuson, https: / / ithu. com / Stability - AI / stablediffusion","en, CreativeML Open AI + + - M icee",tps: / / stiity. ai / bo / stable - difusi - ulic - eles
CM3: A Causal Msed ltimoal Moe of the Internet,CM3,TL,Decd,Cusliy - masd LMs,nan,"This is mehat similar to TM in its use of strtred training data. However, it is a ierent architecture and se causal masking, wih aes the model predict, at the end of the sequence, an etie missing span of text. It also includes image input via Vcto ntized Variational Autoencoding (VQ - VAE) tokens.","Multimodal language mel ih the ability to do structe prompting, er - shot catning, image generation, and enty linking (via trge et prition of hyperlinks )",02 - 01 - 01,"2M (mal ), 80 (sal ), 2. 7B (eium) and 13B (rge )",300. 0,"CC - News, Enih Wiipei",Fcook,nan,nan,tps: / / ilianwn. itub. io / pts / 02 - 06 - 09 - vlm /
LaMDA: anuge Moel for ialg Applications,LAA,Tansfrr,codr,Lanug Modeling,nan,"LDA fuss on how to improve afet, qalt, and groundeness sig different fine - unng strategies","General lnage oling, such as translation, maization, question and wers",22 - 01 - 01,17,1370. 0,1. 56T wor from pubi dialog data and other pubc web doumes,Ggle,nan,cloe source,"htp: / / ai. googleblog. com / 22 / 01 / ada - towards - fe - roudd - and - hi. ml, tts: / / lg. google / teclogy / ai / lamda /"
raing language models to llow nsrtions with human feedback,strucPT,GPT,Dece,Language Mdeig,nan,GPTInstruct starts off wh a pretrained GPT3 ode and ds reward moeig through reforcemt learning after a supvise finetuning,oledge - intensive dial or language tss,02 - 01 - 01,Se as GPT3,nan,"Same as G3 for pretraining, but inetun and optime using labr da and prompts",OpnA,hts: / / ithu. com / opei / fllowg - instructions - hum - eebac,"Closed suce, accessible hroh API","hts: / / sh - tsang. mdum. com / review - instructgpt - train - nguae - modl - to - follow - instructions - with - human - feedback - 7fce4f9a, tps: / / opea. com / resec / instruction - fllowg"
netune language models are ze - shot arner,la,LaA - PT,cder,Instruction unng,nan,eo - ot task lrnig. The oupt space for a given task is either one of several clas (ssificaon) or re tt (geneaio ).,"aura language comprehension tasks uh as infenc, sentiment ansis, paaprae, clsd - bo QA, edig comprehension, coreference, summritio, translation, commonsense reasoning, and strc - to - text",22 - 02 - 08,37,1700. 0,"FL is instruction und on 25 tasks spnng 62 datasets. , aDA - PT is is pretrained on a collection of web uments (incudg those with computer code ), dialog data, and Wikipedia, tokenized no 2. 49T BPE tkns wh a 32k voclary",Goge,hts: / / gtub. com / ooge - research / LA,nan,"http: / / ryanchfer. github. io / logpsts / 2022 - 01 - 20 - gogl - bin - fl. html, https: / / ai. googleblog. com / 22 / 10 / irducin - fa - mo - generalizable. hm"
Tring Compute - Optimal arg nuage Models,Chinhla,GPT,Dodr,nuage Modeling,nan,me as Gopher but ih ptization to reduce mdl size and terefr training / inference im with equal or speir performance,Same as Gohr / G3,02 - 03 - 01,70B,000. 0,"1. 4 trillion raini tokens. Masv ex (2. 35 ilin doment, or bou 10. 5 TB of te incldg Massive Web, Books, ihub, Nw, C4, and Wikipedia.",eemid,nan,cloe source,tps: / / meim. com / earnin - ai / language - modl - ne - proper - trini - 71472f00
DQ - AR: fficen quece - to - Seqce Model via Joint Distillation and Quantian,DQ - AR,BR,Enco / Decoder,denosn autoencoder,nan,Ad uanizion and istilaon to a BART model to improve performance and mod ze,Tt generation and unertndig,20 - 03 - 01,Up to 30x redcto in paramet compared to tnard BART,nan,"CNN / DM, UM, L5, WM1 En - Ro (~ 1M kens )",mazn,tts: / / ghub. com / aaon - scce / dq - at,"pe, Apac 2. 0",tps: / / www. amao. sine / ulcatons / dq - br - efficient - eunce - to - sequence - mod - via - joi - istilltn - and - uanzaton
Teaching language moel to support answers wh verified qute,GoprCit,Goph,Dode,Language odeig,nan,opherte is bse on Gohr but adds a tp using RLHP (Reinforcement Larng fr Hma Preferences) to lar whether not oy a response is plausible but also supported,"Dlog syse, Q & A, general lngag generation aks",20 - 03 - 01,8B,8000. 0,Sa as Goph plus specific dataset generated in the LH poss,Demin,nan,lsed source,hps: / / www. deepmind. com / bl / gphecte - tcing - language - moes - to - surt - swer - ih - vefed - quot
Lnage Mels that Seek for Knowledge: Modular Seah & Generation for Dialogue and Prom Complon,eeKr,GPT (but can tend any amil ),"ncor / decoder or dede only, depend on the se mod it ’ s extending","LM training, Dilog riing",nan,"Seer is an etenio tt can be plie to any Tsforme arcitect by iroduig “ search ”, “ knwlde ”, and “ spone ” modules that are introduced urin pretraining",me as base odls,22 - 03 - 01,"SeeR iague: 0M, 3B; SeKe LM: 5M, 7M, 1. 5B, 22 BlenderBot: 0M, 3B",nan,"Wizard of the Internet / Wiipda, Peronat, Blended Sil Talk, Emthic Dialos, Multi - eson ht, MS ACO, Natural questions, SQuAD, riviQ",Faebo,hts: / / pr. ai / prect / seer /,the code is op ouce,nan
GLM: General language model pretraining with orerssive lak nfilli,GLM,GLM (enra Language Mel ),Eodr / Decoder,uo regreve blank infilling,nan,GLM has a bidirectional cder and a unidirectional deoe in a nfed moe,a Gene Language Model pretrained wt an torgessie lak - filling objective and can be finetuned on vars aual language undtandig and generation tks.,02 - 03 - 01,"Base = 1M, Lag = 3M, and ls 2B, 10B, 13",1000. 0,"Pi, GLM - 3B Chinese corr, P3, DepSuct finetuning dataset",Tsinghua iversty,tps: / / gthu. com / TUM / GLM - 13,"Op, MIT inse",tp: / / keg. cs. sngua. edu. cn / glm - 10 / pst / glm - 1b /
ultask prmte training enables zero - ht task generalization,T0,T5,Encoder / Decr,nan,Natural guage prompts,"T0 stands for "" T5 for Zr Shot "", obtained by fine - unng the T5 model on multitask iure covering many different NLP tasks. Compared it T0, T0p and Tp were ie - tue with more daast. T0pp is reconed as it ads (on average) to the best performances on a variety of NLP tasks.","rfom eo - shot inference tasks by specifying the quy in natural lnuae, and the mols wi generate a predict.",22 - 03 - 01,"T0 - 3B: 3 bion, T0, T0p, Tp: 11 ilio",100. 0,"T0 (Multiple - oice QA, Extractive QA, Closed - Book QA, Structure - To - Text, Sentiment, Smarzaion, Topic assifican, araphse Identification. T0p (same as T0, wt diional datasets fo GPT - 3 ’ s evaluation suite ). pp (same as T0p, it ditinal datasets from SuperGLUE, excluding NLI sets )",BigSinc,hts: / / hggigac. co / igience / T0,"en, Aphe 2. 0",nan
Hierarchical Text - Cdiinal Image Geneion wh CLIP Latents,AL - E 2,"LDE, CLIP",Encoder / Dodr,Cion prediction,nan,ombis CLIP encoder and Diffin ecod similar to GLIDE,Text to iag,22 - 04 - 01,3. 5B,50. 0,Combination of the LL - E and CL dtaes,penI,nan,"lose source, accessible toug API","htt: / / oena. com / prou / ll - e - 2, tts: / / ab. pnai. com /"
Flaig: a Visa angue Model for Few - Shot ernig,laing,Chinill,Deod,Log lielhod of text given se isua input,nan,"It us a frozen textual lngug model (ke hnchlla) onitned on the visual rersetton, which is code fm a Normalizer - Free ResNet",Text to iae,02 - 04 - 01,80B (lart ),000. 0,"MultiModal MassiveWeb (M3W ): 185 million images and 182 GB text + a number of text paired wt ima datet: ALIGN + LTIP (Long Tx & mag Pairs) = 312 million images, and VTP (Video & Text irs) = 27 mlon short vdes (pprxmatly 22 seon on average )",Deind,nan,lose source,"htp: / / medium. com / gkcuure / 3 - overlooked - thns - deepminds - flamingo - a - rge - model - for - coutr - vision - 8492f73c, https: / / www. epmid. com / blog / takng - multiple - tasks - ih - a - ingl - visual - language - model"
PaLM: Sali Language Mling wi Pathways,PM,asfomer,Dedr,Language Mdlin,nan,"Palm ue a typical decoder - ly transformer architecture, but adds quite a few extensions: SwiGLU atiaton, parallel layers, multi - uey attention, RoPE ebddngs, Shared Iut - Output Embeddings, no biases, and a 26 enncePiee vocabulary neratd fm the training data",PalM is designed as a geea purpose lange ode with applicability to nreds of different nuage tsk,22 - 04 - 01,"8B, 62B, and 0B",4000. 0,"80 tokens from filtered wepes, books, Wikipedia, es rtice, orce code, and social mea conversations. od incld 24 programming lnuage.",oole,https: / / gihu. com / lucdrin / Pa - prch,nan,"https: / / bg. oogl / tchnlog / ai / troucng - patys - nt - generin - ai - architecture /, https: / / ai. googleblog. com / 02 / 04 / pathways - nguag - model - palm - scaling - to. tm"
GPT - eo - 20B: An Open - Sure Autoregressive anuae Mdl,GPT - eX - 20B,GPT,ecdr,Language odeng,nan,"Similar to GPT - 3 with otay enodr instead of positional, parallel tetion and feed forward laer, dieren itialiion, and all dense ayes ited of alternate ens / spae",ae as GPT - 3,20 - 04 - 01,20B,000. 0,Pl — 840 GB oe sorc te dataset that combines 22 preexisting datasets,EluherI,htp: / / huggnac. co / leuterI / gpt - no - 20b,"pe, ache 2. 0",hps: / / bg. eeuhr. ai / nouncig - 20b /
A Generalist Aet,at,"“ otro ansfores ” (not per se a family, but grupg ee hoe transformers ht try to odl re general control, RL - le, tasks )",Dcer,MLM (whe kens are either ex or agent cton ),nan,"The standard decoder - ol transformer archietr is preceded by an embedding lay ta can med text and mags, plus add position ecings to add saia formatn when plicabl.",Gato psnts a generlale agent that can be used beon te to ask such as playing Atari or onollin a bot arm.,22 - 05 - 01,"79M, 64, and 1. 18B",18. 0,"1. 5T kens icluin saard text (e. g. MsieTet ), vion (e. g. AIG ), and simuati environments (e. g. ALE Atari, or RGB Stkig Ra bot )",eepid,ttp: / / gthb. com / OigamiDa / gt,closed sure,"htt: / / www. eepid. com / bo / a - eneaist - agt, htp: / / www. epind. com / publicin / a - geealis - ant"
OPT: pe Pre - trained Transformer Langu Mdls,OPT,GPT,code,Language Modig,nan,Basically se architecture as GPT - 3 but wt some trini imrvmnts intrdcd in Mearo - LM,Sa as GPT - 3,02 - 05 - 01,7B (and ote smaller ersio ),1700. 0,180B tokens = RoET + the il + PuShif. io eddi,Fcebo,"htp: / / thub. com / fcbookeeach / tase, htp: / / uggnfce. co / cebok / opt - 30",nan,htp: / / ai. facok. com / bo / eoratzing - access - to - lar - sce - angag - odls - wh - opt - 75 /
Opt: pn pre - trained traorer language mode,OPT,GPT,Dcoe,Language oling,nan,acally same architecture as GPT - 3 but wh om raing improvements ntruced in atron - LM,me as GPT - 3,02 - 05 - 01,15 (and otr slle versions ),1000. 0,180B tons = oBTa + the Pile + Pushft. io Redt,cbook,htp: / / gihb. com / febooresech / meas,"Limited, non - omecial lene",ttp: / / ai. fceok. com / bg / ocrtizing - access - to - rge - ale - lagag - mols - it - opt - 15 /
Ul2: fying lguge learning paradigms,UL2,Tranfre,Ecoe / Decoder,"Mixture - of - Denoisers, which cmnes diverse pretaii padims oethe",nan,UL2 - 20B (niing Language ernng) can be interpreted as a model that is qie similar to T5 but rind wi a different bjecti and sligl fferen alig nob.,A unified rmewrk for pre - training models at are versaly effective acos daaes and seus.,22 - 05 - 01,20B,200. 0,1 illio tkes on C4,Gool,tps: / / github. com / gole - searh / gooe - reerh / re / mase / ul2,"en, Apah 2. 0",nan
lbal Context Vision Transmrs,Global Cont ViT,ViT,Encd,Image Classiiai,nan,hierchic ViT architecture cnsitng of local and glba self - antion modules,image eeratin,22 - 06 - 01,90M,90. 0,mgent - 1K and ohe as dependent dataasets,NVia,htp: / / gtub. com / NVla / CVt,"Lite, non - cmerial icne CC - BY - NC - SA - 4. 0",tts: / / owadsatasece. com / glba - context - visi - tansores - nvas - new - st - age - model - 293bf48e
horeaisic Tt - to - Image Diffusion Mels with Deep Language Udersting,Igen,"Dfuson models, CL, T5",T5 (or IP or ER) for foze text enoe + U - net aciectue for cascaded dffuso mods for xt to image,image / text pr rection,nan,"Imagen ds a few xnsions to the U - net iffson architecture (pold ebding vecr, crs attention oe ex embeddings, and Layer Normalizations )",Text to ige,22 - 06 - 01,2B,20. 0,"a combination of ternl atats, with? 46 ima - text pairs, and the licly available ion daet, with? 400M image - xt pir",Goge,nan,closed soce,hts: / / imae. research. gole /
ling Quanttve Reasoning Problems with Language odel,ierv,aL,Deer,Language deing,nan,Extends aM by fine - uing on the mahmaicl dataset,Mathematical esning,22 - 06 - 01,54,5000. 0,"me as PaLM + 18G atse of scientific paps om the arXiv preprint server and web pages at contain mathematical expressions using aTX, MathJax, or ohr mtematil typesetting rmat",Gogl,nan,closed srce,hts: / / ai. ooglelg. com / 20 / 06 / ierv - solving - quantiti - reoing. hm
Gdl: Lge - sca pre - training for gl - directed dialog,God,"T5, GPT",Dede,Language Moeng,nan,"In contrast with earlier models such as iaoPT, GODEL leverages a new phase of grounded pre - tring designed to better uort daptn GODEL to a ie range of downstream dialog tks that require information extrl to the current conversation (e. g. , a database or ouent) to odue od responses.","open - domain goal - irced diag tasks su as knowledge - grounded rspoe gneratn, sk - oriented dalg, and cnvesatoa QA",20 - 06 - 01,"22 (ba ), 7M (lae ), and 17 (XL )",1500. 0,"17 dialog seons for a otl of 6B tokens from Reddit mmen cins for DialoGPT. And grounded daog corpora like TC7 Task 2 corpus, MS MAO, UnifiedQA, and Schema - Gude Dilg.",icroft,"https: / / huggingface. co / mrooft / OEL - v1_1 - lre - seqs? et = Hey + my + name + is + Mrma% 21 + How + are + you% 3F, htp: / / huggingface. co / microsoft / GODEL - v1_1 - base - eqsq? text = Hey + my + name + is + Julien% 21 + How + are + you% 3F, https: / / gthu. com / icroft / GODEL",nan,hts: / / www. micsot. com / en - us / searc / lg / gol - coming - oa - inted - dalo - wh - real - world - conversations /
LOM: A 76 - Parameter Open - Aces Multilingual Language odl,LOM,GPT,code,Lauag Modeling,nan,Main diferec to GPT - 3 is that it se ll attention instead of srse attnto,am as GPT - 3,22 - 07 - 01,"56, 1. 1B, 1. 7B, 3B, 7. 1B, and 6B",1700. 0,"https: / / opnrevw. net / fom? id = UoEw6KigkUn, 3B toen (1. 5 TB of text ta) mutigual dtae (46 nral lguage and 13 programming lagage )","Huggace, Big cene",htt: / / hugngfe. co / docs / tasforms / del_dc / loo,"Open, but ne to foll restions in Atcment A, BiScnce RAIL icen v1. 0","hts: / / huggingface. co / blog / blm - maron - depspe, https: / / huigfce. co / blog / bloom - nernce - pytorch - scri, hps: / / hgngace. co / blog / bloom - infene - optimization"
BlenderBot 3: a deloy crsatinal agent that continually learns to rensily nage,BedeBot 3,GPT,Deer,Language Mdeln,nan,BlenderBot 3 is bse on a pre - trained OPT. It ds features needed for a dialog agent such as lo - term memory or the ailt to search the ntern. It is also fi - ted for some pcfic tasks given hun febak on them.,sa as GPT - 3,02 - 08 - 01,"3B, 30B and 15",1750. 0,80 toks = RoBERTa + the ie + PushSt. io Reddit,Febok,"tts: / / parl. ai / projects / bb3 /, https: / / github. com / facebookresearch / PlAI / bo / main / para / zoo / bb3 / modecad. md, https: / / ithu. com / facorsearch / ParlAI / ob / main / pjets / bb3 / gets / README. md","Limited, non - comecal, seach ol",tps: / / ai. facebook. com / og / bendebo - 3 - a - 5b - paraer - pubcy - avilab - chatbot - that - imres - its - skills - and - sfty - er - time /
Alexatm 20b: Few - so eanig usg a lre - scale multilingual seqq model,AleM 20B,trformr,Eder / Decoder,tmizes doisng (80%) and Prei LM (20% ),nan,Drid from BART and layernorms oaed exac at the beginning of ac lar. Encoder iniiaed with ineal 10B pre - trained eder.,"umrizaton, mut - lingual machine tanaion and NLU tasks",02 - 08 - 01,20B,200. 0,ikidia and mC4 dtaet in 12 lnuaes.,mazn,tts: / / itub. com / amzo - sinc / lex - ache - models,"Lmtd, non - comeril",hts: / / www. amzo. snce / lo / 20b - parmet - axa - mde - sets - new - aks - in - few - st - earin
mpovin alignment of ialgu agents via targeted umn judgements,Saro,GPT,Deer,Lauag Modeling,nan,Stas ro the Chinhll 70B moe but as RLHF (Reinforcement Learning it Human Feedback ). It also ds inline evidence a la GherCit,Dialog ages and general angug eeraion applications li Q & A,22 - 09 - 01,70B,000. 0,Sm as Chinila + interactive data gthein with uan anotars during the RLHF process,Depid,nan,cled source,tps: / / medium. com / to - cut - a - og - ape - sho / spro - improving - lignmn - of - dogue - gens - via - targeted - hma - judgments - e0876480
Scag instruction - finetuned langa models,ln - T5,T5,Ecdr / Decoder,Istctin Tuning,nan,"strtion ietning with a particular focus on (1) scli the nber of tas, (2) ling the del sz, and (3) ineunig on chain - of - hogh data","The primary use is to underestand how to improve rge language models with the right kind of instruction fine - tuning. The ous is rarch on zero - sh and in - onxt few - shot rning NLP tak, such as reasoning, and question answering; advancing aress and safety research, and ndesandin limitations of current lre language models",22 - 11 - 01,"80M (Fl - T5 - all ), 0M (Fn - T5 - ae ), 70 (FL - T5 - Lge ), 3B (an - T5 - XL ), and 11B (la - T5 - XXL ).",100. 0,"Fn finune it tasks in Mffn, T0 - SF, IV, and CoT",Gooe,"htp: / / gihb. com / gogl - resah / t5x, tps: / / uggifac. co / os / trasform / moldoc / ln - t5","Oe, ache 2. 0",tps: / / ai. ooglelg. com / 02 / 02 / the - an - llecion - avcing - op. tl
Scaling instruction - finune language mode,Flan - Pa,aL,coer,nan,Instucis for zero - ot and few - ht tasks,"an - aM is generated by "" Fa Finetuning "" the PM modl: (1) scaling the number of tas to 1, 836, (2) slng the model si, and (3) finetuning on chn - of - thought aa.","Same as Flan - T5. The ol is to ho Flan finetuning can ee mpov on the aest Google LMs (+ 9. 4% iproven average across tasks ), wt improvements to chain of tght, self consistency, ultlinga tasks, arithmetic reaoig",22 - 11 - 01,"8B, 62B, 0B",0000. 0,"Fl feuned wt tasks in ffin, T0 - SF, V2, and CoT",Goge,nan,coed source,nan
Galactica: A arg lagag ode for science,aatica,trafrer,Dodr,Lngag Modeling for cintiic domain,nan,"Transformer bed ahiecure in a decoder - only stp wi a few modifcons. Data xensons include ecal okns for working memory, citations, genetic at, and a few other biology related ass.","The mels are dsged to pefr scientific tasks, including but not limited to citation prediction, scitfic QA, mheaticl raoing, summarization, cment generation, molecular opery pedctio and entity xaction.",22 - 11 - 01,"in: 1M, se: 1. 3B, stada: 6. 7B, age: 30B, ge: 20",1000. 0,"Trained on 106 billion tken of open - access scentii text and da. This includes aers, textbooks, centiic bsits, enclodias, reference material, knowlg bas, and mo",Me,nan,"imid, non - comerca CC BY - NC 4. 0 lcen",hps: / / galcca. org /
xt mbeddng by Weakly - Supered Contrastive Pre - training,E5,BE,nder,nan,Semantic silrity using contrastive os,Fn - tunes BERT - sed odes to create text string ebddins optimized for semantic elateds,Text ebddngs for semantic relatedness tas such as xt clustering or search retiev,02 - 12 - 01,0M,300. 0,"MS - MAR, NQ, NLI",Mcroft,hps: / / huiface. co / ntfla / e5 - lge,"pe, MIT icee",nan
"One Eeder, Any Task: Intrton - Finetuned Te Emdings",InstruR,T5,codr / Decoder,nan,Wi variety of ntution based te - to - te tasks,ne - nes T5 expicit to optie encoder to produce a general purpose text trng embedding usel for ny NLU tasks.,Any NLU sk requiring a single et srin emedin. As of April 20 strucOR is the top - ranked ytem on the aive Tt Embedding Benchmark (TE ).,02 - 12 - 01,30,330. 0,ietned on MEDI,"Ma AI, Univeiy of shingtn, University of ng Kong",hps: / / uginace. co / kulp / insrtor - xl,"On, Apae 2. 0",nan
LaM: Open and fiient Fundatn Language Models,LMA,rnsormr,code,anuag Modeling,nan,"LLaMA uses a Transformer architecture, and with extensions: Pre - nrmalztin, SwiGLU activations, RoPE embeddings, rdue memory usage and runtime through efficient implementation of the causal multi - ed atento, checkontg to reduce the amount of actions that are recomputed during the ackar pass, mde and seqnc parallelism to reduce memory sge of the model, and uses 1. 4T BPE tokens after tokenization.","ro and few ht Commonsense esonig, Question answering, Code enrtion and eang comprehension.",22 - 02 - 27,"6. 7B, 13. 0B, 32. 5B, and 65. 2B",620. 0,"approximately 1. 4T tokens ro vriu sources: 3. 3 TB ommonCw, 73G C4, 38G thub, GB Wikipedia, 8B Bos, 92GB AXv, and 78GB StackExchange",et AI,"tps: / / huggiga. co / ds / tnformes / main / mdel_c / lam, htp: / / github. com / fcebkreseah / lam","Limited, Non - cmercil bpke iens",tps: / / ai. facbo. com / og / are - nguae - moe - lam - ta - ai /
"Alpc: A strong, replicable istrcto - following del",Alac,Laa,coer,nan,human insttons,Alpaca is in - tuned fr a 7B LMA model,Evaluated on a ariy of text genraio and claiicato aks.,03 - 03 - 01,7B,00. 0,"52K nsuctin - ollwin data gerate using self - itrct echnis, from 175 human - written instruction - otpt irs.",Stafr University,tps: / / ihub. com / tat - lab / stano_alpa,"Lmtd, Non - commril spke license",https: / / mdim. com / rson - 1 / snfor - lpca - a - small - yet - migh - language - odl - for - isucton - foloin - tas - a9e9289a
ythi: A suite for analyzing large lague models cros trnin and scaling,Pyth,ytha,Dcor,nan,nan,Trained wh the lrar GPT - NX,"Reach on language del ’ s behio, functionality, and lmittis",02 - 04 - 01,"70M, 6M, 40, 1B, 1. 4B, 2. 8B, 6. 9B, 12B",200. 0,il,EltherA,tts: / / giub. com / EltherA / pyta,"pe, Ache 2. 0",nan
