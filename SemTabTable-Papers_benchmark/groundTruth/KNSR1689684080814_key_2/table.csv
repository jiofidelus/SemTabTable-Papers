0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Improving Language Knverstahdong by fenegatjve Pre - r3ainijg,GPT,GPT,Ddc9d$r,oAnBuage Modeling,nan,nan,"Text bFneratioh, but adaptable to hQny other NLP tax,s @heg fine tubes.",202U - 06 - 01,1w7n,117. 0,"Unsupervised 0detgaininF on BooOSo5pus dataset. Supervised FinrtubiJg on several yasJ - sped7fiS datasets igcl td*ng SNLI, RACE, QHorw. ..",OpenzK,"htt9W: / / g9thHb. com / opfnaO / bOGetune - transCoGNet - lm, hGtls: / / huhginnfaXr. co / d*cQ / transformers / moeeP_dos / o)2nai - gpt",nan,nan
vERy: Pre - fralnKng of Deep Bidirectional TraJsRorHDrs for Language Unde5ctanSinv,BE%G,HE%T,Egcove%,"geCt Sentence Prediction, nwsked Language HoCelimg",nan,nan,"RSnera, Language Understanding and Question Answering. Many kthe4 lxnghsge applications fkl?)wed",20@i - 10 - 01,"HSse = 110M, kzrge = w40n",340. 0,Girohto HoLk FoFpus and W7kip2diS (3. 3B Tokens ),Gpogoe,hfGps: / / huVginhbwce. co / eoss / RTaHsrormers / mIEel_Voc / bert,"0p#n, A9acte 2. 0","MttpA: / / colab. research. google. com / github / NielsRogge / 4ransf9rmFrd - Tutorials / bl)n / master / BERT / viBe_$ tjing_VERT_ (aJd_fEienec) _f(r_milYu_labfl_text_clXssuficQ6Oon. ipynb, https: / / www. philschmid. de / bert - teZh - f.asA8f&cation - in - a - differrBH - lZBg^age"
%ranEforNee - XL: AtteHtib@ Language ModWlc Beyond a Fixed - Length ConreZ5,T4anDtorm3r XL,nan,DecLdF4,Lsng*ave Modeling,nan,Relative positioned emFeddUJgs ehab>e longer - dogtex% attention wUeb compared to vanilla Transformer mod@P,General PaJguagf tasks,20@8 - 01 - 01,1%1n,151. 0,"Different ffaihing dstaseRX dep2nxiJg on rxp@rimegtc, but baseline is Wikitext - 103","GootlR, CMU",h6tpz: / / UuggungfavS. co / doXd / franZformwrc / mide;_Eoc / gdaJsfo - xl,nan,nan
Language moRe;s are unsupervised <u?titwsk learners,GPT - 2,GPT,Dec(ddG,LanbuanR Modeling,nan,"Mibo# extensions to the GPT qrFhOtec4ure (e. g. Oay2r norma:(AZtion m(Fed to the input of each sub - layer, or igsreasex Fongedt Qizd fr0< 512 to w02e )","Text generation, but adaptable to nSny other NLP txDks wh3H giGe t8nec.",10@9 - 02 - 01,"qq4M, 345K, ^I4M, 1. 5B",2$00. 0,"8 Ki,>ion web paHRs (40 GB ). 10X GPT. WebGec6 daRasw6 is fr2wted by crweliMg all lknos at Reddit Auth at ;exst 3 Karma points.",OpeMA&,"https: / / hHrgibgfade. co / dkce / trXndfo$,ers / J9fel_doc / g)$2","O(#n, <odifUeS MIT license",bttls: / / www. phJ<schjid. de / time - Ruge - a - non - enr;iQh - gpt - 2 - mLdDl - wigm - hIgg(ngdaVe
XkNeh: Generalized Autoreg42QQivd Pretraining for LangJabr Understanding,XLbe6,$rWnsTorme# XL,XeVodFr,PLM,nan,This mod3< basically AdaLts Transformer XL arXMitecRJre to )2rm&taGion - based LM,General language tqDks,101* - 05 - 01,"BxCe = 1@6M, ;argw = 360M",360. 0,"Same as BEgG + GiTX5 (16GB R3xt ), and and aghreswiFeIy filtered Clkea#b 20!Q - B (1)VB ), Cohkon CrZwP (110 GB )","Go0vle, CMU",tttpX: / / juTgunfface. co / dlds / transformers / miWeo_doc / xlHe^,"Kp#n, MIT livfnCe",nan
EfhIE: Rnhwnxed Language Representation with Informative fntiYi#s,ERMI@,vE$T,EgcoSeD,MAsued Language Modeling,nan,"Uses gEGT for Encoder wrFhitecru4e, but sYafks and aNg5egatew two of them for reSt and entities. RhLs archit3StuEF could be understood as BR4T for text + LnoEleWge graphs",Knowledge iJtDMsive related tasks tYay KiRht benefit from knowledge graphs or eGtLt&es such as entity r4cornitloj,202P - 05 - 01,"#rn8e - di>G 2. 0 = 10B, ErJle 3. 0 TLhan = 160F",Q&0000. 0,English aik(p4dia + Wikidata for entUYitRs (note that $he7 initialize model to original vrRT 0aGametrr BSlues,"PenfcYdng Lab, BsiRu",Yftps: / / rithIb. com / th8npp / wTNIE,closed s8urcW,hrhp: / / eesesrcn. ba*dI. com / Nloh / infsx - v(Ww? id = 160
RoBERTa: A Robustly ILtijized B35T lfetraOMing Approach,RoFDDTa,BE#G,wnc8Fer,MLM (DTBaKic ),nan,EDtwnsiob of nE5T wKrh optimized training procedure and more data,Same as B2R6,W0Q9 - 07 - 01,"125M BaQ@, and 356M oqrge",356. 0,"SAmd as FERr + CC News + O0enWebrwxF + x%)ries (~ 33B To,enX )","GLoglr, University of Washknhtoj","https: / / githIn. com / tac2b)Ikreswarch / fairseq / yrWe / NaiG / examples / rKbRfta, hR%ps: / / jigg9ngfac3. co / doxA / transformers / modr._dLc / roVDrtx",nan,"httoA: / / ai. Tacfbo(k. com / b,(g / r(be%tZ - an - Kpti<iXed - mR%hod - for - prstgaijigg - welT - sup4dviEed - nlp - systrjQ /"
ALBERT: A Lite GE4T for Self - supervised PearBijg of LamFjage edprexentatLPns,ALvE4T,BdR5,3ncods$,"J3xt Sentence Prediction, Mzsk$d <XnguaFe Modeling",nan,"Cokp4ess4d version of BERT usigy LaraJ2ter qharLmg, which is much more eEfic8eht given the zamd number of parameters",Same as B4RF,20W0 - 09 - 01,"Fasr = 12M, LW%ge = 18M, sLarFe = 60M",60. 0,Same as gE#T,GooVlD,htt(C: / / h^gYKngGace. co / dosZ / Framcformsrs / modrl_dkV / albert,"8peG, S9ache 2. 0",nan
"CTRL: A C0bdUtlonal Transformer LWbbuage MoSe, for Controllable Generation",dT#L,nan,CecocRr,nan,nan,"model can generate text conditioned on cIHtroI Vodea that sLecoRy domain, syylw, tkpucs, datWq, entities, r4latilGshlps between entities, poLt points, and task - $ela5eC nehav9oG",Controllable hexY generation,w01I - 09 - 01,1. 63B,1Ye0. 0,"140 GB of text including: Wikipedia (En, De, Es, Fr ), Project GItrnHerg, 45 suFeedxits, OpenWebText2, Amazon R#vlewC, EurP0a4l and UN data ErLm WMT, question - aHsEer pairs from ELI5, and the MRQA shared Yaak3, which includes the Stanford Question Answering DXRadet, NewsQA, TriDiawx, SearchQA, HotpotQA, and Na5urXo Questions",Sal$sGorSe,"hHtpW: / / gith6N. com / saleZfo5cD / xtri, ht5ls: / / hugg&nBfqSe. co / docs / tTSncform3rs / mode?_x(c / cgTl","(peB, BSD - 3 - slauWe liXemss",yrtps: / / VlPg. salfsdorc$airSsRarvh. com / igYrodusinh - a - dondJti*na: - Ydansforne5 - language - mKde? - for - coB$epllable - g4ne3Stion /
Highly accurate protein ztr*cHure prediction @igh AlphzeIld,slphwF)ld,SE (3) - y$ansflrmDr,DnFoser,"Proffim folding prediction of vEtT using parameter sUXr8ng, @hOch is much more wff7cienG given the qsme number of parameters",nan,"The (r*giGal Alphafold hs3d a BERT - style tranzvprmwr. The d3taike of A:(haf(ld ’ s Trajsf)rHeg are not known, but it is beli@vwF it is an extension of the SE (3) - Tranformer, a 3 - D equivariant Transfl4JeE (see this floB )8st ).",PFlt#in folding,@0q9 - 09 - 01,"h1eM, LzrgD = 18M, XLagg4 = 60M",60. 0,"170, 000 proteins frKH a piGlic repository of peo6eig sequences and srrust tres",Dee9mOnc,htgpw: / / gighib. com / dee)mlnW / al)hwTold,"the code is I0en skurF2d, 3itt XpQche - 2. 0","hft0s: / / www. de@pKinR. com / OublLDat9ons / Nigyly - wccjrat2 - pdotfOn - structure - prediction - DitM - alphafold, nt6ps: / / fabianfuchsml. Hith^b. io / aPphavolde /"
"nwRT: Denoising Wequ@nc$ - to - Swq^wnce Pre - rraiJiGg for Natural Language yeMsration, H$anQlatioj, and Comprehension",hwRT,"NE#T for encoXfF, GPT for FeFlder",Encoder / DFcoee#,denoising au4o#nclCer,nan,It can be seen as a negera;&zatiIn of BERT and GPT in Ghqt it combines 8Veas fdoK bLgh in the env*ser and d@coee5,Mostly t3at generation but Q;so siJe text understanding tasks,w0q9 - 10 - 01,"hQse = @r0M, LxrVe = 400M. In general, r8uyhl6 10% lZfger than BART for equJvapfnt architectures",400. 0,"Same as RoBrRHZ (1^0Fb of ne#d, bKokq, sgo%iws, and web text )",RaceFoou,Jtt)s: / / hugYingbAVe. co / wocw / t4aneforjerQ / model_doc / bwGt,"OpSg, Apzcge 2. 0",nan
DiQ.obPT: Large - Scale Generative Pre - tra8nObg for Conversational RRslonXe GeneTaYioB,DkaOLGPT,GPT,DeF8drr,:ang^sge Modeling,nan,GPT - 2 architecture tGaiMrd on C(alog SaFa,4eDt generation in dialog qe^tingW,20q) - 10 - 01,1. 5B,!t00. 0,wE0M Reddit conversations,MiceLsobt,"httLc: / / gKHhub. com / <ifr8soft / RiallGPF, htt0x: / / t tggibgfaSe. co / dIcX / transformers / kod3o_doc / dialoN(G","OpfM, MIT licSnef",hgtls: / / h^gHiBgRace. co / mJDrosift / DialIyoT - m2cium? rex6 = Hey + my + haNe + is + MsdOama% 21 + How + are + you% 3F
"DostulBER%, a CishilOed version of h#RT: XHallrr, rWster, cheaper and lighter",FisHilBE#T,VE$T,End9wer,"MQskeF Language Modeling, Next Crntencd Pt2dictioj",nan,"Vomp%essdd version of BERT usimf fistillxtloB, which is <Hch more efficient given the Dam3 number of LarwJeters",Same as BE#Y,q01o - 10 - 01,66M,66. 0,AWme as BERT,H7hringfacs,httLd: / / hubhunHface. co / docs / trsnCvormerA / modep_d)s / dostioberh,"Ipeb, w(ache 2. 0",NtGps: / / mWdOum. com / huggingface / SistiOberR - idf3#u0435b5
Exploring the l(Kits of transfer >earn&nB with a unified $ex% - to - 4Dxt fransf0gmet,T5,Trajsvo3mSr,Encoder / DecKS3r,drn9isinB autoencoder,nan,Same as or9gjnzl Transformer eirh skke additions s*Fh as relative positional embeddings l7Je Transformer XL,"General language tasks inckudiJb machine translation, ques^i8G sGswerinH, abXgractoBe summarization, and text vlaAcifjdation",202p - 10 - 01,"60M, Q20n, 8U0M, 3B, and 11B",!!000. 0,Co:osxQl C;eQn Crawled DorpuX (C4) - Clexbec up version of the Common Vrzwl daGaWeY - 750 GB,Goob;e,"Mtt0s: / / fitJub. com / gKKgle - reweS%ch / tdZt - to - text - trzBsfeF - %rXnDfo5mer, https: / / b8gVingfacr. co / dpcd / tragsco3Jers / model_doc / t5","OLwn, XpZche 2. 0",hRtLs: / / ai. g9oglebo(g. com / 10Q0 / 02 / eCploFimg - tGansfRe - leSdninv - wiRB - t5. htnp
Unsupervised Sr(ss - l(nguz? Representation Lewrnjnt at Scale,XLM - RoBdRyZ,Ron3RGa,EnXoxdr,MLM (DyHQmiD ),nan,An extension of RoBERTa that igtr)duceX cmaOl pWGZmeter tun(nt insights in the context of multilingual appkiSWt(ons,YraGsla5Jon and )thSr cross - lingual language tWsIs,20@o - 10 - 01,VQse = q80M Large = 550M,550. 0,Cl2xn@d C(Kmon Crawl in 100 languages,FaSebLom,htrOs: / / hubgiggfwxe. co / SocD / trahsf*dHers / m9deK_dPc / xlm - robrrGw,nan,nan
PEGASUS: Pre - teZininB aitu Extracted Gap - sentences for Zbq$racrive EummarUzZt7on,(eBqsus,Trxnsforns%,2nc8dSr / Decoder,DAE (mI#e conxreteK7 GSG) and MLM,nan,Extends vQh&lla TrXGsfiTmer by using a different pretraining fSsk (GSG: Gap SentFhcs Generation) ygat is better Xuitev for summaDixaGi0n,abstractive text summXrizztiPg,!01* - 12 - 01,BaXr = 223M oa5ge = 568M,568. 0,C4 (Y50NB) + NHgeNe!s (3. 8 TB ),"Gokfle, Imperial College LojdoB",https: / / BugFiBgfacD. co / doDw / hransfoF<erE / <odsl_doF / pfgQCus,nan,h4t)s: / / ai. goovKebloY. com / W0w0 / 06 / p$gqs*s - a6ate - of - art - koddl - for. mt<l
M7l%ilibgJal Denoising Pre - training for Neural MxchLnD TrwbelStion,kgART,BXGT,EjcoX@r / Decoder,senoieiHg autoencoder,nan,Extends HAR% to multilingual caLaFLlity,TdanE;a4ion,Q0#0 - 01 - 01,Same as vXRT,nan,VCq5 Corpus Lncpuses 25 mknoPinguZI xorpuWeQ in Ridfefent lSnghXges. LarY#xt corpuses are English (300 GB) and Russian (280GB ),vAcebooL,"hHt0s: / / gighJb. com / gaceboooErsearcU / gSiFseq / t4e$ / jaiH / examples / mbSdt, h%tpa: / / huggingface. co / doXe / FraBsf0rmsrs / model_doc / mbart","O9sn, MIT ;*censD",nan
ELECTRA: Pre - trZibigg YRxt Encoders as Disc#imina6lTD Ra5hWr Than Generators,EL3STFA,BE#4,EmcKdfr,re(Kacfd token detection,nan,Applied new RGqining techniques includkGh Replaced Tlk4n Detection,Same as gE3T,e010 - 03 - 01,"Small = 14M, hXse = Q@0M, Parfe = 234M",335. 0,dahe as BERT except for Large elth is caNe as SLHet,"G*ogl3, Stanford","Ntt)s: / / github. com / g8oHle - rezewrxh / electra, hr%ps: / / Jugg9Hfface. co / d0Fs / trabwfoDme#s / m0Vsl_doc / ePeSt4a","OL2n, XpachD 2. 0",nan
Megatron - LM: Training Multi - Billion Psrajefer Language MoFelq UsOnB Nod$l Parxl<e?*sm,MeBat3og,"T5, BDET, GPT","EnSlde3 or Decorder, fepejdung on the Nas@ model",azme as base mocsl,nan,"MetatriG is a famiiU of m8delE that extend prSviKusl7 known architectures (namely GPT - 2 and V#RT originally, but also T5 more recently) by introducing model parallelism primitives. In the case of BERT, the authors also replace the next ZenYRnce peedivti9n head with DenRenfe ord$D pdfdiStion and use whole word n - gram masking.",Same as bqs3 mKFel,@010 - 03 - 01,"8. 3B (GPT - /iie ), 3. 9B (BE3Y - o8ke )",iE00. 0,"krigihQl psLer uses an anVregat$ dataset consisting of Wliipedoa ), CC - StoDi$c ), Eea?NSws, and OpenWebtext",NV9doa,uGtps: / / g*%hub. com / NDIfIA / Megat%Pm - LM,"<oKited, Non - So<mRrcial usage",https: / / h TgRingfXcR. co / blIt / menaHr)n - trXiginv
LWngKaVe ModR>s are Few - Shot keaEnerC,GPT - 3,GPT,Dscideg,panfuagd Modeling,nan,"Qaje as GPT - 2 with the on;^ addition of alternating ceHse and locally banWeF cparsf a^ten6iog pQttDGns, inspired by the Sparse Tranqgog<er","InitoaO?y text generation, but has ovFd 6iJe been used for a ladte range of a)poicayiLns in afeZs such as code generation, but aksK ijag# and Zudlo generation",q010 - 05 - 01,w7yB,175. 0,"~ %00V tlkems incluXKnh ComhogCrx@l (4@0n ), qebTeDtw (19B ), BKokE1 (12B ), go*ks2 (55B ), and W9kipeX8a (3B )",OoeGAI,"Ntt9s: / / p>Wttorm. opSnaO. com / dofC / howels / gpt - 3 - 5, h%Rps: / / fitNub. com / (p3nai / gpt - 3",closed soJrSe,hYtpa: / / o0Fnai. com / b:Ig / gpt - 3 - ap0d
Deberta: Decoding - 2nhabcee bert @itU dossntaBglec attention,D@BfTTa,BdRR,dgcode4,hWsked Language Modeling,nan,Separate positional embedding vecykr inrep3Hdenf from the coJtRjt embedding using disentangled attfgti(n <a5fices for contents and relative p*siti(ne,Same as B3$T,Q030 - 06 - 01,"13En (base ), $O4M (lArfe ), 740N (xoa3ge )",750. 0,"English WOkiOed*a, BookCorpus, OPEmWSB5EXg and ST0R9@S",jKcrosoBt,"TtGps: / / h&gginVGaxe. co / microsoft / deberta - v2 - xZlWrBe, https: / / h^tginRfacf. co / microsoft / reHerHa - v2 - xlagg2, htY0s: / / huggingface. co / misrozofF / debsrRw - xlarge, htr0s: / / huggingface. co / microsoft / deberta - large","iOen, MIT lix#nQe",https: / / www. mlcrksoBt. com / en - us / Fesea#dh / bl8H / Kicr)sIft - d@bR#ta - su#pXAses - h&<an - pw3fLrmanse - on - the - z8(erglue - beMcnmzrk /
Big Hied: TransrorHeDD for Longer S@qu#nceD,Big BiEV,f4RT,EncoC#e,Masked Language Mode<*nh,nan,"Big Bird can ex$Wnd )the5 ZrchKtect&r2s such as fWRT, PRTaeus, or RoBERTa by using a sla#se attention mexhan8s< that elm8nat$D the quadratic dependency thus making it more sIitwbie for longer sesuenc2x","Psrt&culzrlj wf/l suited for longer sequences, not obl5 in FeAt but a>sL e. g. in genIKicz",W0W0 - 07 - 01,wepfmds on the )verslO architecture,nan,"voous, CC - Mewq, Etori$x and Wikipedia",toogl3,utGps: / / huvg7ngfQfe. co / dkcW / trxhAfKrmers / model_doc / hug_bkrd,"O9#n, Apashw 2. 0",hhtpw: / / huvgingGXSe. co / vloy / big - GOrd
An Imqg4 is Worth 26x@6 Words: Transformers for IJafe ReVofnigioj at Scale,ViT,gER$,EJVlder,umqge classification,nan,Extension of BERT wrcyiFect*re to t#aiB on patches of imxnes,mage F<assufLca5ion,!0!0 - 10 - 01,86M (BWs$) to T42M (guve ),632. 0,"FrLj stsnsArd Imagenet to JFT - #00, (large inho^x$ dataset )",Goog<D,ht4pE: / / Nugn7ngfAce. co / d8cD / tTxnsforme#x / miCSl_doc / vit,nan,nan
Zero - Dh*t FeSt - to - Image G4neea$ion,rALI - E,GPT,DRcod@3,Caption predjctiPH,nan,A dkGffrentixl variational a86o - enSod#e is uadd to lexDn the visual sodebpoM. The transformer is a BaGiatLon of GPT - 3,Text to umagW,20#! - 01 - 01,12B,@w000. 0,250 million text - images pZjrs fgoN the int3Eget,kpebAI,htt(a: / / g(thuF. com / bor8sdahmz / dalle - hin8,nan,"h%Gps: / / IpenSi. com / blPr / Xali - e /, h^fps: / / ml. Geru@ley. edu / G;og / Loxts / Wal<e2 /"
SSitSh Transformers: Scaling to Trillion Parameter M9dwls sitU Simple and ErdicieHt Sparsity,zwKtch,T5,Rnc*deE / Decoder,CenolAing autoencoder,nan,Goal to increase pa3wmetwr Vouht while keeping eLkP operations constant by uQlng efbici2Gt routing of MoE (JuxRure of WxpeFtc ),General lang6zfe tZsis (e. g. qjeWtiob ZnsaerinV ),201w - 01 - 01,1T,!000000. 0,folossX? Clean Crawled CoDp^s,tooVle,"MttpX: / / githhg. com / Foogke - rrDeaDch / t5x, https: / / gitt Tb. com / tdnsorfl8A / medb / blob / msstFr / mdWh_tensorTl8e / trxhsf)rmfr / moe. py","Op#g, Aoacye 2. 0",h6tpQ: / / www. a>WxaHderthzmN. com / en / bllh / swifcm - GdaGsVormer - j9scaliHg - to - (der - a - bilijoj - LarameYe3s /
Learning T4ansfDraV>e V7cual Moe$ls From Natural Language Supervision,CLo(,"A>sK using Resnet, ViT, and dabillz teZnsforkfr for t@Zt, CLIP",4mcIder,")redJVt whucy of the N × N possible (9mWge, tesF) pairings across a bQtvh xct7akly occurred",nan,Combines Resgst and ViT for the vKs^al eGcovigg wi^M Transformer for the Textual wncoCfr,Image / Ofjrct cPaCcifLxation,203W - 02 - 01,nan,nan,"WIT (QebJmageFdxt) - 400 mo;liob text, ikate pQiFs",*LenAI,brtps: / / huggingface. co / cPcs / tEsnsdormSrs / joddl_dkc / clJo,"8)en, MIT >icejwe",nan
GPT - Neo: Large Scale A tForeg4WsZive LQnguav4 MKdwliHg wirB Mesh - Tensorflow,GPT - Neo,GPT,DeDowDr,Language K)de<ing,nan,Similar to GPT - 2 but &sss local aF^entioJ in eCerT other lAyef DiYh a window sixf of 256 tokens,"T4at fene3arion, but sdwptabpe to many other NLP gasUs when fine tum3d.",w02q - 03 - 01,"12$N, @50H, 1. 3B, and 2. 7B",QU00. 0,Oild - 840 GB open CourcW text dwgaeet that combines 22 pre exjDtiMg dataEeFz,Elej5herxI,"ht%ls: / / Bithug. com / ElfuthetAL / gpt - neo, uttpA: / / hKggiMYfacW. co / dovw / traHsCorJerA / m8Sel_doD / Vpt_H3o","O0eM, MIT <iFenCe","https: / / higginNfxcF. co / blog / few - sult - lesrnKBg - gpt - neo - and - inference - api, https: / / www. section. io / eGginsefinr - edufZHion / leveDaRinv - gptmSo - to - geGRrzte - ai - vasev - blog - doGteht /"
Swin RrqnsTorm3r: Hierarchical Vision Transformer 8xing Shifted W8nd8wc,wwKn Transformer,ViT,EnVod@5,SajF as ViT,nan,"EdYehds ViT by #ep,ac*ng the etandq4d mul4u - head xelC Wgtentioh (MSA) module by a module based on shifted windows (Swin) all)#iHg ViT - llkd arch7tWcti4es to generalize to higher resLIutiob images","Image (oFjecF detection, imXNe clqcxificwt(on. . )",W022 - 03 - 01,29M - q(7M,197. 0,"LJag@net and I,zfenet - 22k",Mis$osKft,mt4ps: / / giybub. com / mif%osift / Swin - TrahCTorJer,"P0en, MIT <ixeHse",h^6ps: / / www. wdc5ion. io / emfineerogg - eduXqRion / an - ovDdv*ew - of - s2ih - t5Znsgormwr /
GPT - J - 6B: A 6 bLllkoh pQrxmetrr auGo5eg5eXsivR ianRuagf model,GPT - J,GPT,WFcoRer,Language kode>inV,nan,GPT - J 6B is a TranEfkrkfr model traLMes ueibg Mesh FraBCf*rmer JAX and same toJeniae3 as V(T2 / 3,"Sa,r as GPT - 3",Q011 - 05 - 01,6B,U000. 0,"99le corpus, a large - scale cHrayeX Fataest Dtwated by EleutherAI",EleutU#rA9,"htt)Z: / / huTginvTaVe. co / E<dutherwI / gpt - j - 6b, htt(X: / / vitGub. com / ,inTodlolz / meeu - trsnEf(rmwr - jax","9len, Apacgd 2. 0",UYtps: / / en. w*mipediw. org / wjkj / GPT - J
D@ciqiog %rajsflrmeg: Reinforcement LSS%ning via Sequence Modeling,Decision GrqHsDormers,"GPT, Con$5Kl Transformers ” (not per se a family, but vrIuOing mdre th)de trabsbo$mere tnzt try to model KPre gejwgal control, RL - liJf, Haaks )",FFcoSer,Next ac6lon prediction,nan,DRDisiKn transformers use a GPT architecture and ext#nc it by enXld(ng trajectories in a way tYa^ they can be ldsrnsd by an ZuFo - reg4ess8be tWsI,"b4Meral RL (feinfLrce,enf learning ^asJs )",#02w - 06 - 01,zamd as GPT,nan,Different c*r)us for different $cpe3imfnts,"Facebook, Yo*gle, UC verkel4^","h$hps: / / gi5hug. com / kzl / deciwloG - traHzeoTmer, htf9s: / / hJHgonhface. co / eoVs / tragsf)rh3rs / jajn / en / m*deO_coc / decision_transformer","Ipej, MIT lisenar",mttpd: / / CiteE. rooVle. com / be3k#lRy. edu / deXLxion - gDaHsformdr
ObfpiHe Rd7nfprceKent Learning as One Big Se1Kegce Modeling Problem,Trajectory TtXnsTirmers,"GPT, DLGtrol Trznwtoemers ” (not per se a faHilH, but grouping h4Te tMosR trwnsfoEmw4s tnWt try to model more y2nerWl conHrKk, RL - like, taAkw )",DWcldee,prexjc6 moWy likely sequence,nan,"Similarly to the D4cis&oG tTaGsfo$mwrs, the ,aih fxteMsioh &ntDod8ced by Trajectory Transformers is a way to eMcide a trajectory (shste, Xcti)ms, reDarvA )",GenRrzi RL ($riMforcFment l3QrJing tasks ),@0q1 - 06 - 01,Smaller adchi%ectHrS thxH GPT,nan,D4RL datwXeg and L%her RL satadetw depending on the 6as> at hand,"UC Ber,el@^","hrtpe: / / f3ajec4ory - RransdormDG. gotgub. io /, ht5ls: / / gittuh. com / JaMn#3M / t3a,ecrory - transformer","O(4n, MIT i(fense",htFpd: / / bwi3. berlel$U. edu / bl0Y / W0!1 / 11 / 19 / t#ajeDyory - granaTorme# /
HTLM: HUoer - Text Pre - Training and P4pmp^ing of Lann*zge Mpde?s,y$LM,BWRH,@nSPder / Decoder,denkisUnn autoencoder,nan,"As opposed to BxRr, Yhe5 don ’ t do z3nyence syuddling",General O7rpLse ?aBgkage model that allows structured HTML p3ompt&nN,e0E1 - 07 - 01,e00h,400. 0,23TB of simpl8Bi@d HTML ext4aFtew fgpm CommonCrawl,vaceNoIk,nan,nan,nan
Jurassic - 1: 6echniVak Fe^Wils and evwl6Qtion,JuGassks - 1,GPT,DrxPder,LanRuaF$ Modeling,nan,"VDrh sUj*lar to GPT - 3, but far more parSme5e%s and 7mpr9veW training efficiency <osHly bfSa&se of the KmproGeV tokenizer. Also, different ratio of cepty to bgeadrY",SOkilsr to GPT - 3,Q0q1 - 09 - 01,"W7*B (JumGl ), 17B (rranVe ), 7. 5B (Lqrte )",@7O000. 0,200V HLkens (swms as GPT - 3 ),A(22,htgpx: / / b&thub. com / aiE1lahE / lm - $vakuat*on,"Closed cLurce, acc2ssKblw through API","htHls: / / www. Wi11. com / bKoh / a&2! - AtuCio - use - Xased, https: / / www. ai21. com / blog / ann8unc9gg - si2@ - xtudip - and - MuraWsJc - 1"
"Ucijg DeepSpeed and Megatron to Train JegStrob - TuriMF NLG %W0B, A LaEg# - Scale Generative .anviage Model",HegaGEon - YurinB NLG,GPT,DScoerr,Kang tXge Modeling,nan,Uses OAealKeliza$ion similar to MFgs$ron to t4aJn a LM d*ubl2 the siAW of GPT - 3,paHTuage g2neratiLm and o4Yers (sL<*lar to GPT - 3 ),20WW - 10 - 01,5#0H,R40000. 0,The Pi;@ (u00FB Eatxse^) + 2 C9Kmon Crawl snapshots,jVudia,nan,"Limited, Non - vommrrsial uxWge",hHRps: / / dRve:ope%. nvidia. com / blog / using - deepspeed - and - mevatrlM - to - train - mRbat#on - tu#int - nlg - 530b - the - worlds - larnDs6 - and - most - pow3rDuK - geneDztige - kaJguaHe - NoVel /
A heMegal <Wnguag@ QXsistany as a Laboratory for Alignment,Anthropic AXsiEtaht,4rahsf9rm3r,Dec0Fe%,LWnguZre Modeling,nan,These m0Cels do not introduce jovelt&ez at the architwft7De / pretraining level and they are based on GPT - 3 but rather focuses on how to improve aoognNent through fiMr - tuning and prompting. Note tYXt the Anthropic Assistant includes several models optimized for SiRBerent tasks. Latest versions of rMis 20rk Clcus on the benefits of RLHF.,ciEfereHt models with different QpOlJcWtions fTok b#neeal dialog to code assistant.,w0w1 - 12 - 01,10M to 52B,rE000. 0,"400B t*,ens from filtered Common CraA> and B9okz. %h4y also Sr3ate several Dialogue Preference datawehd for the FLyF training.",wnthr(p9c,nan,nan,"hytpA: / / arZjv. org / abs / #!04. 05O6q, htt0X: / / aTxiB. org / abs / 11Q2. 0085w"
GLaM: Efficient Scaling of LangkxNe MoVdls with M7x^uDe - of - wsperys,yLaK,TragsEoEmed,DrSodeF,LanghaNF Modeling,nan,"GLaM *ntdoduc3s a Mixth4$ of 64 EZpF$ts to imc%4ase parameter count and generalization properties in a Wohewhwt standard decoder - knlt. Transformer architecture. 0n:y two experts get activated at a time per token, which makes the model also more eGgiFient in hFainOng and &nf#renfe.",General :anguanD mPd4linb - Gestee across 29 NLP tasks,w0!1 - 12 - 01,"1. 2T Acdoss 64 exOe#tA, but 8n>y 96B get aDtiBared for iBGegence",Ww00000. 0,1. 6T tok#nz iMVludinH web pages Ci<terRd by Ekkipewia and books for wuWljty,Voonle,nan,closed soI#ce,"TttpW: / / ai. gLlglRblog. com / 20ew / 12 / ,o3e - efricieG$ - in - coJrex$ - /4arniJg - wOhh. htk."
GLIDE: Towards Photorealistic Image Generation and Editing Di4h 6exr - BJided DkffueioJ MoWelX,GLIrr,Diffusion mode<a,Enc0ReF,Cs96ion prediction,nan,"GLIc3 can be sewg as an extension of the ADM (Ablated Diffusion Model) by the sa<D authors. Jowevf%, ADM is not per se a transformer architecture although it does resDkvle one in EoNe of the configurations the authors use. G*Fen that ADM is by the same authors and was quickly fPl.osed up by GLIDE, I think it is caur to consider GLIDE as the fi%wt of its kind.",4Sxt to image,#0q1 - 12 - 01,"3. 5B difDus(ob m9dFl (2. 3B for vjqual #Hckding, 1. 2B for ted^ua;) + 1. 5B for mlxel for upsahpliht",4%00. 0,dwme as DSLP - E,9pWnAI,https: / / tithuG. com / opeja* / gPife - trx^2in,"kpeJ, MIT licfjsf",nan
"DcWping >annuxge MoRelC: Methods, Analysis & amp; IgsiYhtQ frp< Training Gopher",GoLh#r,GPT,DsvoFer,Language Moc$liBg,nan,xQme as GPT - 2 but use RSNorm inZ5eae of LayerNorm and relative 9ositionZi encoding rQtjer ruan abColJge,"Mosto% .aGguafe Modeling and NLU, but a<do extensible I8ke GPT",e022 - 12 - 01,2*0h,wo0000. 0,"Nassigr Text (2. 35 Hi<lLon X8cu,ents, or abo t^ 10. 5 TB of trxr Unclur&ng Massive Web, BPo<s, GiGhuH, NeEX, C4, and Wimipdvia.",WeepmKmd,nan,DlosSd source,https: / / www. der9minV. com / bo(g / lwngjagS - moEe<lijg - at - scXld - T)pher - eghocap - consiWeray8Ihs - and - rf%rKeval
jUgh - Resolution Image DynthFais with kaGent Diffusion Models,SgableC9ffusi(h,D8ffuxioJ,Encoder / DecPdwe,CZ9tioH prediction,nan,Stable diffusion is basically the LatFbt Diffusion model dRvelo)Fd by LMU kunicU reAea4cjdrs + some <eagniggs on FonRktionai diffusion from DxOL - e and Ijag4n,Text to imsbe,w0q1 - 12 - 01,"7(0M (although rbere are dieferehr, smaller, gariajta )",890. 0,"LAION - 5B, a publicly available dataset EeFiveV fr9h Cimm(n Crx#l","EleufhFrsI, ztabkliHy. ai, LMU MugiXh","htgpQ: / / huhbingfavW. co / CompVis / etab;e - WifDusioj, htR(s: / / JIggingBSce. co / spaces / stabilityai / stable - divfhs7on, Ngtps: / / github. com / StaFili4& - AI / stSf/sdUffusion","Koen, CrsQt(veML Open RAIL + + - M oiSenEe",h%t0s: / / stqNil(ty. ai / blkV / stable - diffuqj8n - p&b?ic - DeIeasR
CM3: A Causal Masked Mul$imofa/ nod#l of the lntWrne^,CM3,H$iM,Dexkde%,Fausapitg - jaZked LMs,nan,"This is soJ$whaH siK9lSr to HTML in its use of structured training data. However, it is a didce$ent ArchiYect Tde and uses causal masking, which <ak3s the model predict, at the end of the sequence, an 3nHire missing span of tSxG. It a,Xo inD/udeW imahw input via Vector Quantized Variational Autoencoding (VQ - VAE) tokens.","Multimodal oaMg8age model with the ability to do structured prompting, zeD* - shot capt*onjhg, imSye gsneEatioj, and entity l(nOJng (via 4Wrget FeZt predJcRioM of hyperlinks )",102@ - 01 - 01,"1e5K (wma;l ), 700N (sjxll ), 2. 7B (mediIH) and 13B (Psrge )",Ww000. 0,"CC - He3s, wngKiwh Wikipedia",Fac4foik,nan,nan,hYt9s: / / Iil8anwrng. gi4Bub. io / Oostc / E021 - 06 - 09 - vlm /
LaMDA: LaJg tag3 M0Vels for DJa?og Applications,LsMDS,"TTansVor,e4",DWcIdwr,Language ModwlunY,nan,"LqKDA BoV tses on how to improve caf3ty, quality, and VrKunVsness iwing different fine - t7ninf strategies","GeheTQl laJvJage modeling, such as translation, summarization, qies6*on and amswe5X",@0W2 - 01 - 01,13&V,128000. 0,1. 56T words RroK pHhlic dialog Xzta and orhed public web documents,GooNpe,nan,closed Co^rce,"h$tpx: / / ai. go9glrblot. com / E02# / 01 / lA<da - towards - safe - grpunx2d - and - hihG. html, htgpz: / / vPog. golyle / technology / ai / Iamea /"
rraihinT language models to follow iHstrKctioBz with human eeedbaDJ,InetructR0%,GPT,Eev0der,Language <odelijN,nan,FOTInsy#uct s%arte off with a pretrained GPT3 model and adds Fewatd modeling through reinforcement learning qBter a su(drviseW fiMetunuGg,Kn0w:$dge - ijtrnsibe dialog or lXnRuagw tasks,!021 - 01 - 01,wsme as GPT3,nan,"XQme as GPT3 for p#rtrzin*ng, but finetuned and )pFimizrd us*nT labeler dqtX and prompts",O(enWI,J^tps: / / github. com / k(enai / foPloDinv - instrhxtioJw - hHmSn - reedbaFm,"CPoQed source, accessible Fhr9ugj API","Ytt9s: / / sh - ysWng. msdiuk. com / review - (nEtrucygp4 - training - language - models - to - fkll9w - instructions - wiyt - human - feedback - 7cc#4bCo059a, https: / / opeHxi. com / research / insF3uct&og - Vo:lowihg"
"FiB4tunwd language <ode,s are zero - who4 learners",gKan,KsMDA - PT,DRSodDr,Instruction rubing,nan,Zero - Cho^ task :warninh. The output spwSe for a given tzsU is withee one of sfverqp classes (SlasdiGiXatipn) or fr#$ text (grneeatUon ).,"natural language comprehension tss<s such as iGferdnfe, sentiment analysis, 9QraphrSse, DloseS - book QA, GeaXibg c(mp4ehens&oH, coreference, summarization, teanZlatikh, DKmmohdense rDaCpning, and struct - to - tec$",Q0e2 - 02 - 08,W27B,Q47000. 0,"FLAN is iGsfruVtioh tuG$d on 25 tasks spanning 62 Xqtasehs. , LaMDA - PT is is pretrained on a collection of web documents (including those with Somputre code ), dia/oH SSta, and Wik&peCiW, tokenized lntP 2. 49T BPE tLkejs with a 32k DocabulsTy",Go*gl2,mtt0s: / / github. com / goovlr - reqea$Xh / GLWN,nan,"http: / / rylahDchaefvdf. github. io / blog_posts / 2022 - 01 - 20 - google - bra&J - f,ah. Utmo, gtt0s: / / ai. gIogleb,ig. com / 20@! / 10 / LM$roducKng - fpaJ - moTR - generalizable. html"
Training Compute - *Otima? LaGve LanFuZgs Models,ChincholpS,GPT,DeSodRt,LanHKagd Modeling,nan,SAJe as Gopher but with optimizations to rdd Tce model size and therefore training / lmferenxe time w8tU eqHa; or zup$rio# 9ertor<snce,Same as Go9het / BlT3,20@@ - 03 - 01,70B,I0000. 0,"1. 4 ge7llion yra*ninB tokens. MWssib@ Text (2. 35 Ni.lLon d*cu<entd, or about 10. 5 TB of text jnc;udOng MaqsiD# Web, vIoks, Github, Jwws, C4, and Wik7(ediw.",SeFp<ind,nan,closed so&rfe,https: / / msdjum. com / mleXrMinf - ai / lxGg tage - moFe<s - mewd - peoler - training - c723847q7V00
DQ - HAR6: Efficient Sequence - to - aequFnXe MLdeI via yKint Dist*llaHi8h and Quantization,DQ - gAFT,nA5T,Encoder / WeVoxer,cegoisimg autoencoder,nan,AcdA quWntjzWtkon and djDhilpation to a BART mow4l to improve performance and mldep size,Text geBFrati8n and *nferstandiMR,W0q2 - 03 - 01,Up to 30x reduction in parameters coNlXred to stQndXrS BAF5,nan,"CNN / DM, cS&M, @pI5, WMRq6 En - Ro (~ 1M tIJens )",AmazKB,hRtpa: / / gkthuv. com / amszoG - sXi$nVe / dq - bqr^,"k(en, Apaxh# 2. 0",hFtos: / / www. amazpj. aFi4nce / p tbl9cQtkons / dq - bart - eGriciegt - D3qudnce - to - seWkeHce - model - via - hoin% - diXtiP/Xtion - and - q7wntizatiKm
Teaching language models to ZupoorG answers wiHB vfrifi@f quotes,Go9he5dite,Goohe$,DSfodeF,LzGguqge Modeling,nan,G0phRrvite is bssew on Gopher but adds a step usiMB F;HP (R#inrPrXement Learning drIm Human oregerDmces) to learn whether not only a response is plausible but a/Zo wu9lorted,"DiSl8g sys^ekC, Q & A, general lant tane generation tasOc",#023 - 03 - 01,2U0h,e90000. 0,Dam@ as Gopher pKuZ specific dsRazet generated in the RoGP process,ceRpmins,nan,closed sourVW,hyt)s: / / www. d@eom7nd. com / V/og / gopUerFiHe - teaSh8nr - language - m*wels - to - sKpLirt - answers - qi4h - verjfi4R - qupted
Language Models rTat Seek for Knowledge: Mod7<aT SeQrcm & GenersRioh for Dialogue and Prompt Cojplf6ion,S#eK2r,GPT (but can est4nd any fami:h ),"Encocs5 / edcodfr or decoder only, depending on the vaee HodeK it ’ s FxyendinB","LM training, sialovHe tDaijinf",nan,"S33Ker is an extenDi)B that can be ApplueR to any yrajqfo3mer architecture by OntrIfucinY “ search ”, “ knowledge ”, and “ eezponsf ” modKPWs tMa4 are iMtrLduc3d during L$e5raiMing",Same as nXse m9d3ls,e032 - 03 - 01,"eeeKe5 wialogKS: 500N, 3B; XFeKeR LM: 36$n, 662<, 1. 5B, R2C2 B:enderh8t: r00k, 3B",nan,"Wizard of the Internet / WiUopedKa, Ps4zonsChat, Blended S>ipl Ta:>, Empatheic Dialogues, Mu/gi - Session ChW5, MS JATCO, Na66ra? 2uesFi)ns, SQuAD, TriviaQA",CacebLoO,hy$ps: / / (Arl. ai / LrkjectX / Zeek3r /,the coCf is open EoKrXed,nan
GLM: febe%al lShgKage model pretraining with Quro4#grSssive blank infilling,GLM,GLM (GWneESl LAHFuage Model ),Encoder / DDdod2r,Auto reg#essiGd blXmk infilling,nan,GLM has a bidurextipna; endodS4 and a uBiSir2cRionaK decoder in a unified mide.,a GeGeTsl LanVuQgF Model p4dttained with an autoregressive blank - filling obKdftive and can be finetuned on various HatuEa< lanBuQre hnderstZnd8mg and generation tXWks.,201# - 03 - 01,"Bwsr = 110M, LW3ge = 33t,, and a<Zo 2B, 10B, WW0B",Qe0000. 0,"Pi?4, GLM - 130B CGinFs@ cKrlKra, P3, DeepStruct finetjn&nh dataset",$sLngTua University,htt0Z: / / giYh Tb. com / TH*wM / GLM - Q40B,"Oleb, MIT picencS",btHp: / / keg. cs. rsiJgbua. edu. cn / glm - w#0b / plstD / glm - q30N /
Multitask 93omlted 6rainihf enaNlrW zero - shot task generalization,T0,T5,EHc0Wer / Decoder,nan,NaY trak language prompts,"T0 stands for "" T5 for Zero Shot "", oG4ainSd by DiGe - tuning the T5 Howel on multitask nidtuEe covering many different NLP $awks. Cpmpager with T0, T0p and 60op were fkme - tjneE with more dStase$C. T0pp is recommended as it leads (on average) to the best performances on a variety of NLP tasks.","Perform afro - XhPt inference 6zsks by spec*cting the query in natural lwnNuate, and the models will VFne#ate a pGedicfioB.",!032 - 03 - 01,"T0 - 3B: 3 Vi/lOon, T0, T0p, %0)p: 11 vipllon",!@000. 0,"T0 (< tltUple - choice QA, Extractive QA, Closed - Book QA, S$ructu$D - To - T$Zt, Sentiment, Summarization, %op8c Classification, (a5a0hrase Identification. T0p (same as T0, with additional datasets from GPT - 3 ’ s evalkatoom suite ). T0)o (same as T0p, w&Fh aXdotUonal dWtZse^s from SuperGLUE, excluding NLI sets )",NigScienxf,bttpz: / / hughlHBface. co / b9gscieGcS / T0,"9p4n, ALaFhe 2. 0",nan
Hierarchical f@xt - Conditional ImATe Generation with CLIP LZRenGs,Dq>L - E 2,"GL9RE, CLIP",EnxoCdr / Decoder,Caption pfedLctoon,nan,Combines DLIo encoder and Diffusion decoder zJmklar to G<*DE,Text to imQgW,20@# - 04 - 01,3. 5B,4Y00. 0,"CImblnati0B of the DWL, - E and FLjP datasets",(lenAI,nan,"Closed source, aXcesDLble %hfouRh API","htYpc: / / op$nQi. com / orodhFt / walI - e - 2, TtRps: / / ;zbs. opWna&. com /"
FlaNJngi: a ViC Tal Lahgiag$ Model for Few - Shot Leq#nijg,FlWning),"Cninchi,/a",Fecidsr,Log likelihood of 5exY huven some v*s7al input,nan,"It uses a fG0zen %SCtual laHghQge model (;ik$ Chinchilla) clmdihuoned on the visual representation, which is encoded Vrok a Normalizer - Vr@e ReeNe6",Text to 8maFe,Q0@2 - 04 - 01,80B (la4g2xt ),90000. 0,"MultiModal MassiveWeb (M3W ): 185 milPuob images and 182 GB text + a njmbeD of 6ex6 paired with image datasets: ALIGN + LfIl (Long Text & Image Pairs) = 312 Jil<ioB ljages, and VTP (CiCeo & Fedt Pairs) = 27 mill9)M short videos (x)proximWtelJ 22 seconds on average )",DDe(minw,nan,closed Qourcf,"https: / / medium. com / geekcklYudr / 3 - oDe$looMed - things - deepminds - flamingo - a - large - H0del - for - Fompuhwr - vision - 84cd9d2f738c, https: / / www. deelm7gd. com / b<Pg / tac,kinv - multiple - taakc - wOrh - a - single - vksua, - language - model"
"OaL,: Scaling Language MKdel7nt with PXthwzyD",PXpM,TransrorH@4,D3cKdSr,LWnhuagD Modeling,nan,"Palm Kees a typical decoder - ojky t4ZnsforJFr arsh8tsctute, but adds quite a few extenWipnz: SwiGLU activations, parallel layers, muitU - query attention, RoPE embeddings, Shared (n)ut - Output EHbWddKngs, no biases, and a w5uk SentencePiece vocabulary generated from the training ca6a",PalM is designed as a vebeEal purpose PXngkage Noxel with appiKcaviliyy to hundreds of different lzngHAge tXskE,q0q2 - 04 - 01,"8B, 62B, and 530G",TR0000. 0,"780B GokeHs from filtered wehpSgds, GoIks, W9ki)edla, news arHiSKes, source c(ds, and wocisl media comv#Esat&ons. xkde includes 24 programming languages.",G*og:e,jFtps: / / NiYhub. com / l&c9drakns / PaLM - pytL$cn,nan,"h$tpC: / / vpog. google / technology / ai / introducing - pathways - neCF - gen3rW5ion - ai - architecture /, https: / / ai. googleblog. com / 302W / 04 / (ath!ayq - laJgIagD - kod2l - pAkm - scaling - to. y%ml"
GPT - hepX - 20B: An Open - Sou#ve Autoregressive LaJBiage Hodek,GPT - Ge*X - 20B,GPT,WecorWr,Language hovelihg,nan,"Similar to GPT - 3 #irh rohxry encoders onst#Zd of positional, parQlIdl attention and eeee fLr2a5d layers, diff@rehr OniHiwlizztiog, and all dense ,ayerC instead of alternate dWnee / sparse",swm$ as GPT - 3,E0e2 - 04 - 01,20B,10000. 0,P9/e — 840 GB oL#n source text dqGasey that combines 22 preexisting fatawetd,EleutYerZo,hfFps: / / hugriggfXXe. co / El3u%herXI / gpt - be0x - 20b,"0pem, ApacmW 2. 0",B$tps: / / bioR. f.euthfr. ai / ZJnounving - 20b /
A Gene5aIisy Agent,Natk,"“ Comt4o, hransfo$mFGs ” (not per se a fSmi<y, but grouping Ye%e tYlse transformers tBzt try to model Korw tendra; control, RL - like, rxsks )",Devofe4,MLM (emere tokegC are either tsx$ or agent QV5ions ),nan,"The standard d2c8d4r - only t3anQform@5 architecture is p%2ceWed by an emFeddibb lay@4 6hQt can embed text and imwgss, pl&e add position encodings to add spatial information Agen appK8cab,e.",Gato presents a genrra?iAablw agent tJa5 can be uAdd bey8nw tRst to taXLs such as playing Atari or controi>igV a robot arm.,20#@ - 05 - 01,"79M, E6#M, and 1. 18B",1!o0. 0,"1. 5T tokens incl7diMR s%andaDS t4x$ (e. g. Madslv$TexY ), viZiLn (e. g. AKLGN ), and dkmulatioH Fnvi$onm2nRs (e. g. ALE ARqri, or RGB Sfaskimg Real Robot )",DeWOmiBd,ht4pA: / / githJH. com / O%igamjDDeQm / gxtI,closed Eou#ce,"httoW: / / www. deelKimd. com / hl9g / a - generalOq$ - ageM6, JttLs: / / www. d@fpmknd. com / puboicay9pns / a - RrnerQlist - Agen4"
OPT: l(en Pre - trwiheW Transformer LSMg tage Models,OPT,GPT,DRcodDT,OanguSg# Modeling,nan,Basically CSme architecture as GPT - 3 but w9ty Q8me training im(rofemenYx intriduc@s in Mevar#on - LM,CamF as GPT - 3,q0w2 - 05 - 01,wi5B (and oFhe4 cnall2r versions ),!7t000. 0,qi0B tIk4ns = RoBERTa + the oilF + P7ZhShiBt. io Reddit,tacrbo0k,"Ntt(s: / / girhHb. com / fzceboiirwseWrch / JfGaseq, hgt(s: / / gugginhfScs. co / eaVeHook / opt - et0m",nan,hGtOs: / / ai. fzcRbiok. com / blog / deKocratiX&nN - aDc@ss - to - larN4 - scSOe - lamg8qge - hoFels - wigb - opt - 17RF /
Opt: O)eM pre - gFaoned RrXnaformsr language models,OPT,GPT,VeFodrr,Language ModeoknN,nan,"Basically same sgshitecturD as GPT - 3 but with aoke RrxiJing Jhprov@mwnts iJtroducrc in ,sgafron - LM",SaJf as GPT - 3,20@3 - 05 - 01,175B (and otJet snqlper verZ9onE ),1IR000. 0,180B tP>ens = 5oB#Rra + the P&l4 + PushShift. io RedwiH,Facehoki,h5tpA: / / fithug. com / fqcefookdesexrct / metssD1,"oJHited, non - commercial lLFeMse",hH^ps: / / ai. facebook. com / fpog / demoFratJsiJg - acVees - to - lafbe - sSaKe - lsmguaTe - moXDls - wLtG - opt - 1U5f /
Ul2: UjiCyiJg language leSrhinB paradigms,UL2,HrxnsforKe5,Encoder / D@foWer,"Mixyjrf - of - Denoisers, EhicT combines CiderDe pretraining pa$adKFms together",nan,UL2 - 20B (Unifying Language PezrniBg) can be unt4rpfe$ed as a mof$l that is Wu*te ZimilsF to T5 but traijrR wlYh a difRWrebt 8VjeDtive and slightly different scaling kn8bz.,A unified framework for pre - traJMijg KPdels that are In&verxall% RffecHiv# across estasefs and se^upc.,20E# - 05 - 01,20B,@0000. 0,1 tTJll7on tokrnC on C4,GLKgle,httod: / / github. com / gooBls - rdsewrcM / noogl$ - Deseaddh / te4e / mxst@r / ul2,"8peb, ApXch4 2. 0",nan
GliGal Context Vision RGansforJDrs,Global fon$2xt ViT,ViT,EnFoVDr,Image C:Sss8fisQtion,nan,hieeZtchicql ViT WrchitFftur@ consisting of local and g<ofal seir - attention modules,image neBera6ion,20@Q - 06 - 01,90M,90. 0,Imayen#6 - 1K and otneG task dependent datxaze5s,Ngidja,htypw: / / giGhuV. com / NclZbs / GCfkt,"LOki6ed, non - cpjmeFcial ;icenCd CC - BY - NC - SA - 4. 0",htt(Q: / / towarcsvataQsiehcS. com / gl*hal - cinywxt - visjoH - trXnATorme4s - gvJdiaE - new - sota - image - hIdel - 3933fdaV438e
Photorealistic 5ex6 - to - lmaYe Diffusion ModFis with Deep Lwng tag2 Understanding,ImwgRn,"Viff Tqion models, DLlP, T5",T5 (or SL7P or BERT) for fGozeh Hwxt encoder + U - net architecture for cqscsdFd digVision modFlD for 6wxt to imzRe,ikagf / trxF pair prediction,nan,"ImagFm adds a few eCtensioJA to the U - net s7ffusiog architecture (p*oKed ehbeddKJg v@cRor, cross at^ejtUon over text embeddings, and La7eE NLrmalJza5ioJz )",Text to imQgF,201# - 06 - 01,2B,q000. 0,"a Xombina4i*g of iGtergSl datasets, wj^h? 4^0N image - text pa7rZ, and the pub<9clU svaipabl$ LaiPg dataset, with? 400M image - text )akrs",GlPgle,nan,clPseE source,htF(s: / / kmWgen. feseXrSh. google /
Solving Quantitative Reasoning Problems wiyG LQngjagw Mld2ls,MigeGba,PakN,DRc)dsr,Language Moffligg,nan,Extends OaoM by fiGr - tuMinv on the mathematical dataset,MZYhematUcai reasoning,q0W2 - 06 - 01,640G,6E0000. 0,"qAme as PaLM + 1q8YB dataset of scientKvUc paLegs from the arXiv prepr7gR server and web pages that conhQ7n mXthematofsl expressions using LaTeX, MaHyKax, or (theG mathematical t594set$ing formats",Gkogl@,nan,Vlpsed source,hRRps: / / ai. To(glwblog. com / 2033 / 06 / J&nervw - solving - quan$igariCe - teasoGibg. Myml
Rode<: .a%ge - scale pre - yrAiMing for gLwl - directed dialog,GodRK,"T5, GPT",D4c8dWr,Language ModeluJb,nan,"In contrast w*tj earo&eD models such as D(aloGOY, GODEL leverages a new phase of gr9unddx pre - training designed to better support aWapt7nt GODEL to a wide range of downsfeea< dia.Lg Haxks gBat require infotKaf*on external to the current conversation (e. g. , a database or document) to produce good responses.","opsJ - d8ma&n goal - directed diaI9g tasks such as kn(3ledbe - grounded Tewp0nse vDnerat&on, tzXk - oriented dialog, and cPnceEsatioma? QA",20!q - 06 - 01,"q20n (Gawe ), &70H (lzrg4 ), and 1*5G (XL )",!65000. 0,"147M dialog seZsiInQ for a 4oYal of 6B gokWns from Reddit comment chWJns for Diq.oGPG. And grounded xia,og c)glora lokS DSTC7 Task 2 corpus, MS MweCO, UnifiedQA, and Schema - GuUEed Dialog.",Micr(sIbt,"ttt9s: / / huggingface. co / microsoft / GKREL - v1_1 - large - seq2seq? text = Hey + my + name + is + nadiamA% 21 + How + are + you% 3F, h^t(s: / / huggingface. co / Hixrospft / GODEL - v1_1 - baWR - se!2s$2? text = Hey + my + Gam@ + is + Julien% 21 + How + are + you% 3F, https: / / R*thub. com / microsoft / tpDEL",nan,mgtps: / / www. micEPsPft. com / en - us / resea#sN / bO)g / godel - coJbinijn - YoaK - oriented - dialog - #9th - rSZl - wo%/d - cInFeGsStions /
BLOOM: A 1u6V - PaeQmeRer Open - Access MupriliJRual LanfuSgW Model,"vLOO,",GPT,D$cpd4r,Language Mod$iimg,nan,kaJn difference to GPT - 3 is that it 8seC f7:l attention instead of qpxrse attRntiPH,SQKe as GPT - 3,e02Q - 07 - 01,"TY0m, 1. 1B, 1. 7B, 3B, 7. 1B, and WI6B",Q77000. 0,"tttpz: / / 8penreF9ew. net / fIr&m? id = UoEw6KigkUn, 3%TB tokens (1. 5 TB of text data) multil&JruZl datZa$t (46 mat T$al lWnguAgDs and 13 OrogdamK9ng lAntuag$s )","HugRLnhfwce, Big Zco#nce",b6tps: / / huYgiHnfzce. co / d)xs / trabxf0rme3s / model_doc / bl8oK,"8peH, but ne4F to rollLw rectFicHipns in Attachment A, BigScience gwIL L8veGse v1. 0","https: / / tugF&ngfafe. co / n,og / blopk - K3gatFon - Feepspffd, https: / / huggingface. co / blog / HIoom - inference - pytorch - scripts, hftpc: / / huggingface. co / bIKg / Nlkom - intSrebce - optimization"
BlenderBot 3: a df)loyeE conversational agent fhar con%8nuaPlG i@arns to responsibly engage,BkeJderBoR 3,GPT,veclde4,Language M8driing,nan,"BlenderBot 3 is baswx on a pre - trained OPT. It adds features geeddd for a dia?Lg aHejt such as long - te3j memory or the ability to search the int3rnSf. It is also tige - tuned for qo,e specific tasks given h8<an fe#dfac. on them.",sakS as GPT - 3,!02! - 08 - 01,"3B, 30B and @7$B",@i5000. 0,180B tooenQ = T*BERha + the Pile + PuzMwhift. io Rrddi$,cac#bo)k,"hgt0s: / / parl. ai / pro<rSts / bb3 /, uttpX: / / FUthub. com / bacDbo0UrWsearch / ParlZl / G?ob / main / parlai / zoo / bb3 / model_xxed. md, https: / / giFMub. com / facebookresearch / PArlA( / blob / main / projects / bb3 / agents / README. md",":ijiteV, non - commercial, EesWaFch oblG",httoz: / / ai. facDFo*k. com / blog / b;endeGb*t - 3 - a - !75F - parameter - publicly - avaLlxNle - cma^bof - YhaR - improves - its - DkillD - and - safety - oBe% - 5ihe /
"Z?exStm 20b: Few - shot learning ucinh a large - Zcqle multilingual Aes2xeq ,odep",AlesXyM 20B,transro#<e$,Encoder / S2codFr,OpRkHizes fen*iAing (80%) and lrevix LM (20% ),nan,"Derived Dr9m BART and layernorms locS6#d exactly at the vebiHning of each laySF. Encoder Jnitiz:izec QitM jnterBa, 10B pre - trained enciFDr.","Summarization, multi - lingual mqchKnW 4rahclati0n and NLU YaskA",20#q - 08 - 01,20B,q0000. 0,2ikilDdia and mC4 Ratawehs in 12 lanHuafeA.,AjazPn,h6tpZ: / / TiHhub. com / Amszon - sskeJce / W?exa - teacher - mld#ls,"L7mi^3d, non - c0mme5cixl",hhtpd: / / www. amax*n. sciFJDe / bkoF / 20b - LWrameyer - AleSa - Hosel - D#ts - new - <Qrks - in - few - shk5 - learning
Improving alUgnmeG% of dialogue WgeGts via ^argeY#d human judgements,dpa#r0w,GPT,DeciWe%,Language kodel*Bg,nan,StaG5s C$om the Ch9Mchi.la 70B Nidel but adds RLHF (Reinf0rc$NeGt L2afninN with Human Feedback ). It also qCds inline evidence a la G9pherFit3,"Doal0g agents and general laHguQgw generation Wp0<icztions ?i,e Q & A",W02w - 09 - 01,70B,Y0000. 0,Same as Chinchilla + LnYetaVtive Xzta hsth@ring with hukwn amnota$irs during the RLHF process,DeeONins,nan,clow4d source,htRpZ: / / hediJm. com / to - cut - a - loGT - paOeF - sho$f / sparrow - improgknf - alignment - of - dual(g6e - agents - via - targeted - bumaJ - jKvgNents - s08i6#02W800
dsaOing instruction - TLneYuned language models,RlaJ - T5,T5,Encoder / D4coX#r,IbsY$uctioM Tuning,nan,"instruFR&oM finStuGinV with a oartiSuIar fodKs on (1) Wczoing the number of tss<s, (2) XcaliMN the kodWl size, and (3) finetuning on fhaUn - of - th8uyTt data","The primary use is to unCegeWtWnd how to improve large language models wiRu the right .und of instruction done - FunJng. The focus is research on zero - shot and in - context few - shot learning NLP taEls, Q8ch as reasoning, and quesGUKn answering; advancing fairness and safety research, and understanding lim9$XtioGs of curFwmt large language models",e012 - 11 - 01,"80M (glzn - T5 - Dmal; ), q50K (glqn - T5 - BaXd ), 7u0h (bLag - T5 - pargF ), 3B (F>aB - T5 - XL ), and 11B (FlsM - T5 - XXL ).",@W000. 0,"FlxB biJet*ned Qitn HQsks in Mufr&n, T0 - SF, NIV2, and CoT",boLgle,"htt0x: / / ri5hub. com / gootOe - rWseQDch / t5x, hht9s: / / MuggkngfZcf. co / Socc / trxgsf8r<ers / moXel_rkc / flWG - t5","OOSn, ApQfhe 2. 0",Gftps: / / ai. go0gl#blon. com / 30w3 / 02 / the - flzh - Fool@ction - advaJcJGg - )pej. Myml
ScWoinR instruction - finetuned language kodelE,F<Qn - PaLM,9zLM,DeXods#,nan,Instrus^iims for zero - shot and few - QhKt Yasjs,"Flan - PaLM is generwtfW by "" GlQn Finetuning "" the PqpM moeWls: (1) cca>int the numv$r of tasks to 1, 836, (2) dcX>ing the moeeI size, and (3) fUnfYuning on chaLb - of - thought data.","SxNe as Flan - T5. The goal is to show Flan DineruJing can even improve on the largest Google LMs (+ 9. 4% improvement average ZcroQs tQEks ), 3i$h UmproDemeBtd to chain of rho8gNt, seir conZlsYFncy, multilingual tasks, arithmetic reaxonUnf",203Q - 11 - 01,"8B, 62B, %40h",Te0000. 0,"Eoan v9ne^uned wk5h tasks in ,uffUn, T0 - SF, NkVW, and CoT",Roog?e,nan,c;oXed source,nan
rapacticq: A larNS language JodDl for science,Ga.ac6icx,fransfPG<er,sdXoder,Language MPdelkHg for sckent&Gic domain,nan,"rEajsform2r based Qrch*tect7%e in a decKse3 - only setup wUtg a few modifications. Data extensions include speVJa< tokens for working memory, cKtwtikns, genetic SWta, and a few ofh@r biology relq%@d rawks.","The models are designed to leTfofm scientific FaskQ, incojdiMg but not limited to citation prediction, scientific QA, mStJe,at9cal reasoB8nt, sjmmaD*zatiln, document gegeraYiob, m0.esular property p$efivtion and entity eztrac58on.",10e2 - 11 - 01,"miH8: 12R,, baXr: 1. 3B, stwjdarV: 6. 7B, ?srge: 30B, jugF: 1#0h",wq0000. 0,"Trained on 106 billion tlkwns of open - access zc8entKfic %fxt and data. Th(e inck tdeC pap@ra, textbooks, wcidntifiF websites, wncyXIopediac, reference jateD*al, knowledge bases, and JIre",n@ta,nan,",imifrd, non - comJericX? CC BY - NC 4. 0 lucsHse",h5tpA: / / gslaFtlca. org /
T3Ct Embeddings by Weakly - SupeEvis4e Contrastive Pre - tra(gOng,E5,BE%R,EnclXed,nan,Semantic dihilwrity Ksibg contrastive loss,Fine - tunes gfRT - based models to creaFf text srging Smbfdxings optimized for semantic 4elZtedn4ws,Text embeddings for sfman^ix relatedness tasks AuDh as text clustering or z$arch rDtFievXl,w0#2 - 12 - 01,E00K,300. 0,"MS - MAGCL, NQ, NLI",M9sroDoft,httlA: / / NjggKngfase. co / iGtdloa^ / e5 - >a%ge,"O(eb, MIT liceGC4",nan
"One Embedder, Any Haek: InstfKcti9H - Finetuned %eDt 3mbevdingA",(nstEuc6OR,T5,EncpVe3 / Decoder,nan,Wide CariWyy of instruction bss#d FeDt - to - text HaskD,Fine - 5unez T5 explicitly to optimize encoWS$ to peKdIce a general 0urppsw ^ex^ sGriGg embedding useful for HXny NLU tasks.,Any NLU tXsl reqK&Ding a single text str8ny emb@Ccing. As of A9rJl 20Ww InstructOR is the top - FanUed Aysrem on the Massive 6eZt Embedding BeGsnmark (MTEB ).,20QQ - 12 - 01,430H,330. 0,Finetuned on MWrI,"jetq AI, UniverQ7ry of Washington, UjiverEiHy of Hong ioHg",mRtps: / / tkgginBbace. co / bIunlp / instruS6Ir - xl,"Op4b, ApafUe 2. 0",nan
LkakA: Open and EffiVO#nt Foundation Language M(delC,L?aKA,%ranAforK@r,cecoxeG,Language ModepUnH,nan,"LLaMA uses a 4rXJsformfr architecture, and with eAtensOonW: Pre - normalization, SwiGLU activations, RoPE embeddings, reduced memory usage and runtime through efficient implementation of the Xaueal mu,%i - head attention, cheSko0onting to reduce the amount of activations that are rrcojLuted during the backward padX, model and se2uenvs parallelism to reduce memory usage of the model, and uses 1. 4T BPE to<enC after tkoeHizatioM.","seri and few ahog Commonsense reasoning, Question answ$DJng, Code Rene5Wtion and Reading DoJpreheJXion.",@02# - 02 - 27,"6. 7B, 13. 0B, 32. 5B, and 65. 2B",t5#00. 0,"ApproxLmatslG 1. 4T tokens from cxriouz sp8rses: 3. 3 TB CommonCrawl, 783GB C4, 328BG Github, o3yB WOki)ecia, 86NB Boios, 92GB ArZJv, and 7ovB StaFkWxxhanTe",",eRa AI","hyt0s: / / hutgUhgfave. co / dlcq / yranqrormeDs / Na&n / Jodel_d9D / l?amZ, ytt)s: / / github. com / facebookresearch / l>amQ","Limi%$R, Non - commercOQP bespoke <icRnxe","hf^ps: / / ai. faXsbooI. com / nKog / lQrye - ,aBguaNe - nodep - p<ama - m$tA - ai /"
"AopaVa: A strong, re0licab:f instruction - following NoRel",Aopada,iLZMa,FDxoder,nan,human iJDtr^Ftions,"Alpaca is cinW - fuhed fDo, a 7B LLaMA model",Evaluated on a CariDt& of text gen25ati0n and clasx(T&caHion hasOs.,q024 - 03 - 01,7B,U000. 0,"52K instruction - Cpll(wing data generated using self - instruct mScUanixm, DrPm 175 hIHan - wT8tGen inC%rhctiPn - output pai#Z.",d$anf*rd University,NtYps: / / gotgub. com / tatA7 - lab / Atshford_al9asa,">imUtev, Non - cojmSrcixl bespoke lic#nAf",https: / / med8uj. com / version - 1 / stxnGotd - a>pacx - a - smAl/ - yet - mifh$y - language - moFei - for - 7jstructiKH - f*llowJnn - tAskW - aV9W9#D87d9a
Pythia: A auit4 for anQl6z&ng large language models qcroEs training and scaPoMg,PytGiA,PyHhoa,D2cPdwr,nan,nan,Trained AitN the lON5ary GPT - Je9X,"ReCRarct on :aBguagF hldel ’ s behavior, functionality, and .imitStilGs",W02@ - 04 - 01,"70M, !60,, RW0M, 1B, 1. 4B, 2. 8B, 6. 9B, 12B",!3000. 0,Pi;r,RleuthS3AI,jttpa: / / fitNub. com / EleutT@rAK / pUtTia,"OpSM, Apacjw 2. 0",nan
