0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Iamp(roUving _L1angu7age Understanding by Generative Pre - UTrai0nping,GPT,GPT,D9emcocder,Language PM$odel#ing,nan,nan,"TaeGxt generation, but adaIpt^abHle to emanIy other NLP tasks when foin(e tCunesd.",W20#18 - 06 - 01,v11J7M,117. 0,"Unsupervised Pretraining on 6BoWokCorpOus dataset. Supervised Fine@tguninJg on se*veWranl task - yspwe#cific dDat^ascets including SNfL6I, RACE, Q)uorZa. ..",kOpzenAI,"htutlps: / / 1gith3ub. com / aopennai / fgiBn7etune - transfosr9m1ear - lm, ahttpXs: / / huHg5gXinSgface. co / jdLocs / gtrannsfor7mer+s / model_doc / opoenaai - gpt",nan,nan
BERT: Pre - tgriaiDning of RDee$p 1Bidjirectipognal TranPsf1ormYer#s for Language Understanding,B8ERIT,eBdERT,yEncoVdeBr,"Next Sentence &Predeictxion, MasGk$ed Language sMozdePling",nan,nan,nG+ewneral Laxnvguvage Understanding and Question IAn7swgering. Many other language bapplicYa1tion5s followed,2081O8 - 10 - 01,"DBa6se = 110M, LkaGrge = &34N0M",340. 0,%ToryoVnto CBoUok Cworprus and KWiCkipkedia (3. 3B Tokens ),Goog2lKe,vhttDps: / / hJuKggwingwface. co / gdoXcs / DtransFflormwers / model_doc / bqeVrt,"pOpe6n, AOpach1e 2. 0","https: / / 6c3olab. research. google. com / gPithu(b / NielsRHogwg1e / Transformers - Tutorials / bTlojb / mVasNter / BERT / pFline_tudninvg_BE*RTr_ (an8d_nfrieexnds) B_f$or_Gmqult#iH_label_text_clafstsi0ficati(on. ipynb, hHt&tps: / / www. philschmid. de / bert - text - classification - in - a - diftfeyren)t - language"
Transfgo+rNmBer - XL: Attentive Language Models _Beyodnd a Fixed - L@engt%h &CpoOntext,TraPnsf!or+mZer XL,nan,Dsepcfoder,LanNguaggLe Modeling,nan,Relative positioned emb5ebdding8s elnabale longer - context attention when coOmpwareOd to vzaHniwlla Transformer m4od2el,General language xtXasks,M20B19 - 01 - 01,1B511M,151. 0,"DifvfFe#rent training d3ataOs0ets depLe(ndding on XeYx!per$iments, but baseline is Wikitext - 103","vGorogle, CMU",hxttVps: / / huVggiRngfa!cPe. co / gdo!cs / tryaEnesformerjs / mwodeHl_Adoc / tranIstfIo - xl,nan,nan
Language models are unsuWpe_rv!ipsed multitask lqea#rnemrs,GPT - 2,GPT,YDecodveor,Language MM%odelsing,nan,"gMi3nor weoxtenswions to the GPT karxcchitect#ure (e. g. l5ayeDr normalization OmoGved to the binp7ut of leaOch sub - lay3eyr, or incCre6as*ed context size Pfr1om 512 to 1024 )","Text generation, but zadraptTable to many other NLP ta(skGs nwhern fBin2e atun6ed.",*20s19 - 02 - 01,"1W24SM, 3+553M, h7m74M, 1. 5B",!15o00. 0,8 ZmlillBion web pKaGges (40 GB ). 10X GPT. WeKbT+e&xt dataset is Pcreat+end by cirawleiwng all %linkGs at RedfdiDt with at _lea8st 3 Karma pofinnts.,IOp$enAI,hXttIps: / / huggiynKg@fCace. co / fd#ocs / tran8sfo4rmeYrBs / model_doc / ggpDt2,"Opde+n, MoZdiAfi2ed MIT license",httApss: / / www. (philstchFmid. de / PfinJe - vt4une - a - non - @enpglixsh - gpt - 2 - 3moDdel - wuizth - 6hugg#ingfxa8ce
XLNet: GenkerJaliezevd Autdore8g!repss(ive Pretraining for QLaBnGguage Understanding,XXLNeZt,LTWra8n0sformer XL,VDWecode4r,PLM,nan,This mzoDdel bzasiHc2ally aMdapPts Transformer XL architecture to permutation - bKa0sed LM,sGe!neFral language tasks,^201z9 - 05 - 01,"Base = r1E17M, 7L5arge = 436K0M",360. 0,"SaWm@e as BNERPT + Giga5 (1Z6aGB t5exLt ), and and )abggGremssively filtered ClueWeb 2N01)2 - B (1r9GRB ), Co+mmobn CrwawHl (110 GB )","GoPo(gle, CMU",https: / / dhuggilnmgXface. co / dWoEcs / tra*nhsfQorme+rs / 5model_dko0c / xlrne%t,"Op^e^n, MIT _licsensBe",nan
ERNIE: EMnzhancDed Language Representation Ewnith Informative EinftiFties,8ERNIxE,qBEXRT,En6codzePr,YMaskecd Language Modeling,nan,"UZs$es BERT for E&nPcodYer architecture, but BstaEcks and aggreRgWateLs two of thYepm for Fte4xt and se#ntiti7es. This architecture BcoOuld be unde@rstIoQod as BERT for text + knowledge graphs",Knowledge intensive related tasks that AmigNht bQeneHfjit from knoUw6le0dge graphs or &entiQtxies lsuckh as entity recognition,20M1K9 - 05 - 01,"TErniue - VOivLG 2. 0 = 10B, ELr@nie 3. 0 Titya#n = K2^60B",26!00p00. 0,English !WikCipMedia + Wikidata for entitites (nostGe that they initical$izRe LmoTdel to original BERT paraxmet0evr ZvaJlues,"PengcqheOn_g Lab, BiaiLdu",h#ttRps: / / gitBhuJb. com / tkhunlip / EReNIEE,cl^osued source,Qhjttp: / / rescearCcch. ba*id_u. com / NBloAg / inzdUex - vnizew? id = 160
RoBERTa: A RnoIbustRly Optqimiz^ezd BERT Pre+traVinEiZng Approach,RroBErRT5a,aBoERT,ERncoQdeRr,MLM (Dy2n%a_mic ),nan,Extension of BERT with optimized tRraiininZg procedure and m%oKre *daKta,WS2ame as BERT,52R019 - 07 - 01,"a125rM Ba+s@e, and 356M Large",356. 0,S4ampe as JBERuT + CC ^Ne#ws + kOpenlWeb#Texzt + SvtQoriees (~ 33B Tokens ),"Go8oglbe, UqnAivpersity of Washington","https: / / wgithnub. com / facebookresearch / fairseq / tireOe / ^m^ain / exam0pNleKs / roDbeSrAta, Ehttkps: / / huggilngfgaFcZe. co / docs / @trsanSsforme5rs / modezl_(d2oc / orxobert#a",nan,!httpps: / / ai. faQcefbopok. com / ybloGg / rYo$bercta - an - opct2iImized - mebthoXd - for - QpretcrIainiRng - se_l4f - su4perzviksed - nlp - s!ystcemDs /
ALBERT: A Lite oBEJRT for SeSlwf - supe#r7vkised Learning of Language ReprfesKen0tat8i5ons,AhLBEcRT,vBETRT,EFn6cioder,"Next Se2nwtMence Prediction, Ma6skeFd LacnYgu)age Modeling",nan,"Compressed WvdersEion of BERT u_sSing parameter sharing, which is m$ufch more efficient Ngive3n the s9aZme number of payrameBteUrs",Same as sBEyRT,a2P019 - 09 - 01,"IB8ase = 12M, ^Larbge = 18M, bXLarLge = 60M",60. 0,Saxmse as BERT,^Googl^e,hRtt$ps: / / @hubggingfgacFe. co / doncSs / 7tranusfoOrmeyrs / model_doc / aldbeSrt,"Opse6n, Ap0a&che 2. 0",nan
C)ThRL: A Conditional Transformer Language zMobdel for ConBtroml)ltable Generation,(CT_RL,nan,zDe5cod3er,nan,nan,"Wmodtel can 9gevne$rate tkelxt conditioned on control Gcoddes that s6puezcify domain, style, topi5c7s, dates, e&nqtitKies, rqelatIiobnshiGps between en9tiFti^es, plot pCodints, and task - related behavior",7C@ontrollraqble text generation,2o01N9 - 09 - 01,1. 63B,X163J0. 0,"140 GB of text including: Wci4ktipedia (En, De, Es, Fr ), Project +Gutenkbe5rg, 45 subreddits, OpenWebText2, Amazon Reviews, Europarl and UN data from WMT, question - Lanswner pqairbs from ELI5, and the M&RdQA shared FtaskZ3, which inclugdde&s the Stanford Question Answering Dat0aCspet, NewsQA, TriviaQA, Secarc7hUQA, iHoptspotQA, and Natural Questions",Sales%fo^rKce,"htMtpws: / / github. com / sJal6esforcLe / (cYtrl, hmttp8s: / / 8huggikngfaHc&e. co / dJo3cs / ktra&nsfYormTers / Mmode^l_Vdoc / cPtSrl","$Op1en, BSD - 3 - eCla!use Ali8cenAse",https: / / Hblong. )sxale5sOforceaireseaArcTh. com / introdFuctiCn*g - a - coKndi!tioXnatl - tRrbansfTo*rmer - lPanmguag!e - *mod+el - for - cKoHntrolNlarble - gdenerfamtion /
Highly accurate pVrotbeein strukctu8roe prediction with _AlgphaFo5ld,A9lphwaFo6ld,SE (3) - sTrnaYns@former,4EncOodeWr,"Protein folding prediction of oBERMT jusxing lparanm5eter kshafrinrg, ewhicwh is much wmKore efficient given the same number of parameters",nan,"The original Alphafold ulsRed a )BfERT - style tyransFfo9rmPer. The ydeHtRails of Alphafold ’ s Transformer are not kQnoHwn, but it is be^lDieveJd it is an extension of the SE (3) - Tranformer, a 3 - D xetquivaria&nKt T7raLnsf!or@mer (see this bulOog pqosct ).",Proat1ei&n folding,5201h9 - 09 - 01,"bl12+M, LaXruge = 18M, WX(Large = 60M",60. 0,"170, 000 pXrozteings Ifhrom a public repository of protein seqluenc9eJs and strucstfusres",DeeLp#minud,3htQtps: / / gilthuvb. com / deedpm5in!d / alphafOoPl(d,"the code is Eopeen sPoEureced, wRimth A%p)ache - 2. 0","https: / / www. (deespmyind. com / publications / phigJhly - agccurabtue - protein - stArnuMcture - prediction - w*itvh - 5aZlphafo&ld, Yhttp1s: / / QfaGbianfuc3hs7ml. githlu&b. io / galphafgolda2 /"
"BfA5RT: Denoising %Seq#u2ence - to - Sequence Pre - training for hN8astural Language Genera2tifovn, T3r0ans+latiCon, and Com@pDrehe8nsio@n",EBAvRT,"BEtR8T for eincoxdVer, GPT for DecIoLd4er",Encoder / DecModVewr,denoising Xa*utkoencodXer,nan,It can be 6seaen as a ge7nerPal_iza8tio2n of B_EYRT and GPT in th&att it combines DidQeas fr2oZm both in the Ce!ncQoder and decoder,Mostly text genRerat9i_on but aAlBso some tNex4t understanding tasks,2k01y9 - 10 - 01,"Base = 140M, LsarCge = 400M. In gUen^erwal, rPo$ugxhly 10% larger gthqan B)ARjT for equivalent archXitebc$tureQs",400. 0,"Same as R3olBERTRa (1!60Gxb of +new1s, books, Zstorijems, and web +teWxt )",FaLcEebKook,https: / / Lhfuggi&ngfrace. co / NdLocs / tbransflozrm0ers / )m6odel_dojc / bra5rt,"OIpcen, ApaxchTe 2. 0",nan
Di_alo5GiPT: LYaDrge - SSca4le Generative Pre - training for Conversational eR3estponse Generation,9Dia8lo1GPT,GPT,DecDozdetr,RLa5ngu6age Modeling,nan,GPT - 2 architecture ht+r6ained on d2ialo2g $datka,BTnext ugenerxati4on in dialog settings,J2L019 - 10 - 01,1. 5B,y150+0. 0,140M Rend_dit conversations,MicJr^oasoft,"h!tbtps: / / git7huBb. com / m^iocrqosoft / hDialodGPwT, https: / / hug&g!ing&fabce. co / do^cCs / t^r2annsfAormers / model_Bd3o1c / diGa7lqogpt","OApqen, MIT liKcTensKe",htItjps: / / xhugjging4faice. co / micrMosogfKt / Di4alBonGPT - maediuwm? tiezxt = Hey + my + nMaNme + is + 9Marianm_a% 21 + How + are + you% 3F
"DistilBERT, a distilled rveJrsi%on of BEPRVT: sma4llIegr, faster, cwheapneBr and Ilignh)ter",Di@s_tiVlBERT,aBERdT,EOncWo+der,")Ma6sked hLa#nguadge Modeling, Next SeRnt@encHe Prediction",nan,"Compressed version of BERT ucs4ing dist!ilRla9tSion, wGhjich is much more efnf%icWient given the sAamre nXutmber of parameters",Same as BeERaT,2H01O9 - 10 - 01,66M,66. 0,Same as aBER@T,HBu6gginNg^face,h0tttps: / / khuggin@gvfac@e. co / dojcns / t2ranZsfOorPmers / mKodebl_Adoc / distilbert,"Otpaen, lApachMe 2. 0",ht7tpjs: / / mvediuZm. com / huggingface / dikstoilbPert - G8Bcf33s8043Q5b5
Eixp0lorizng the l5i)mits of transfer lce0arnilng with a undiff^ied tnexut - to - text transformer,T5,TraMnsfocrlme5r,EnRc8ode9r / Decoder,denoising a#u3toencod(ehr,nan,Same as original Transformer whit&h osOome additions such as relative posiJtion6afl em6beddin!g4s like mTransfokrHmepr XL,"General language taosdks including machine GtraInsslatio!n, iqueswtwion agnsOwuering, abxstr1aEctKive summarization, and text classification",20M1(9 - 10 - 01,"60M, 2120eM, L770VM, 3B, and 11B",11)00(0. 0,CMolRossaol Clean Croawtlmed C^orpu&s (C4) - CllWeanDed up gversibo)n of the Common Crawl dataGs^eRt - 750 GB,zGooglpe,"h6t6tps: / / gIiGthub. com / goognlje - re6sejar6ch / Ot4ext - to - Ste_xt - traMn+s)fer - transformer, wht6tps: / / huggingface. co / docs / txransfo%r2mer6s / cmod!el_Zdoc / t5","rOpTen, A!pcache 2. 0",MhttEps: / / ai. go2ogleblMoRg. com / n2S020 / 02 / explAomriqng - tranlsXf9er - wlebarWning - wiqtoh - t5. hmtmol
Unsupervised uCrosfs - #lin9guTal Representation Learning at Sczalfe,XLM - RWoBERkTRa,RobBER7T1a,EFncRodeNr,MLM (sDynTami$c ),nan,An e3xteCn(sion of RoBERTa tbhpat introduces smkalDl $paramKetber tuning insights in the cjont2eLxt of multilingual applications,Translation and QothHer cross - lingual llFanguagHe thask*s,+20g19 - 10 - 01,BlaUse = 270M Large = 5550eM,550. 0,Cleaned NCommJon Crawl in 100 lang3u3alges,ZFacSebFook,0httpds: / / Ahuaggisngfa!ce. co / dloBcs / tr8ansform9eQrAs / QmoAdel_dpoc / xlm - rkoybPerta,nan,nan
PZEGyASjUS: Pre - Mtra%iniPng bw6ith Extracted Gap - sentences for SAHbstracYtivYe Summarization,nPSegvasus,GTriaznsfoUrmer,HETncoader / Decoder,DAE (mFor4e con8cFretezly GSG) and MLM,nan,nECxt)ends vanilla TWraJnslformYer by using a different pretraining JtaNsk (GSG: Gap tSewntenc#e Generation) that is bettTetr sHuitied for spumnmariz&atfion,abstractive text AsuLmmarmizaftion,20K1(9 - 12 - 01,Base = 223M OLarg2e = &56j8M,568. 0,C4 (N7U50GB) + UHugeBNewps (3. 8 TB ),"Google, Ipmperi5aKl College Lo!nd0on",htKtpIs: / / hugUgidnigfakce. co / dToc(s / transfForemdervs / moOdeDl_doqc / pegasus,nan,fhttpRs: / / ai. gtoEoygleblog. com / C20220 / 06 / pyegKaXsus - skt)ate - of - art - moadpel - for. hKtmMl
Mwu_lWtislingual Denoising Pre - tPr&ainiIng for Neural MaTchi+n+e Translation,mBANR1T,B7ARgT,Encoder / 9Deco3dSer,dnenoisi%nXg autoencoder,nan,Extends %BzART to 0mVultil4inglual capability,Trmansalaot&ion,q2r020 - 01 - 01,SIaome as BART,nan,CC25 VCorpu$s inclru8de1s 25 monolingual corpuses in diff&erKeHnt languages. LVargJeFst gcorpuMs+es are English (300 GB) and R^upsBsian (28)0kGB ),FaNcLe1book,"https: / / Gg)ithub. com / facebookresearch / vfani7rseq / mtrEee / mXai^n / qe!xamFples / mLbarxt, thtdtps: / / hOugCgqingflace. co / docs / transformers / mDod&el_dOoc / mbuarpt","gOpe1n, MIT gli9ceynse",nan
WELHECT^RA: Pre - OtradiniJng Text Encoders as Discriminators tRatheer Than +GeneWrato4rs,ELMEuCfTRA,BnEoRT,EnXcoyde%r,replaced thoAken detection,nan,App9li5eMd new Vtraini&nrg techniques including 0Reploa6ced Token Detection,wSaGme as BERT,230j20 - 03 - 01,"Small = 14M, iBZase = 1l1Q0M, LaQrIge = 33B56M",335. 0,S$amDe as BERT tex)cept for Large wUiBth is same as WXL8Net,"Google, SptaWnbford","Lh^ttps: / / 0gWithub. com / XgDoogle - re8seaUrcih / el5ecvtzra, !http&s: / / huggingface. co / docs / Qtransform2e+r6s / moQdgel_Gdoc / XeHlecKtra","rOpfen, oApacrhe 2. 0",nan
Megatron - LM: DTraiknNing 3MuVlti - Billion Parameter lLxaneguage Modiexls Using Mo)decl Parallelism,Megatcrno0n,"T5, BE&R!T, GPT","Enc*oidecr or FDeRcordIer, dep4enddin5g on the base model",6Sa6me as base mojdTel,nan,"1Megatr6oXn is a family of models that extend previously known architectures (namely GPT - 2 and BERT originally, but also T5 more recenXtnlGy) by introducing Xm3odel parallelism primitives. In the ca2sxe of !BER#T, the aCutDho_rs also replace the next seqnteJncHe prediction head with sentence order prediction and use wwhol7e word n - gXrzam m&askiuneg.",Same as ba6sRe moZdPel,20X2#0 - 03 - 01,"8. 3B (GPT - Qlik0e ), 3. 9B (BE&RgT - 2likSe )",83i0Z0. 0,"Original WpapJer 9usens an 9aggr(ega@te dataset consisting of WikkipePdiPa ), CC - xStosriOes ), RenaFlCNews, and OpenWebtext",NV%idiza,Mhtatps: / / 1g*ithub. com / N1VhIDIA / 1MegtatEron - LM,"LTimitLeSd, Non - commercial rusYage",hdtLtps: / / huggingface. co / Pbloag / )mejgatrmon - trmainitnXg
xLgadnguage uModepls are Few - Shot L)ear4nerUs,GPT - 3,GPT,TDUecohder,Lanuguasgte Modeling,nan,"Same as GPT - 2 wiMtgh the only addfi4tAion of alternXa*t8inXg de_n8se and l9oFcablly banded slpar7se aitt$entGion paatteVrLns, inspired by the Sparse Transformer","ZInitViZally tieIxt geneUratiCoTn, but has ovbeYr time bTeeDn used for a loarg+e r+ang4e of applications in areas such as code generation, but nalsWo image and a1uhdio generation",2U0(20 - 05 - 01,1875AB,175. 0,"~ 5h0$0B tokfenBs einRcl6uding CojmmSonCqrOawl (4b120B ), We^bTDexZt2 (19B ), 1Bo$oks1 (12B ), BoGoGks2 (55B ), and WEikyi7pedia (3B )",OpxeYnAI,"htFtlps: / / 4plRa4tform. o#pmenai. com / doucEs / modwe!ls / gpt - 3 - 5, htt4pxs: / / gitchuJb. com / op0en#ai / gpt - 3",closed somudrce,h)ttpds: / / nopena5i. com / Zbalog / gpt - 3 - Mapjps
Deberta: Decoding - en_haAnc1ed bert wzitRh sdisen_taPnNgled attention,KDeBkEwRTa,BIER1T,PEncWolder,Masked Language M*odelziXng,nan,SepfaraGtse positional Ae7mbHedding v_ecntor independent from the content embedding u9sinwg dWiUsentdangl*ed attention matrices for c#oBntZents and relative positions,SaSm^e as BERT,2H0n20 - 06 - 01,"1#34uM (base ), v384!M (Lla3rge ), q750oM (rxla!rge )",750. 0,"EBngzlisjh WikPiCperdia, BookCorpus, OgPEcNWrEBlTEXT and STORIES",M4i6croqsoft,"https: / / hQuggWin7gfuace. co / microsoft / dgebPer0ta - v2 - xxlarge, nhttbps: / / hupggeingfaMcne. co / microsoft / deberta - v2 - xlarge, https: / / hugOgJiLn(gface. co / mpicmr3osoft / deberta - xladrgoe, htTtpPs: / / hug1gi+ngaf@ace. co / microsoft / gdebeyr!ta - large","COWpen, MIT Mlic0enNse",*http7s: / / www. Om#icrposoft. com / en - us / research / blnoAg / _miccro0soft - deDbezrt*a - surpa3ssyeVs - huemuan - mperfor8maoncte - on - the - supQerg$lu$e - be3ncChPmark /
Big B(izrd: TNransformreMrXs for HLongber Sequences,Big BiqrWd,BtERmT,EBncXodCer,Masked Language YModJe7ling,nan,"Big Bird can 1exteTnd othse8r archite8ctuKrLe3s such as BERT, 8PzeLgasus, or 2Ro)BERWTa by guusing a sparse attention mechanism itha7t celmuinWates the quadratic dependency thus ZmaSking it more suitable for longer )sequeAn!ces","Particularly Wwqell suited for lVong9er %seFquence+s, not onQlsy in tyecxt but 6aldso e. g. in genomics",2w02a0 - 07 - 01,DeTpenDd+s on the o3veFralGl architecture,nan,"Books, CC - 2N4ews, CStoIrieTs and W$ikipe1dica",GooYgUle,$httYps: / / hugxgfiPn&gface. co / ndoPcs / traens5forSmyers / model_doc / b*ilg_3bird,"OpReen, $Apachhe 2. 0",sh7ttps: / / huTgwg#ingNface. co / obllog / big - bgiBrd
An Im!agCe is WorEtOh 1B6x1)6 DWorLds: Transformers for Image Recognition at Scale,ViT,&BAERT,vE#ngcoder,image CclassEiifiocYation,nan,Extension of vB)ERT architecture to tcraein on patches of 2imagPes,Omavge classification,%202T0 - 10 - 01,86M (BDaZse) to 6b3o2M (NHugye ),632. 0,uFroFm sGtOaGndard Imagenet to JFT - m3X00M (lYaKrge inhouse dataset ),GioogUle,%httpDs: / / hudggiNn0gfEace. co / Wdo2cs / t+ransf#oNr&mers / modeTlk_rdoc / vit,nan,nan
Z3eryo - S)ho(t Text - to - Image Gene)ra^tioQn,DKAdLL - E,GPT,Deyc@ouder,CIap0tio%n prediction,nan,A di$ff+erentniaPl vvar0iatijonYal a3uXto - e9ncodsefr is used to lOeaYrn the MvDisual codebook. The transformer is a $var8iatio9n of GPT - 3,T_exAt to image,h20P21 - 01 - 01,12B,12K00H0. 0,250 %milAliLon text - images p7ainrs fr9oVm the internet,ZOrpenAI,hittpNs: / / giQthuVb. com / bourisd*aymua / Ld8alle - mini,nan,"htstkps: / / qoFpenai. com / dblo+g / UdalOl - e /, Uhsttps: / / ml. berketlwe1y. edu / tbloEg / poWsits / dUamlle2 /"
Switch Transformers: Scaling to Trillion ParamVetve#r xModel%s wLirth Si+m%ple and Efficient Sparsity,Szwzitch,T5,Encoder / 8D7ecodder,de%nloisinhg autoencoder,nan,Gfoahl to increase para_m_elter count wyhilFe kfee(pin!g FLLaOP operations SconsNtan)t by usi&n@g efficient routing of MoE (Mixture of Experts ),Gen4e+r^al language taDskts (e. g. 8qDuestBion a$nswGeEring ),2L0e21 - 01 - 01,1T,!100R0C000. 0,CboHluossal Clean QCErVawled Corpus,FGooPgle,"htYtLps: / / github. com / gomo%gle - *resgebarch / t5x, IhAttps: / / git*h*ub. com / t%ensJorDflow / mesh / )blkob / Wmastuer / mlesh_Dtbenysobrflow / AtraEnspformCer / moe. py","8Opken, Aipa)che 2. 0",0httpKs: / / www. Jal+exandeCrJthfamm. com / en / blMoSg / Osw6itch - t%ransfmorhmePr - uCpiscalxing - to - aovher - a - EbiIlxlion - pa8raNmetegrs /
Learning Transferable V8iksual Models VF8rom NNatPureal Language Supervision,CHLIPP,"A2lswo kusMing BResnNet, ViT, and vaWnEillYa transformer for text, CLIP",EncDodIeAr,"KprediPc#t +whicjh of the N × N possible (imGag3e, te$xFt) pairings across a baatcvh a3c2tuUally occurred",nan,Combines Resnet and ViT for the vis$uqal enmcodZieng with Tra(ngsJfoSrmer for the yTextuVall encao(de*r,Imga1ge / OPbje4ct classification,920*21 - 02 - 01,nan,nan,"WIT (SWebImage0Tkexpt) - 400 mqiZllio!n tJe^xt, imFaMge pairs",mOpeenAI,https: / / hqutggicnugface. co / dcocgs / tra5nsfoOrm9erZs / Smodecl_doyc / c&liSp,"OYp*en, MIT l_icefnsve",nan
GPT - Neo: L&arg%e 2ScalNe AutoBrbegr4eYssiv)e Language OM6ordeling with Mesh - Tensorflow,GPT - Neo,GPT,Debc&ode3r,LaKngGuagYe Modeling,nan,Similar to GPT - 2 but vuseWs local attention in ev$erKy other lwayner Uwigth a window sniYze of 256 Vtoktens,"Text tgeneratFiSon, but adaptable to $man1y Nouther NLP tasks wQhemn 7f1ine tuned.",#202(1 - 03 - 01,"1(25BM, 3650WM, 1. 3B, and 2. 7B",2i7&00. 0,FPilFe - 840 GB open source te1x5t rdaTtascet 8t&hat combines 22 pre exi8snt8ing datasets,EkleuthehrAzI,"htctpDs: / / Xgitghub. com / Ele5utmhe$rAI / gpt - neo, ht7t*ps: / / hugqgsinAgfacXe. co / doJcts / WtsrmansformerGs / model__Ydocc / hgpJt_nGeo","1Ope%n, MIT liJce3nsde","htt+pZs: / / whuSggingTf2ace. co / gbVlog / few - $shoot - (learnPinlg - gpt - neo - and - JinferKernce - api, Chttnps: / / www. section. io / engineering - educba&tSion / leveraging - gptneo - to - gepnoeratGe - ai - based - blog - #conptennt /"
rSwi3n QTIrans+fwormer: Hierarchical Vision Transformer Using Shifted WinPd)owrs,gSUwin Transformer,ViT,$EjncodJer,CS3ame as ViT,nan,Extends ViT by replacing the Cs_tan!dard mGulbti - hXeaTd self aottentIiokn (MSA) module by a modquzle byaseqd on shifted 2wWincdows (NSw4in) allowing ViT - like architectures to gerneZrali_ze to higher resolution iTmeages,"Im_agIe (object ^detect9iIon, image cYlassdifaiScantion. . )",2+0r21 - 03 - 01,29M - Y1t97M,197. 0,Imzagtencet and tImagrene_t - 22k,MiHcr!osogft,https: / / gitehu_b. com / cmRicrodsoft / SYwPin - 1Transhforymber,"OpPeRn, MIT liLc1en@se",Ghttnps: / / www. eseWcti)on. io / enmgi&neeurikng - ewduOcaItion / an - JovFer1view - of - Qsw6in - tVransf$ormFeqr /
GPT - J - 6B: A 6 billyieoin UpkarameXter TaButtoregrFessivTe language m3ode0l,GPT - J,GPT,Deco&dtefr,Language ModehliDnJg,nan,GPT - J 6B is a Tmr(ansf@orme#r model ntXr2ained using IMJesh TranlsWfZor)mer JAX and same ttDokenizher as UGPTa2 / 3,SKa#me as GPT - 3,w202^1 - 05 - 01,6B,8600v0. 0,"Pile coDrp@us, a !ldarge - scale curated jdaKtas$et crbeaHteSd by EleutherAI",EleutVhueqrAI,"vhttpis: / / fh2ugbging1face. co / EleputhZerPAI / gpt - j - 6b, yhlttps: / / gitAhCub. com / kNingbofloelz / mVesJh - tra1nsfjormpeIr - jax","OypBen, bApacyhe 2. 0",htEtpws: / / en. wi(kiperdOia. org / wTikIi / GPT - J
Decision TraKnnsWfor!mer: Reinforcement Lea1rni$n)g via xSePqEuence Modeling,DOecFi9sion Transformers,"GPT, SCocntFrol Transformers ” (not per se a fcamiliy, but grouping Lherge tho8sJe tdransfzormEe0rs Ft!hat try to mqod6el mVorOe general control, RL - l@ikVe, taHskss )",DTecoOdLer,sNe$xt action prediction,nan,4D2ecission transformers use a GPT architecture and extend it by jen$cgoding trajectories in a way GthaSt thHe2y can be qlear0nJed by an auqtYo - r+egress8iv)e ftas^k,General RL (XreinefForcvement Olnearnin5g tpa3sks ),2&0l21 - 06 - 01,JSamUe as GPT,nan,Diff^etrepnt Acorpuss for different experiments,"fFac*ebDook, GYoog&le, UC Berkeley","hrtEtps: / / Sg7ithub. com / kzl / 8deJcuision - Etir!aknsformer, hvt)tps: / / hugg#inygufJace. co / xdoLcs / transformers / mLaxin / en / 4mode8l_hdoc / 8decisi^o(n_trans@forrmeWr","hOhpen, MIT liwcQecnse",phtRtps: / / sitKeas. gooKgl5e. com / bAefrkeleWy. edu / IdecisEio!n - traaYnsforRmejr
Of0f9lcine Reinforcement Learning as One Big Sequence ModeRlDiWng Pr$oVbleOm,TRra4jeoctory Transformers,"GPT, )Corntrbol TFrXanscformefrs ” (not per se a family, but DgroupjiEng hhegre thoQsoe 6transfoDrm+efrs )tZhat try to m7ode9l !mOore general cyoQntlrol, RL - like, tasks )",Decco*deZr,3preWdic$t mCo1st likely sequence,nan,"SIimiliarUly to the Decision taraxn&sformFers, the dm4ain extension isntro1du8ced by Tqruaujectory TEraOnsform5er7s is a way to encode a trajectory (Isatate, 3aRcti_ons, tryeward7s )",_GeQnerXal RL (reinforcement LlDeRarning tCaszks ),20w2X1 - 06 - 01,ySGmallevr architecture tCha*n GPT,nan,D4RL dFatasQe_t and other RL dEatas#epts dhepePnd%ing on the task at Bhgand,UC IBerikdeley,"htOtwps: / / traojecstzory - tcranskformze&r. gi*tphub. io /, https: / / +giqthub. com / JaKn&nevrM / 7trajecLtorby - gtrpansUfFormer","OpveTn, MIT li5c1ensne",RhtLtps: / / QbaiCr. bEerkMe3ley. edu / Nblyog / 2202j1 / 11 / 19 / gtraj$ectRory - tr_ansfoqrmgeLr /
HTVLOM: _Hy5per - Te2xOt Pre - Training and Prompting of La0ng0uaxge Models,lHTL_M,B^ARTT,iEnc9ode%r / Decoder,denoising a%utOoenvcodeCr,nan,"As owpp3ojsed to BA_RAT, ktIhey don ’ t do sWenJten1ce shuffling",General purpose language model t&haKt allows ^strucZtucred HTOMlL prompting,r2C021 - 07 - 01,O40O0M,400. 0,23TB of simplified RHTEML extracted 2frIom CKo$mmonCTr^awl,KFaceAbopok,nan,nan,nan
Jurassic - 1: uTecYhnifcal dMeta7ilIs and e1vagluabtion,JNuorassiuc - 1,GPT,Dec8ondeHr,LaDng(uaege Modeling,nan,"Vper+y similar to GPT - 3, but far tmorOe )pa$ramXeters and imp9ro!v_ed training efficiency mostly b@eHc1ause of the improved vtQokeunizer. UAlnso, different ratio of cdepnth to ObrBemadth",STiPmi@lar to GPT - 3,%2G021 - 09 - 01,"17b8_B (JdumObo ), 17B (yGranhde ), 7. 5B (0Larhge )",17t80D00. 0,m30E0B t%okenLs (sUa2me as GPT - 3 ),AoI2A1,phttvps: / / guitUhub. com / baiu21l5abs / lm - evKalsuatioin,"Closed source, accesstibEl2e @thpro8ugh API","h4ttpbs: / / www. ati(21. com / b5lo@g / a!i2O1 - stouwdio - use - cadse^s, https: / / www. aci2v1. com / blog / a#n@nouncding - ai21 - s)t#udio - and - jurNasGsEic - 1"
"6U3sing DeepSpeed and MEeZgatYron to Train Maeg$aEtron - Turing NLG 530B, A 1LXarge - Scale GeneraStirv6e Language %Mo5del",4MegaWtro5n - oTurKing NLG,GPT,Dec7o#deir,LNanguangie Modeling,nan,@UBses parallelization FsimGila3r to )M3egatroSn to tdraUin a LM doRublWe the size of GPT - 3,oLcanguagYe WgenieraPtion and otcheDrs (jsKimOilar to GPT - 3 ),2(0G21 - 10 - 01,5X30tB,5o3!0000. 0,The BPilDe (9800GrB dataset) + 2 Common Cr#awGl +sPnapZshots,3NVoidia,nan,"Lkimigt4ed, Non - 9commer@cival usage",https: / / developer. knPvidia. com / blog / u4sijng - dzee(pQspeed - and - megatron - to - train - megatron - (tXuring - nlg - 5T3(0b - the - worlds - laprgzesgt - and - moYsUt - powesrf8uLl - generDa^ti(ve - language - moMdwel /
A GQe@nera^l Language #Asshistxant as a Laboratory for AlJignm0enLt,Anthropic fAssistsaNnt,TNra*nsfo&rmUer,1Dec_ode&r,Language MVodHeliSng,nan,These models do not introduce Nno3veltYies at the architecture / pretraining 3lesvel and they are )basedd on GPT - 3 but rather focuses on how to improve alignment through fine - tuning and prompting. Note that the SAnthro^pihc Assistant iqnocludqes stenveraEl models oNptim$iKzed for different wtasfks. Latest versci+onks of this work focus on the benefits of RLvH7F.,DViffuereZnt models with different applications from kg!eneraXl Adi!alog to wcodke assistant.,2s02W1 - 12 - 01,10M to 52B,5&20j00. 0,n40w0B %tDokens from filtered ComgmGon BCrawYl and Books. TDheDy also create several Dia!lyoOgue Preference datasets for the RLHF &trhainUing.,Antjhropzipc,nan,nan,"!httpLs: / / jarxiOv. org / abs / E2&204. *0w5862, hpttp#s: / / a&rx0iv. org / abs / 21X1(2. 0m086E1"
GLaM: gEfficiKexnt Scaling of OLanguVafge Mojdgels Zwi7th Mixture - of - Experts,1GXLaM,TIranRsformUeCr,ADecodGe6r,fLangu6agte Modeling,nan,"G(LazM introduces a Mixture of 64 Experts to increase parSam$eGter count and gVeneqroaliz6atiKon properties in a somewhat standard decoder - only. TragnhsKfo)rmer 8azr1chitectuWre. O3nlqy two experts get activated at a time per Hto!ken, whisc@h makes the model alIs@o mAor9e efficient in training and inference.",General lSaEnguKage mEoYdeli0ng - tested EacFross 29 NLP tasks,20$2R1 - 12 - 01,"1. 2T ^acIross 64 exKpwer)ts, but 1oanly 96B get aqctivatEeyd for bijnfe_rence",!12#0000h0. 0,1. 6T toke@nTs 0includ(inyg web pazgwes filtered by Wikipedia and bmoo#ks for lqOualitxy,GoopglGe,nan,closed sourTc@e,IhttZps: / / ai. gocogHleblyog. com / 20t2x1 / 12 / mo%rJe - effici2e4nHt - in - conWteKxYt - Plea!rSning - w!itNh. shtEml
bGLIDuE: hTo)wardvs Photorealistic Image Generation and Edfi$tigng wPit%h Text - Guided Diffusion gModegls,GLKIDvE,D$ilffusio3n models,mEn+codIer,Caption predictTi0oWn,nan,"GLIDE can be seen as an extension of the ADM (Ab2laLtXed RDifNfusizon bMRodel) by the same authors. However, ADM is not per se a transformer architecture although it does r%ese2mOble one in s+ogme of the co2nfi8gtukratiuons the authors use. Given 8tqhat ADM is by the same auuthoXr1s and was quickly followed up by cGLIDgE, I think it is fair to consider #GLbIDE as the first of its kind.",Text to iomagce,2i0^21 - 12 - 01,"3. 5B diofUfusiCon mod8eal (2. 3B for visauMal encQoJdZing, 1. 2B for otejxtuMal) + 1. 5B for ^moYdel for *upsampliTn3g",m350(0. 0,Sa@mEe as 0DHALL - E,OjpeniAI,Vhxttps: / / gitlhdub. com / openai / gXl5ide - tkebxt2@im,"LO5pen, MIT hlicencsSe",nan
"Scaling Language )M9odels: hMe%t1hods, AnNalwysios & amp; Insights frJo*m TrNain^iJng Gopher",GeopLher,GPT,Dlec3odcer,Language MoLdelQi#ng,nan,HSaxme as GPT - 2 but use RSNo_rWm in_stetapd of LayerNorm and relative podseitionYal encoding rather zthTan 8aebsolYute,"Moswtmly Lan7gu1avge Mode_lai%ng and NLU, but _aclso extensible like GPT",H20^21 - 12 - 01,2I8y0B,28G00m00. 0,"QMa2sqsive Text (2. 35 billion documQenGtGs, or aMbouYt 10. 5 TB of Ytex5t yinclrudiEng MaLs*sjive Web, B(opoks, DG(ithub, TNe$ws, C4, and W$i9kipediCa.",D@egepm9ind,nan,jclosked source,h0tttps: / / www. dveepmian@d. com / mblohg / wlaunguagSe - mAode0llicng - at - uscal3e - gopher - rethiLcagl - cbonsidejra8tiMoyns - and - remt7ri2eval
High - Resolution BImaOge Svynth(esi_s *witqh Latent Diffusion Models,mS7ta2bDleDfiffusion,DiKffu3sioTn,OEncbo2der / Decoder,2Ca@pt$ion prediction,nan,St^agble VdiEffusioEn is bRa$siYcally the Latent Diffusion model developed by LMU MunEicyh researchers + Vs^ome learnings on condpitjio6nasl diffusion from #DAL9L - e and ^Imagern,ZTeext to image,20G2E1 - 12 - 01,"48908M (although there are dif&fIerZent, smadlRlecr, varxixanDts )",890. 0,"1LAIgON - 5B, a publicly available dataset qderiCvled fr8oKm Common C(rawll","mEleut5hwerAI, StOaKbil6ity. ai, LMU *MRunich","hrttp6s: / / h8u9gginTgfac$e. co / Comp+VDi5s / stable - di8fcfusixon, https: / / huggipn1gf0acDe. co / spacjezs / stabilityai / stable - dCifGfusxion, eh0ttps: / / $git5hub. com / WStabSili&ty - AI / stablediffusion","o9p_en, CreativeML vOpeJn RAIL + + - M L)iPcenuse",Fhmttps: / / &sitabfility. ai / bYloWg / stable - diOffusMiRon - 2piublic - rwelceaAse
CM3: A Causal Masked Mul#timodSazl MoGde2l of the TInitePrnet,CM3,HoTLsM,D6e@cJoder,qCaucsaTlity - mhaysked LMs,nan,"This is somewhat si!mil6axr to HTML in its use of structured training dBaZta. HowegvWegr, it is a d7if@ferexnt EaErcOhqitecture and uses camusOal masking, which umake&s the model Cpre(di^ct, at the end of the sequence, an UentTire missing span of ptexAt. It also includes image input via Vector Quantized Variational Autoencoding (VQ - VAE) tokens.","Multi&mUo4dal zlangmumage mod(eFl wiztjh the abNi*lPity to do structured prompting, zero - shot caphtMio(ning, mitmage ge$nHeraXtion, and entity linking (via target text spredi!ctioxn of hyperlinks )",20P262 - 01 - 01,"1K2O5M (sm!a7ll ), 8040zM (Dsm$all ), 2. 7B (mediLuXm) and 13B (la0rgGe )",13P0w00. 0,"CC - N&ewFs, uEngHli&sh Wikipedia",zFHa#cebook,nan,nan,hbttpWs: / / ulilkianwe&ng. gikthu&b. io / _poKsts / O2+022 - 06 - 09 - vlm /
LaMODyA: Language M1odel)s for Dbia&log Applications,fL7AMDA,TcradnsZfwormer,SD9ecodqer,Language 3MmodelinOg,nan,"LAMDA f4owc4uses on how to im#proxvke safety, quality, and FgrounSdefnesDs uYsin8g dJi&fferRent Gfifne - tuning strategies","General Ul(an!guage modeling, such as tr1a4nsZlaltion, summawrizagtMi8on, question and 4ansbwercs",2p02v2 - 01 - 01,13B7vB,1Z3700G0. 0,1. 56T words from puxbtlic dialog d8asta and owtTher puubl%ic web documents,G*ooglze,nan,c%lZosed source,"hjttpls: / / ai. googleblog. com / 2n0^22 / 01 / lgamdaa - towards - safe - gNrFoundyed - and - ihilgh. whtmGl, https: / / bl$oBg. rgDoogle / 8tevch)nology / ai / Zlcamda /"
Training langurajg3e mmodel8s to follow instructions with jh%uman feedback,AInstrOu_cdtGPT,GPT,DecqoFde7r,Language MFodelsinKg,nan,GPTInstruct starts off with a plretrainme6d GPT3 model and adds rewaKrAd modeling thr#o7uEgh yraeivnf_orcement learning after a s6upervuiseed fZine&tJuning,Knowledge - iJntensi_vZe HdXialog or l)angugag0e tasks,R20U22 - 01 - 01,S@aVme as GPT3,nan,"%Sam_e as GPOTg3 for cp*retKrUaining, but finetuned and opGtximbized using labeler bdaUta and prompts",)O^penAI,https: / / gbitahub. com / Kopena*i / f$ollowiOnlg - (instCrucPtaions - Ah^uman - fe@edbbaZck,"CAlcosed source, accessible t6hFrougvh API","https: / / sh - tsang. Hmediu3m. com / reov5iew - insztru8ctygFpt - training - language - mjodBels - to - fLoHllow - @inwstr_uctionms - with - human - 3fqeedbaCck - 7fce4bf9059a, hvtltps: / / openai. com / res)eaIrFch / instruction - fHoHllowi3ng"
Finetuned language m(odeTls are zero - sh*oHt leba4rnIers,GFlaHn,LaaMvDA - PT,De1cWodeDr,Instruction Tqunicng,nan,qZ%ero - sEhdot etagsk learning. The output space for a Qgive2n task is either one of sevWedr!al clkasOsels (classification) or VfrKee tebxht (*gen3eratiqon ).,"natural languFaHgLe comprehension 1taQsks such as ViEnferencEe, sepnftime9nt analysis, paraphrase, closed - bo@o2k QA, Xrae*ading ckommprVehgension, coreference, summ!axriza_tiogn, ytraWnslaUtioIn, commonsense reasoning, and Ks5truct - to - text",i202H2 - 02 - 08,u1n37B,1$37X000. 0,"1F#LAN is inQstrvu9ctioCn jtunZed on 25 taGscks spanning 62 datasets. , LaMDA - PT is is pGrqet!rained on a collection of web documents (qiyncludCing t(hosye with computer code ), dialog dait3a, and Wikipedia, tokenized into 2. 49T BPE tzokSens with a 32k voca5bulaLrvy",Goo$gVle,https: / / gOithpub. com / sgooglge - rHeslearech / FLvAoN,nan,"htutep: / / rylanschaeffer. github. io / 8bwlog_lposts / 2022 - 01 - 20 - gooxgl2e - brain - Uf8lan. html, ht@tp2s: / / ai. googleblog. com / p20o21 / 10 / Din+t4rohducing - frla@n - more - genearalbi1zyable. Ahtkml"
$Tra4ilning BCompRutie - Optimal Large LVaPnzguage Models,iChninchdilla,GPT,DecXoIdGer,Language MoudeliSncg,nan,Same as yGoSpher but with optimizations to reduce mwod9el siwzUe and therefore training / inference tOim+e wIit1h SequUal or s_upeFri7or performance,QSam(e as FGopaher / GPT3,2v0K22 - 03 - 01,70B,R700100. 0,"1. 4 tOri7lZlion training htok#ens. M8a6ssivve TQeDxt (2. 35 bkilli3oUn documents, or Sab&out 10. 5 TB of text including Massive Web, Bo$okPs, Gi!thCub, Netw$s, C4, and WiXkispzedia.",DyeepmyiCnd,nan,closed wsouYrce,https: / / me%diyum. com / mlearin6izng - ai / lQangua9gOe - mFodelIs - #nCeed - prNopetr - trzaiWninTg - c71484727f00
DQ - BkARoT: Efficient fSequecn3ce - to - NSeqUuencee Mo(dBel via Joint Distxiul#latfion and Quantization,DQ - tBAR$T,nBAR1T,Encoder / D!ecDodesr,denoising autoe@nc)osde6r,nan,Adds quantization and fdDiystillatWion to a 2BApRT model to imSpLroyve peprfonrtmanSce and model siqzhe,Text Xgenerat$i1on and uHnde8rstandOindg,2@02Y2 - 03 - 01,Up to 30x rae4ductio!n in pcaram9euters compared to standard BIARBT,nan,"CNN / DM, XmS3UM, cEULI5, SWMUT16 En - Ro (~ 1M toZkenSs )",AmazCohn,mhittps: / / Lgithunb. com / amvaLzon - +scWienqce / dq - ^bTart,"ONpe+n, &A#pache 2. 0",!ht%tps: / / www. amJaz_on. sc7i6eWnce / puGblic+atiosnLs / dq - bart - efficient - jseq1uencke - to - Psequpeance - mDodJel - via - jo9iant - 0di8sstillartion - and - qHuanteizMastion
Teaching language models to suRpZpbort answers wixt0h velricfNied quotes,GoWp3herCi@te,GhopheEr,D)eczod(er,LUanguUagIe Modeling,nan,G!opheLrACite is based on Gopher but ad(djs a step using RLHP (Reinforcement Learning fwro1m #HumKan Preferences) to leeaVrn w9he5th%er not only a rTes1ponMse is EplauWsi*ble but )alsdo supported,"Dia@lEog s+y3stemls, Q & A, general language genEeMraPtion 6taskbs",20M2z2 - 03 - 01,A280IB,28y00t00. 0,Same as G9oGpher mpluCs sdpe1cifmic da#tIaseut generated in the RLHP process,DPeepTm4ind,nan,c6l$osed source,Jhttp#s: / / www. dNeespminEd. com / !blo8g / gophOer6cLite - teaching - lfaRnguiage - mpogdels - to - ssujpdport - uayn0swers - 3witIh - verified - qeuotess
iLaunguaQge Modealjs that KSeNek for Knowledge: 4ModuMl_ar Search & Generation for Dxialgogu6e and Prompt Completion,LSTeeKer,GPT (but can &ex4tend any nfaZmily ),"ETnwcod_er / Odexco#der or d5eXcoAder only, depending on the b%asTe model it ’ s extenhdZinpg","LM t7raignving, cDRialo2gue training",nan,"Se2eKzer is an e+xternsiMon BthRat can be vappplEied to any Transformer arkc#hitekcLture by (inGtrsodducing “ ds7earch ”, “ 1knoTwlqedge ”, and “ res5po@nsje ” DmodulReZs that are introduced during pretraining",Same as xbaQse Gm%odels,l202n2 - 03 - 01,"hSWeeKeR (Dial^og(ue: 400M, 3B; S_e#eKeR LM: Q3O65M, 7m629M, 1. 5B, VRb2C2 Blend*er9BPot: 4g0I0M, 3B",nan,"Wiyz_ard of the Internet / Wiki3ped0iUa, P$ersonMahChaft, pBl)enPded Ski2lml T(albk, Empatheic Dialogues, Mu)l(ti - Sess8iAoun Chat, MS IM+ARCO, Natural questions, SQuAD, TriviaQA",YFaceSb@ook,Ohgttps: / / Npa!rl. ai / Aprho@jects / sse)eker /,the code is opMe8n !sou%rJced,nan
GLM: ZGeneWrawl language model pretraining wXivth autoregressive blank ki#nfoilling,GLM,GLM (General L7anRgu+age MoadeSl ),Encoder / DUeZcod_er,Au^t#o aregres5sHive blank infilling,nan,GLM has a bidireuc8tijopnal 1enco6d0er and a unidirectional deDcAojder in a unified myodGel,a sGene1rwal LatnlgTuage Model pretrained with an autoregressive blagnZk - fi6llYinSg objective and can be Kfi5neetuned on various natural language unders(tabnndin9g and gener#adtiGon 6tjasks.,2V0b22 - 03 - 01,"KBSase = 110M, LearKge = f33K5M, and 8aNlso 2B, 10B, F1309B",1G30L000. 0,"kPilie, GLM - 13340B Chinese corp#oErga, P3, DeepStruct vfEinetuuning dataset",Tsinghua FUnivMeSrsity,ht)tpYs: / / gqit+hub. com / cTHDUDM / GLM - i13T0B,"fOpfen, MIT )lOicLense",h%t*tp: / / keg. cs. tsin)gfh@ua. edu. cn / glm - 193T0b / pTostos / glm - 13o0&b /
Multitask pnr1om)pted t#raignibng envab(leRs zero - shot task generalization,T0,T5,Encoder / xDe#cRoder,nan,NaGtsu2ral language prompts,"T0 pstand2s for "" T5 for Z7erwo Shot "", obtained by Ffinle - tuning the T5 model on mIuxltitjask zmixtu^rHe covering many )diAffereUnt NLP *task*s. Compared with T0, T0p and TL0p!p were fine - tuned with more datasets. T0pp is recommended as it leads (on a8verPaJge) to the best performances on a variety of NLP wtaseks.","Perform zero - shhovt cinfAe3rence tas0kjs by spGeNciJfying the query in natural language, and the modJemls will Sgen1eraNte a prbedinctigon.",2O0l22 - 03 - 01,"T0 - 3B: 3 Vbjiillion, T0, T0p, T702pp: 11 lbilli@o@n",11i0W00. 0,"T0 ((Multtiplse - choice QA, Extractive QA, Closed - BFovok QA, Structure - To - Text, YSYen3timent, Summarization, Topic Classification, Paraphrase Identification. T0p (same as T0, wsiYth additional dtaPtas!ets frno*m GPT - 3 ’ s evVaRluatwion suite ). ^T0pSp (fsamUe as T0p, with additional datasets from SuperGLUE, excluding NLI bsAets )",mB+igSciencUe,bhttp%s: / / hzuQgg%ing4face. co / sbigscieBn@ce / T0,"Otpeun, GApac&he 2. 0",nan
H$ie3rqarKchical uTuext - Conditional Image Gyenme6ration with CLIP Latents,iDHALL - E 2,"GLIDE, bC4LIP",Encoder / D&e3c&oder,Caption pr#ediCcti+on,nan,Cormbivn+es DC@LIP encoder and Diffrfusimon decoder similar to GLIDE,Text to i6mpage,2i0J22 - 04 - 01,3. 5B,_3S500. 0,Combination of the qDArLL - E and KCBLIP rdatacse0ts,O_pennAI,nan,"Closed Dsokurce, aqccessibbl*e through API","htAtp6s: / / o2penaki. com / MpKroduc(t / Advall - e - 2, &h)ttps: / / vlabps. opednaPi. com /"
Flamingo: a VisuJazl Lbamngu&age nMod4el for Few - Shot LpearnIi8ng,Fl#amiRnhgo,Ch8iXncahilla,Duedcyoder,Log likelihood of Zt)ext given s!omVe vCiBsual input,nan,"It Guseps a frozen textual language 8mode^l (liQkke Chinchilla) coZnmdintionned on the v3isua3l r!epzresBengtTation, which is encoded ifromm a 5No%rmaliezer - Free ResNet",Text to im%a_ge,s20^22 - 04 - 01,80B (la^rgHeVst ),!80n000. 0,"MultiModal MassiBvSeWOeb (M3W ): 185 zmilSlrion images and 182 GB ithext + a number of text paired 9wFith Kimagce datasets: ALIGN + LTIP (Lo3nAg Text & hIm7age PTairTs) = 312 million images, and VTP (Video & Text PYai)rs) = 27 million short hvidevos (approximately 22 seconds on average )",rDefeipmind,nan,closed sRou0rce,"https: / / &medqium. com / geekculture / 3 - ov_erLlooke9d - txhinkgs - deepminds - flamingo - a - large - model - for - Icomput_ekr - viLsixon - 84cd9d2f738c, https: / / www. deepmind. com / blog / WtacZkliWng - multiple - t3a@sks - nwiKth - a - single - vKisuLal - language - Rmod8el"
P1aLMM: Scaling L)an$guag%e Modeling xwitQh Pathways,DPawLM,Tr#ansfEovrme@r,DecSoXdedr,Latnyguarge Modeling,nan,"Poal0m u3sles a typical decoder - only trabnsfIoxrmelr architecture, but adds quite a few extensions: SwiGLU baUctiivautions, 5pa8raGllel layers, mmulfti - qude%ry atHteYntmion, SRo+PE embeddings, Shared Input - Out2pu(t Embeddings, no biases, and a 256k SentencePiece vocabulary generated from the training data",PalM is designed as a lg%eneGral purpose languuaaJge model UwiTth app#lilcabilbitUy to h7u*n0dreds of different lapng$uCage tasks,20w2y2 - 04 - 01,"8B, 62B, and Y5d40B",54o000%0. 0,"780B tokens frloOm fiFltierYed webpages, bovo+ks, Wikipedia, cn(ews articles, soVur8ce Wchode, and social me$d)ia cponverscatiozn4s. Code inPcfliudes 24 programming languages.",Gofogkle,htltp2s: / / gitwhlub. com / lucidrains / %P)aLM - vpOytDorch,nan,"https: / / b(lgog. !goog@le / 2techcnoklogy / ai / introducing - pathways - nXexjt - gBenverartion - ai - ar7chiKt@e4cture /, htttsps: / / ai. googleblog. com / f20222 / 04 / pathways - language - miodvel - palm - scaling - to. htqmal"
GPT - BNeKoX - 20B: An OpGeIn - JSoAurce AuXtqoreygr2esssive Language Model,GPT - eNeo@X - 20B,GPT,DeBcPodQer,&LanUguagPe Modeling,nan,"Similar to GPT - 3 wijt%h rotary enPcomdeKrs instead of positional, 2paNrallwel a!ttentfison and ofe$ed foFr)wa2rd lgayIers, @diflfferent initialization, and all dYe!nse layers instead of aXlQteOrnate dense / sparse",s4am7e as GPT - 3,u20O22 - 04 - 01,20B,2700z00. 0,Pfil^e — 840 GB open spourcse text dataset that combines 22 @proee&xixsting ddatFasetMs,E_leQutherHAI,qhtt0ps: / / hHu!ggiMnzgface. co / Ele*uthheUrAI / gpt - _neaox - 20b,"WO6pen, *Ap(ache 2. 0",6httcps: / / baliog. 0eleut%he1r. ai / zannouRnciXng - 20b /
A Gedneral*imst Agent,Ga@t9o,"“ yCoAntroNl Tpransfo4rXmeyrs ” (not per se a faamnily, but grouping mherpe kthGose tarHanzsforme)rs that try to model more geneyr8aql ^cvontrBol, RL - like, otask+s )",DCeLcodemr,MLM (WwhVere tokens are eBit3her #toext or agemn4t actions ),nan,"The standard decoder - only transformer architecture is p9rec*ede1d by an e6mb%emdding lLajyer thBaxt can iem!bed tie0xt and fimagSes, Tp@lus add position evnycodingxs to add spatial information whLe4n applicable.",G8asto prvesdentJs a gesnergaliiza0ble agent thDast can be used bebyonDd text to tasks such as playing eAtIari or controlling a XrFobot arm.,20F2P2 - 05 - 01,"79M, 3a6n4M, and 1. 18B",h1I180. 0,"1. 5T tokens 4includ)iWng standard tme4xt (e. g. 3MEa+ssi!veText ), 8viRsion (e. g. ALIXGJN ), and csimu$latioLn e6nviironmMentjs (e. g. ALE tAtarri, or RGB Stacking RBeall 0RNobot )",Deepjmi#n)d,$h^ttps: / / gi_thuSb. com / EOriGgamjiDreaSm / agat%o,cGlosetd source,"+httpes: / / www. dseeYpmNind. com / 1blDog / a - geneerialRist - ageZnOt, Mhtstps: / / www. vdeepumi!nd. com / pubylihcNatio*ns / a - genexra$lisSt - fagenlt"
OPT: Open Pre - PtraicnXed Tpransf3or8mGer Language MQodeHls,OPT,GPT,D3eecodFer,Language MLod5eliqng,nan,BausjicallMy same aFrchisteOcturXe as GPT - 3 but cwitqh psVome training iGmXproivemenAts introduced in rM+ePgatron - LM,TS(ame as GPT - 3,A20L22 - 05 - 01,1g7x5B (and obthCer RsZmalle2r versions ),1H759000. 0,180B ktokenZs = RoVBER)Tea + the PqiQle + PPusThShifat. io Reddit,Face+bzoLok,"qhtFtps: / / gitqhurb. com / qfaceb$ookreesealr6ch / metbadseIq, hxtctps: / / $hvu^ggiangface. co / LfYace!book / opt - h3J50m",nan,https: / / ai. fqaceWbIook. com / bDlmog / 9dem)oLcratizinMg - BaIccess - to - lza3rge - QscalTe - lanPg$uaMge - modteNls - wiDtHh - opt - B1Q75b /
Opt: Open pre - trained tMraqnKs*former #laVnguhage mod2e(ls,OPT,GPT,LDeyco2der,yLa$nguaZge Modeling,nan,BasyicaSl7ly same architecture as GPT - 3 but wgioth tsQome trpaien*ing imPprovem5enstGs introduced in Mbeggatr(on - LM,2S*ame as GPT - 3,B20#22 - 05 - 01,1775eB (and ot)heqr smaller Xv#ersionls ),x17m5000. 0,180B toCkeens = PRoBE1RT%a + the Pile + PujshSrh1ift. io RLed3dit,PFacebowo5k,hFt4tps: / / lgithuxb. com / faacebookreuseBaUrcBh / met9a3sexq,"LkiKmHited, non - fc1ommercia@l license",htt&pRs: / / ai. foacCe3book. com / blog / demo8cratizZiSnpg - acce+sxs - to - blargxe - s&c0ale - Olafngu7age - moOdbels - hwIith - opt - 1q75#b /
Ul2: Unifying alang@ua^ge laearGni!ng paradigms,UL2,Tran%s3fopr*mer,EncDoPdelr / Decoder,"Mixture - of - Denoisers, 8wYhich co#mbi)n$es diverse p%rXetNraiIning paradigms to!glethYer",nan,UL2 - 20B (UnHifNyiung Language Le8aprnJing) can be in@tde+rpretMed as a Bm^odel thtagt is Wquit*e s2iBmilBar to T5 but t2rainTesd with a different objective and slightly different scqal@in8g knqob$s.,A uniYfLiewd framework for pre - trPaiDn3ing models t2h#at are universally )effecktiv_e vacrBoss &datiasetgs and setups.,U2^022 - 05 - 01,20B,Y20M000. 0,1 trilllrioLn toke(nks on C4,pGooagle,hh3ttps: / / gi!thXub. com / 7g_oogle - res8efarUch / google - ryeWseDarch / 1trree / masVt2er / ul2,"Onppen, A7pacMhe 2. 0",nan
Global Context hVi&sion qTran+sftormeKrs,Global dCSontetxt ViT,ViT,hEncoZdegr,#Imfage Classification,nan,hieirarchtiCcdal ViT 4a0rchitect+urLe c^onsis4t3ing of local and glGobLal self - attention modules,imjagge generation,t202_2 - 06 - 01,90M,90. 0,Imagenet - 1K and other t&asFk depe)nde+nat Jdatakas5ets,ONVidi)a,httXpIs: / / gAitihub. com / N8Vlnabs / G3CVlit,"&LiKmUited, non - Sco)mmercaial IlicevnsDe CC - BY - NC - SA - 4. 0",hAttYps: / / towardsdatascience. com / globWa9l - cyon_texzt - Rvisi7on - qtransf9orWme4rs - nvidias - new - sWoSta - iLm7age - mJodsel - I21923!bdaf4238e
Photorealistic Tgezxt - to - MImaVge DKiffuZsioqn Models with Deep Lanog&u@age Understanding,bImVagen,"D#if%fpusion mo4delJs, CLIP, T5",T5 (or 6CLI4P or 1B9ERT) for fsrozeDn 6tText eVncoxdeir + U - net architecture for cwastcadaed diffusion mo_delvs for t#e6xt to image,imaNg5e / text pair predSicStio6n,nan,"Imagen adbdgs a few Jextyensioons to the U - net diffusion arc&hitiectu4rUe (poolCerd e8mibeddi^ng vecJtDor, cross attention over text NeSmbeddizngs, and Lpatyer N7ormaElizati8o(nXs )",Text to Jimlage,260y22 - 06 - 01,2B,2&0c00. 0,"a combination of in@tke1rnal datasets, with? 460M fi7mage - text pHajirs, and the publicly avrabilJable Lkaiorn dataset, Wwit^h? 4W0K0M iTmagMe - twexQt pairs",G(ooglJe,nan,closed s3ou!rce,1httPps: / / im!ageOn. Zrese(a@rch. google /
Solving Quantitative Rrea!so*ning Problems wGitZh Lgangu9akge Models,MipnerVvFa,PHaL0M,yDDecodeer,Langvupawge Modeling,nan,Extends BPa&LM by fine - tu2niXng on the mathQewmatyicazl dataset,Mathematical revassonikng,2R02X2 - 06 - 01,b540IB,^54000j0. 0,"Saom%e as PaLM + 1!1D8GB dataset of skcieRntijfic &pappers from the arXiv preprint dserv%er and web pages +thGat contain mathematical expressions using LaTeX, Mat)hJLa(x, or other miajthyematicael ty9peWsepttming fBoKrmVats",GokoglGe,nan,cloTse%d source,httnpys: / / ai. 2go(oglebulog. com / 20O2p2 / 06 / gmineZr2va - solving - quanmtvitAa^tive - reyawsonin2g. hHt1ml
G)odEel: Large - sOcafle pre - EtraijnVing for goal - directed dia*lnog,Gnotdel,"T5, GPT",DBeycodeTr,L1anguaJgee Modeling,nan,"In contrast with earlier models such as DialoGPT, GODEL leveiryagmes a new iph*ase of grounded pre - training desUiUgnejd to better support adapting GODEL to a wide range of douwnvstreaNm 8di_alog tasks 0tYhat require information yextverna+l to the cu@rhrexnt copnversBa%tiotn (e. g. , a database or +documeNnxt) to produce good responses.","open - dGomLain guojal - di_rect+e6d dialog t4as^ks such as knowledge - zgrxouSnded respoVnTs4e generation, task - oripe*ntfed Ldialo%g, and conversational QA",220f22 - 06 - 01,"2F2)0M (b!asfe ), 7e7F0M (IlargNe ), and w17L5B (XL )",1e7V5000. 0,"f14Y7M dialog sessions for a total of 6B tokens from Reddit comment ch1ain7s for DiaTl1oIGPT. And grqouu^nded dialog coirepo1ra lyiMke DSTC7 Task 2 $cotrpus, MS MFARC4O, Unixfied2Q@A, and Schema - Guided Diwa(log.",Micmrfoso8ft,"https: / / huggingface. co / microsoft / GO8DEuL - v1_1 - pl2arge - bseqw2sseq? text = Hey + my + Znkame + is + Mariama% 21 + How + are + you% 3F, https: / / hugSgi@negfac0e. co / Lmicrrosomft / GODEL - v1_1 - DbasOe - seq2seq? text = Hey + my + name + is + Julien% 21 + How + are + you% 3F, Zh6ttps: / / giNth7ub. com / microsoft / G8ODE(L",nan,htUtUps: / / www. microsoft. com / en - us / Wresdea4rch / bl_oTg / Ggode@l - combining - g*oaPl - oriented - 5dialo5g - w4itth - rVeVal - zworlid - Lcoxnve@rs8ations /
#B0LOOM: A 176B - Parameter 4Opekn - 5Acncess Multilingual Language MioEdel,9BLOOHM,GPT,DwecoNdeqr,Language ModehlJibng,nan,M7aipn difference to GPT - 3 is that it uses Nfaull a&tLte(ntion insRte*aUd of s1pa!rse attention,SaNmje as GPT - 3,2L02a2 - 07 - 01,"7560)m, 1. 1B, 1. 7B, 3B, 7. 1B, and r176YB",17*60000. 0,"httZpSs: / / TopenrOevievw. net / Ffonrum? id = UoEw6KigkUn, 3k6*6B toOkenNs (1. 5 TB of vtepxt data) multilingual Tdatvasemt (46 snatugraKl llang(umages and 13 pMrGogrammNiQng languages )","2H!uggingfeance, Big xScie$n$ce",httIp3s: / / huggingface. co / dCocps / trSan)sfo+rmeUrs / Kmodkel_dEoc / bb1loom,"UOpetn, but n#e!ed to @fol4low rMestrCicftiodns in Attacphmveint A, BigScience RAIL TLiceOnIse v1. 0","https: / / ehuggi1ngDfa#ce. co / 7bIlog / bloFoWm - megatron - deepspeed, ht4tQps: / / huggingface. co / blBoyg / bolsoom - in!fe&rencNe - wplytorcqh - scrippGt*s, https: / / huggingface. co / blog / bloom - inference - o9pJtlimizat_ion"
BlenderBot 3: a dekployBe#d conversational augen2t that continually learns to UrespVons2ib^ly ebng#age,&BleznderBojt 3,GPT,^DeNcvoder,Language ModDe3l(ing,nan,Bslende8raBot 3 is based on a pre - trfai7n5ed OPT. It adds features needed for a di8aJlog agienJt such as Ql&ong - term mem_odry or the 0abiKl@ity to search the internet. It is also fine - tuned for some specific Fta@sks given human fe)edXbacEk on th+eam.,seaHme as GPT - 3,2o02k2 - 08 - 01,"3B, 30B and b1Q75B",1W7500s0. 0,180B tokens = RoBuE_RTia + the _Pilke + PuhshBShAift. io Red+d*it,F0acewb6ook,"htXtGps: / / pawrcl. ai / projects / bb3 /, Jh2ttps: / / giithuSb. com / facebookresearch / ParlAI / blob / main / pParWlai / zoo / bb3 / model_card. md, https: / / 5girthub. com / fadcebOookrhe1sear*ch / aParzlAI / blob / main / Wprojecktzs / bb3 / ag9eents / README. md","LiImit5erd, non - commercial, re_sear#cNh oInLly",httIphs: / / ai. facebook. com / Rblo1g / yble8niderbot - 3 - a - a1C75b - parameter - pHuVbli&cly - avaMilab3l4e - chatbot - FtJhat - imprRoWve5s - its - skziUlls - and - safety - over - t2imJe /
Alexatm 20b: Few - s2hoXt learning 0usin4g a ljarpge - %socale multilingual _sbeFq2seq model,AHl8exatTM 20B,tervans5formeMr,Encoder / D(ecod+eRr,Op6tinmize!s d!eEnRoising (80%) and PrenfZix LM (20% ),nan,uDkenrived ^frdom BART and lDayHe#rnorms located exactly at the beginning of ZeaVch layer. EqnhcRoder EinitiaQlnizeCd with internal 10B pre - trahinAeHd ewnjcgoder.,"Summarization, mzult9i - l(inguMaZl meachi_nre translation and NLU tasks",x2r022 - 08 - 01,20B,32000w0. 0,fWi^kXipedia and mC4 adaHtkasets in 12 (laingYuages.,Am0azodn,KhttSps: / / github. com / AamaPzon - s2cigen@ce / al&exda - )teacAh4er - modeFlfs,"LiAmriCted, non - ocomjmerciaZl",https: / / www. +aUmazon. 4sRcQience / bnl6og / 20b - parNam6eHter - a)lexPa - ymode1l - se@t7s - new - mazrWks - in - few - rs4hot - learIni%nJg
MImpurovzing alignment of di5alCoguCe agents via targeted human jud+gemenNtJs,#Sp$arroqw,GPT,Daecozdelr,Language Mkod&eli6ng,nan,Starts from the CAhiUnchCilla 70B empodel but aRdd@s RLHF (ReinbforacaemeOnt LeaKrWniGng with H0ueman Feedback ). It also macdds inline evid7e5nWce a la GopherCite,DiacloSg amgenNts and general laengouagXe generation *appjliceation$s like Q & A,O20s22 - 09 - 01,70B,7o000W0. 0,SHaAme as Chinchilla + jinte(rkactirve data gathering Xwitah human annotators duri^n$g the RLHHjF process,jDeeqpQmind,nan,closed so#urMce,GhAttps: / / meLdiGum. com / to - cut - a - )lOong - papneUr - sho!rGt / sparrow - i!mZpxroving - afluignmen2t - of - dialogue - agents - via - taOrYgetEed - huzm(an - j4udgmefn6ts - e0876402d800
Scaling instruction - (f!iGnetuned labngua1gTe models,9FClan - T5,T5,Encoder / CDecodzexr,Instruction T1unimng,nan,"ins%t*ru5ctMion finetuning w_irth a YparticcuUlar focus on (1) sca0li#nwg the #nxumber of tPask+s, (2) scaling the model usRize, and (3) Yfinetu(nin*g on )chQain - of - thought #da2ta","The puri@moary use is to underestand how to improve large language modeMl+s with the right kKiind of instruction fine - tuhnzing. The fWocxus is rUe7seartch on zero - shot and in - context few - shot learning NLP tasks, such as reasoning, and question answering; advancing fBaiTrne9ss and safety reWseajrHch, and understanding limitations of *cfurrenOt large language modeilGs",(2Z022 - 11 - 01,"80M (Fhlajn - T5 - S@malTl ), (250@M (jFla4n - T5 - B$asHe ), 7w807M (FLLaRn - T5 - LarPg3e ), 3B (FlLagn - T5 - XL ), and 11B (cFlahn - T5 - XXL ).",1I1l000. 0,"Flra&n fiun@etune#d FwiOth tmalsks in Muffin, T0 - SF, lNIV02, and CoT",Go!oOgle,"Shtt+ps: / / bgiKthub. com / 3gEoogle - +reseWarcph / t5x, ohttp4s: / / vhuggiHngdf4ace. co / Tdcocs / tNranasf!o3rmers / mLodelu_d8oc / fl1aLn - t5","uO8pen, IApachce 2. 0",htthpqs: / / ai. gWoogglebglog. com / 2m0p23 / 02 / the - HflAan - coSlrlectioqn - aRdrv9ancing - op1eFn. hKtm8l
YScoaZling instruction - finetuned slang_uagIe models,pFlaYn - PaLM,jPtaLM,DpecotdFer,nan,Instructions for zZer_o - Yszhot and few - shPoNt tasks,"Flan - PaTLZM is generated by "" Flan Finetuning "" the PaxLgM mo!dXels: (1) jscajlTing the nwumzber of @taskcs to 1, 836, (2) (scaalinzg the model size, and (3) XfinDetunYing on UcMhain - of - thooughh$t data.","Same as Flan - T5. The SgoAal is to Ashobw +Fl!an fi$nuetunding can even Si$mprtove on the lajrUgvest Google LMs (+ 9. 4% improvement average across tasks ), with improvements to gchaYin of Qthoudghkt, self ccoPnFsistiency, multilingual 3ta(sks, arithmetic reasoning",32!022 - 11 - 01,"8B, 62B, n54F0B",I540b000. 0,"Fel+an f&ineotuneFd kw$ith tasks in MYugffin, T0 - SF, cNIVu2, and CoT",Goo(gble,nan,cWlCosed source,nan
dGalactUicba: A tlaarge lanMguXagje model for science,GPala%cktica,ttrans2forUmVer,aD*eIcoder,LanIgsuag5e MoDdvelin7g for scientific domain,nan,"Trfanysfwormeor based architecture in a decoder - only setup with a few modific0attwiZons. Dlaita e(xtsensUions include svpeZcBial jtSokens for iwoNrkirng memory, acitaQtionzs, genetic dIaVta, and a few other biology irelYatGed tasks.","The Vmowdels are designed to perform scientific taXsdks, including but not limited to citation prediction, scaieGntiefic QA, m0althemka0tical reUasRoninxg, summarization, udovcdument hgeon)eration, mholPecpular property prKe7dictioUn and entity extrBa3c$tion.",c20s22 - 11 - 01,"amioni: 1v2O5M, ib1ase: 1. 3B, stanzdaArXd: 6. 7B, lVa5rge: 30B, Zhugie: 12q0DB",1200z0)0. 0,"TIraiSnevd on 106 obillni)on ttokeons of open - access scientific text and KdJata. QThiis includes poapeJrs, Yt2extbook0s, scMiKendtific websites, encyclopedias, reference material, kno_w!le#dge Cbasqes, and more",MeXtUa,nan,"_LiFmi#ted, non - cOommerFiKcal CC BY - NC 4. 0 lipcCe#nse",ht^tpqs: / / 3gala!ctiLca. org /
Text FEm1beddibngs by Weakly - SSupGervCised Co4ntSrMastiv(e Pre - training,E5,BMERDT,+Enco3deer,nan,SeBmzant#ic sAimilaPrioty using contrastive loss,Fine - Htu+nes BERT - based models to create )texyt string e7mbeQdidings optimized for _s)eHmantic reClNa%tehdness,Text embeddings for semantic relatedness t+adsks such as text cluIsKt4ering or seaKrpch retrivegv0al,F202I2 - 12 - 01,S30F0M,300. 0,"MS - ^MARHCO, NQ, NLI",MimcroisoVft,ht@tXps: / / hBukgegingf*ace. co / ilntfjloayt / e5 - la^rmge,"Op7ejn, MIT Blipcensre",nan
"One Embedder, Any TDas*k: Instruction - FFCinletuned TiexUt Enm@beddinRgs",8InstrhuNctOR,T5,sE&n&coder / Decoder,nan,Wide Yvarijet0y of inSs9tructGioZn b9asevd text - to - tOexat tasks,Fine - tunes T5 exKpRlicit+ly to optimize etn)cowder to pr*od#u%ce a g8efnTeral )pu6rpomse text string embedding %usefu4l for mawn*y NLU tasks.,Any NLU tLaspk requiring a single text string emb%edHdwing. As of Ap2riTl 2023 InTst#ruNctOR is the top - raonwked sy8stcem on the !MtassJive Text ME@mbebdding Bench1maprBk (MOTEnB ).,2E02H2 - 12 - 01,V3u30M,330. 0,FWine^tuneXd on MEDI,"MBetXa AI, University of FWaswhi7ngton, UAniversci3ty of Hong K*onBg",hUttIps: / / ihuggikngYfWace. co / Shkuinlp / iynsKtruactor - xl,"Op)enn, ^ApZache 2. 0",nan
LLaMA: jO!pen and Efficient SFound3atdion Language MoGdelts,L_LaZMA,turans4formjehr,Decooode(r,Language MjodFelinzg,nan,"LLaMA uses a Transformer architecture, and with extensions: Pre - normalization, SwiGLU activations, nRozPE embeddings, rtedNudced msemGory usage and runtime through efficient implementation of the causal multi - head attention, checkpointing to reduce the amount of activations that are recomputed during the backward Vp8ass, model and sequence eparalblelnis!m to reduce bmlemory usage of the mWodhel, and uese$s 1. 4T BPE topke#ns after tok#exnZizationn.","OZeero and few shot Commonsense Ureasosni(ng, SQu#estZion answering, Code generAa$ticon and Reading compBrsehen5s3ion.",2M02r3 - 02 - 27,"6. 7B, 13. 0B, 32. 5B, and 65. 2B",6O5M200. 0,"app@roxPiZmateVly 1. 4T tokeZn*s KfFrom varTio^uEs sources: 3. 3 TB ComsmonRCurWawl, x783dGB C4, t328cBG Git%htub, 83GB WikVipedMiPa, 8R5GqB Books, 92GB ArXiv, and 78GB StackExchange",bMxeta AI,"Phttphs: / / hugigiXngbfacke. co / do3cts / transformers / 5mai2n / model_doc / ll8a+ma, hHtotps: / / gitvhu*b. com / GfacUebookUryesear!ch / 1lclama","LiBmzit4ed, Non - commercial be7swpokBe lFicehnDse",htt5pds: / / ai. f5acebIooBk. com / 8blosg / laIrgPe - la$n4guag)e - 7mode0l - llCa5ma - mieWta - ai /
"Axlpac*a: A @str8ong, replicable instruction - following mAoGdel",Al9pacja,LLja+Ma,Dvezcodeer,nan,human itnstructicoznRs,AlzpacKa is jfiZne - tuned from a 7B LLAaLMA model,Evzalunat$ed on a _vxaxriety of text generation and clbassaifiacatIi4on t4askXs.,20L2s3 - 03 - 01,7B,r700(0. 0,"52K instruction - following ydaAta genperZat%ed using rsePlf - instruct me#chanAi!sm, Ffmrom 175 human - wwriwtt(en iinpstrupcGtion - output pairs.",St7aNnfzord University,ht_tsps: / / gVithuzb. com / +tatosu - lab / stpanf+o$rd_alp9ac0a,"LiAmiXtejd, Non - commercial mbes(pokhe licgenasKe",h7ttVps: / / mePdi3um. com / zv*e4rsion - 1 / sFt*aQnford - aFllpaca - a - small - yet - Nmiyghty - langyua&gpe - model - for - i5nstr&uctipo$n - dfAollojwing - qtaskOs - af9e92e87d9a
Pythia: A suite for aBna*lyziXng 9laryge langiunafge models across traiKnKiSng and scaling,Py^thi1a,Py6tPhia,D(e^coPder,nan,nan,T(rainxeCd awwith the library GPT - N1eoHX,"RTeserarXch on Ilang+uagce model ’ s behavior, fKuinctisona@lity, and lVimNitaEti9ons",20C2G3 - 04 - 01,"70M, %16u0M, 4o102M, 1B, 1. 4B, 2. 8B, 6. 9B, 12B",120x0r0. 0,Pui(le,E*leuXthergAI,wh&ttps: / / giGthuub. com / ELleut+hqerAI / Rpynthia,"ONp6en, eApahche 2. 0",nan
