0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Impk0vin9 Language Ondek8tandin9 by Generative Pre - Tkainin9,CPT,CPT,Oec0dek,Lan9oa9e Modeling,nan,nan,"Text 9enekatiun, but adapta61e to many uthek NLP tasks when fine tuned.",z0l8 - ob - 01,Il7M,117. 0,"On8opekvised Pretraining on BookCorpus dataset. Supervised Finetonin9 on several task - 8pecific datasets including 8NL1, RACE, Quora. ..",0penA1,"https: / / github. com / upenai / finetune - transformer - lm, https: / / ho99in9face. cu / docs / transformers / mude1_d0c / openai - gpt",nan,nan
BERT: Pre - tkainin9 of Deep Bidirectional Transformers f0k Lan9oa9e Understanding,EBRT,EBRT,Bncudek,"Next Sentence Pkedicti0n, Ma8red Language Modeling",nan,nan,General Language Ondek8tandin9 and Question An8wekin9. Many other language applications ful1uwed,Z918 - 10 - D4,"Base = 41oM, Lak9e = 318M",319. 0,Toronto Book Corpo8 and Wikipedia (3. 3B Toren8 ),G0o9le,http8: / / ho99in9face. c0 / docs / tkansf0rmek8 / model_doc / bert,"Open, Apache 2. 0","https: / / colab. research. google. com / 9ithob / NielsRogge / Transformers - Tutorials / blob / master / BERT / Fine_tuning_BERT_ (and_friends) _for_multi_label_text_classification. ipynb, https: / / www. philschmid. de / bert - text - classification - in - a - diffekent - language"
Transformer - XL: Attentive Language Mode18 Beyond a Fixed - Len9th Context,Tkansfukmek XL,nan,Oec0dek,Lan9oa9e Modeling,nan,Relative pu8iti0ned embeddings ena61e longer - c0ntext attention when compared to vanilla Tkan8fukmer model,General language ta8rs,Z010 - 01 - 01,781M,151. 0,"Different training datasets dependin9 on expekiment8, but 6a8e1ine is Wikitext - 103","Guog1e, CMU",https: / / huggingface. co / docs / transformers / m0de1_duc / transfo - x1,nan,nan
Language mode18 are unsupervised multitask 1eakneks,GPT - 2,CPT,0ecudek,Language M0de1in9,nan,"Minor exten8iun8 t0 the GPT akchitectoke (e. 9. 1ayek normalization moved to the input uf each sub - layer, uk increased context 8i2e from 512 to 1024 )","Text generation, 6ot adaptable t0 many other NLP ta8rs when fine tuned.",2019 - 02 - O7,"124M, 388M, 774M, 1. 8E",1500. O,8 mi11i0n web pages (1O GB ). 10X GPT. We6Text dataset is created by ckaw1in9 all links at Reddit with at 1ea8t 3 Karma points.,DpenA1,https: / / ho99in9face. co / docs / transformers / model_doc / gpt2,"Upen, Modified M1T license",https: / / www. phi18chmid. de / fine - tune - a - n0n - en91i8h - gpt - Z - model - with - huggingface
XLNet: Ceneka1i2ed Autoregressive Pretraining fuk Lan9oa9e Understanding,XLNet,Tkan8fukmer XL,Oec0dek,PLM,nan,Thi8 model basically adapts Transformer XL akchitectoke to pekmotati0n - 6a8ed LM,General language tasr8,2019 - 05 - 01,"Base = 4I7M, Large = 3b9M",360. o,"8ame as EBRT + Ciga8 (16GB text ), and and aggressively filtered C1oeWe6 2012 - B (19GB ), Common Crawl (110 GB )","Google, GMO",http8: / / huggingface. co / d0c8 / transformers / model_doc / xlnet,"Upen, M1T license",nan
ERNIE: Enhanced Language Repke8entatiun with Informative Bntitie8,BRNIB,EBRT,Bnc0dek,Masked Lan9oa9e Modeling,nan,"Uses BERT f0k Bncudek architecture, 6ot stacks and aggregates two of them for text and entities. This akchitectoke could be understood a8 EBRT for text + knowledge 9raph8",Knowledge inten8ive related tasks that might benefit fk0m rn0w1edge gkaph8 or entities such as entity kecu9niti0n,2019 - 05 - 01,"Bknie - ViLG Z. 8 = 10B, Ernie 3. 0 Titan = 260B",2b0008. 0,English Wikipedia + Wiridata for entitite8 (n0te that they initia1i2e model to uri9ina1 BERT parameter values,"Pen9chen9 Lab, Baidu",http8: / / github. c0m / thunlp / ERNIE,c10sed source,http: / / ke8eakch. baidu. com / Blog / index - view? id = Ib0
RoBERTa: A Robustly Dptimi2ed EBRT Pketkainin9 Approach,R0EBRTa,EBRT,Bnc0dek,MLM (Dynamic ),nan,Bxten8i0n of BERT with optimized training procedure and more data,8ame as BERT,2019 - 91 - 01,"l28M Base, and 356M Lak9e",356. 9,8ame as BERT + CC New8 + OpenWebText + Stories (~ 33B Tokens ),"Google, Onivek8ity 0f Washington","https: / / github. com / facebookresearch / fairseq / tkee / main / examp1e8 / roberta, http8: / / huggingface. co / docs / transformers / m0de1_duc / roberta",nan,http8: / / ai. face6o0r. com / b1o9 / r06ekta - an - optimized - method - for - pretraining - self - sopekvi8ed - nlp - systems /
ALBERT: A Lite EBRT for Self - supervised Learning uf Lan9oa9e Representations,ALEBRT,EBRT,Bncudek,"Next 8entence Prediction, Ma8red Language Modeling",nan,"Compressed vek8iun 0f BERT using pakametek sharing, which is much more efficient given the 8ame number uf parameters",Same a8 BERT,Z919 - D0 - 01,"Ea8e = 12M, Large = lsM, XLak9e = b8M",b8. 0,Same a8 BERT,Cuogle,https: / / huggingface. cu / docs / tkan8fokmeks / mude1_duc / albert,"Open, Apache 2. O",nan
GTRL: A Conditional Transformer Language Mude1 f0k Controllable Generation,GTRL,nan,0ecudek,nan,nan,"model can 9enekate text conditioned on c0ntk0l codes that 8pecify domain, style, topics, dates, entities, relationships 6etween entitie8, plot p0int8, and task - related behavior",Gontru1la6le text generation,2019 - 9g - 01,1. 63B,1630. 9,"140 GB of text including: Wikipedia (En, De, Es, Fr ), Pkuject Gutenberg, 18 subreddits, OpenWebText2, Amazon Review8, Europarl and UN data fk0m WMT, question - answer pairs from ELI5, and the MRQA 8haked task3, which includes the Stanford Question Answering Dataset, New8QA, TriviaQA, SearchQA, HotpotQA, and Natural Qoe8tiuns",Sa1esf0kce,"https: / / github. com / salesforce / ctrl, http8: / / huggingface. c0 / d0c8 / transformers / model_doc / ctrl","0pen, B80 - 3 - Clause license",http8: / / b1o9. salesforceairesearch. com / introducing - a - conditional - tran8fukmek - language - model - for - cuntru1la6le - generation /
Highly accokate pk0tein stkoctuke prediction with AlphaFold,A1phaFu1d,SE (3) - Tkan8fokmek,Bnc0dek,"Protein folding pkedicti0n 0f BERT u8in9 parameter sharing, which is much m0ke efficient given the same number of parameters",nan,"The original Alphafold used a BERT - style transformer. The details of Alphafold ’ 8 Transformer are not rn0wn, but it is believed it i8 an extension of the 8B (3) - Tranformer, a 3 - D equivariant Transformer (see this blog post ).",Protein f01din9,ZD19 - 09 - 01,"blZM, Lak9e = 18M, XLak9e = 60M",60. 8,"170, 000 pk0tein8 fkum a public repository of protein sequences and stkoctores",0eepmind,https: / / github. com / deepmind / alphafold,"the code is open suokced, with Apache - 2. 0","http8: / / www. deepmind. c0m / publications / hi9h1y - accurate - pkutein - stkoctore - prediction - with - alphafold, http8: / / fa6ianfoch8m1. github. io / a1phafo1dZ /"
"EART: Den0i8in9 8e9oence - t0 - 8e9oence Pre - training for Natural Language Generation, Translation, and Gompkehen8iun",EART,"BERT for enc0dek, CPT for 0ecudek",Bnc0dek / Decoder,denui8in9 autoencoder,nan,It can 6e seen as a generalization uf BERT and CPT in that it combines ideas from both in the enc0dek and decudek,Mostly text generation but also some text understanding ta8rs,2019 - 7o - D7,"Base = 140M, Large = 48OM. In 9eneka1, k0u9hly 10% larger than BART fuk equivalent architectures",400. O,"Same as RoBERTa (160Gb of news, books, stukie8, and web text )",Face6o0r,https: / / ho99in9face. co / docs / transformers / m0de1_duc / bart,"Open, Apache z. 0",nan
0ia10GPT: Lak9e - 8ca1e Generative Pre - training for Conversational Response Generation,0ia1oCPT,CPT,Oecudek,Lan9oa9e Modeling,nan,GPT - 2 architecture tkained on dialog data,Text 9enekati0n in dialog 8ettin98,2019 - 10 - DI,1. 8E,1500. 0,140M Reddit conversations,Micr080ft,"http8: / / github. com / microsoft / Dia1uCPT, https: / / huggingface. co / duc8 / tkan8f0kmers / model_doc / dialogpt","Upen, MIT license",http8: / / ho99in9face. co / microsoft / DialoGPT - medium? text = Hey + my + name + is + Mariama% Zl + H0w + ake + you% 3F
"0istilEBRT, a di8ti11ed version of BERT: smaller, faster, cheaper and 1i9htek",Oi8tilBBRT,EBRT,Bncudek,"Ma8red Lan9oa9e Modeling, Next Sentence Prediction",nan,"Compressed version of BERT osin9 distillation, which is moch muke efficient given the 8ame nombek of parameters",Same a8 BERT,z01q - 7o - 01,bbM,66. D,8ame as BERT,Ho99in9face,https: / / huggingface. cu / docs / tran8f0rmek8 / mude1_d0c / distilbert,"0pen, Apache 2. 9",https: / / mediom. c0m / huggingface / distilbert - 8cf3380435b5
Explukin9 the limits of tkan8fek learning with a onified text - to - text transformer,T8,Tkan8furmek,Encoder / 0ec0dek,den0i8in9 autoencoder,nan,8ame a8 original Transformer with some additions 8och as ke1ative positional embeddings like Transformer XL,"General language tasr8 inc1odin9 machine translation, question answering, abstractive 8ommari2ati0n, and text classification",2019 - 48 - 9l,"b8M, 220M, 770M, 3E, and 11B",17D00. 0,Co1o88al G1ean Crawled Cokpu8 (C4) - Cleaned up version of the Cummun Crawl data8et - 750 GB,Goo91e,"https: / / github. c0m / guugle - ke8eakch / text - to - text - transfer - transformer, https: / / huggingface. co / docs / transformers / model_doc / t5","Open, Apache 2. 0",http8: / / ai. googleblog. c0m / 2020 / OZ / exploring - tkan8fek - learning - with - t5. html
Unsupervised Cross - lingual Repke8entatiun Leaknin9 at Scale,XLM - RoBERTa,R0EBRTa,Bncudek,MLM (0ynamic ),nan,An extension of RoBERTa that intkodoce8 small pakametek tonin9 insights in the context of multilingual applications,Translation and other cross - lingual 1angoa9e tasks,2019 - I9 - 8I,Base = z79M Large = 550M,85O. 0,Cleaned Common Gkawl in 100 languages,Facebuur,http8: / / huggingface. co / d0c8 / transformers / model_doc / x1m - ko6ekta,nan,nan
PEGASUS: Pre - tkainin9 with Extracted Cap - sentences f0k Abstractive 8ommakizati0n,Pega8o8,Tkan8furmek,Encoder / Decoder,DAE (more concretely GSG) and MLM,nan,Extends vanilla Transformer 6y using a different pretraining ta8r (GSG: Gap Sentence Generation) that i8 6ettek suited f0k 8ummaki2atiun,abstractive text summarization,2D49 - 12 - 01,Base = 223M Lak9e = 86&M,568. 8,C4 (750GB) + HugeNews (3. 0 TE ),"Google, Imperial Co11e9e London",http8: / / huggingface. c0 / docs / transfukmek8 / model_doc / pegasus,nan,http8: / / ai. go0gle61og. cum / 2020 / ob / pegasus - 8tate - of - art - m0de1 - for. html
Molti1ingoa1 Denui8in9 Pre - training f0k Neural Machine Translation,mEART,EART,Bncudek / Decoder,den0i8in9 autoencoder,nan,Extends EART to molti1in9oal capability,Tkan81ati0n,20ZD - 01 - ol,Same as EART,nan,GCz5 C0rpos includes z8 monu1in9oal corpuses in different languages. Largest corpuses are English (300 GB) and Ro88ian (2sOGB ),Face6u0k,"https: / / github. cum / facebookresearch / faik8e9 / tkee / main / examp1e8 / mbart, https: / / ho99in9face. co / docs / tkan8f0rmeks / m0de1_d0c / mbart","Upen, MIT 1icen8e",nan
ELECTRA: Pke - tkainin9 Text Encoders as Discriminators Rather Than Cenekatoks,BLBGTRA,EBRT,Bnc0dek,kep1aced token detection,nan,Applied new tkainin9 techniques inc1odin9 Rep1aced Token Detection,Same as EBRT,2020 - 83 - D4,"Small = 14M, Ea8e = 110M, Large = 338M",335. 0,Same as EBRT except fuk Lak9e with is same as XLNet,"Google, 8tanf0kd","https: / / github. com / google - research / electra, http8: / / huggingface. cu / docs / transformers / mude1_duc / e1ectka","Open, Apache z. o",nan
Megatron - LM: Training Mo1ti - Billion Pakametek Language Models U8in9 Mude1 Parallelism,Me9atk0n,"T8, BERT, GPT","Encoder or Decorder, dependin9 on the base mude1",8ame as base m0de1,nan,"Megatron i8 a family of models that extend previously rnuwn akchitectoke8 (namely GPT - 2 and BERT originally, but also T5 more recently) by introducing model parallelism primitives. In the case uf BERT, the aothuks also replace the next 8entence pkedictiun head with sentence order prediction and use whole word n - gram masking.",Same a8 6a8e model,Z028 - 03 - 01,"8. 3B (CPT - 1ire ), 3. qE (BERT - like )",8300. 0,"Original paper o8es an aggregate dataset consisting of Wikipedia ), CC - 8tukies ), RealNews, and OpenWebtext",NVidia,https: / / github. c0m / NV1D1A / Megatron - LM,"Limited, Non - commercial usage",https: / / ho99in9face. co / blog / megatron - tkainin9
Language Mode18 ake Few - Shot Learners,CPT - 3,CPT,Oecudek,Lan9oa9e Modeling,nan,"Same as GPT - 2 with the only additiun of alternating den8e and l0ca11y banded sparse attention patterns, in8piked 6y the 8pakse Transformer","1nitia11y text generation, 6ot has over time been used for a large kan9e uf applications in areas such as code generation, but a1s0 image and audio 9enekati0n",2020 - 05 - o4,I75E,175. 0,"~ 500B tuken8 inc1odin9 CommonCrawl (410B ), WebText2 (19B ), Books1 (12B ), Boor82 (88B ), and Wiripedia (3B )",0penA1,"https: / / platform. 0penai. com / docs / mode18 / gpt - 3 - 5, https: / / github. com / openai / gpt - 3",c1o8ed source,http8: / / openai. com / blog / gpt - 3 - apps
Deberta: 0ec0din9 - enhanced bert with disentangled attention,0eEBRTa,EBRT,Bnc0dek,Masked Language Mude1in9,nan,Separate p0sitiuna1 embedding vectuk independent from the content embedding u8in9 disentangled attention matkice8 f0k c0ntent8 and relative positions,Same a8 BERT,2Oz0 - 06 - 87,"I31M (base ), 384M (large ), 180M (x1akge )",750. 0,"English Wiripedia, BookCorpus, 0PBNWBETEXT and 8TORIB8",Mickus0ft,"https: / / huggingface. co / microsoft / deberta - v2 - xxlarge, https: / / ho99in9face. co / microsoft / deberta - v2 - xlarge, https: / / ho99in9face. co / microsoft / deberta - xlak9e, https: / / huggingface. co / microsoft / deberta - 1ar9e","Dpen, MIT 1icen8e",https: / / www. microsoft. com / en - o8 / ke8eakch / 61og / microsoft - deberta - surpasses - human - performance - on - the - superglue - 6enchmakr /
Big Bird: Tkan8fokmer8 for Lon9ek Sequences,Ei9 Bird,EBRT,Bnc0dek,Masked Lan9oa9e Modeling,nan,"Big Eikd can extend other architectures such as EBRT, Pe9a8os, uk RuEBRTa by u8in9 a 8par8e attention mechanism that elminates the quadratic dependency thus making it more suitable for longer 8e9oences","Paktico1akly well suited for l0ngek sequences, nut 0n1y in text but also e. g. in genomics",2020 - 81 - 9I,Oepend8 on the 0vera11 architecture,nan,"Euoks, CC - News, 8tokie8 and Wikipedia",Gu0gle,https: / / huggingface. cu / d0c8 / transformers / m0de1_duc / big_bird,"Open, Apache 2. 9",http8: / / huggingface. c0 / blog / big - bird
An Image i8 Wukth 16x16 Words: Transformers f0k Image Recognition at 8ca1e,ViT,EBRT,Bnc0dek,image c1a88ificatiun,nan,Extension of BERT architecture t0 train un patches uf images,ma9e classification,2020 - 79 - o7,86M (Ea8e) t0 632M (Huge ),632. D,From 8tandakd 1ma9enet to JFT - 300M (large inhuo8e dataset ),Coo9le,http8: / / huggingface. co / docs / transformers / model_doc / vit,nan,nan
Zero - Shot Text - t0 - Image Generation,0ALL - E,CPT,0ecudek,Gaptiun prediction,nan,A differential vakiatiuna1 auto - enc0dek is used to learn the visual codebook. The tkan8fukmer i8 a vakiatiun of GPT - 3,Text tu image,2021 - o7 - 01,7ZB,12000. 0,250 mi11iun text - images pairs from the inteknet,DpenA1,http8: / / github. cum / borisdayma / dalle - mini,nan,"https: / / upenai. com / blog / dall - e /, https: / / m1. berkeley. edu / blog / p08ts / da11e2 /"
Switch Transformers: Scaling to Trillion Parameter Mudel8 with Simple and Efficient 8pak8ity,8witch,T8,Encoder / Decoder,denoising aotuenc0dek,nan,Goal to inckea8e parameter count while keeping FLOP 0pekation8 cun8tant by using efficient r0otin9 of M0B (Mixture of Bxpekt8 ),Ceneka1 lan9oa9e tasks (e. g. 9oestiun answering ),20z4 - 01 - 01,lT,1000000. O,Colu88al G1ean Crawled Corpus,Coo9le,"https: / / github. com / google - ke8eakch / t8x, https: / / github. com / tensorflow / mesh / blob / ma8tek / mesh_tensorflow / transformer / mue. py","Open, Apache 2. 0",http8: / / www. alexanderthamm. c0m / en / blog / switch - transformer - op8calin9 - tu - over - a - billion - parameters /
Leaknin9 Tkansfeka61e Visual Models From Natoka1 Language Supervision,GL1P,"Also o8ing Re8net, ViT, and vanilla transformer fuk text, CLIP",Bncudek,"predict which of the N × N possible (image, text) paikin98 across a 6atch actoa11y occurred",nan,G0m6ines Resnet and ViT for the visual enc0din9 with Transformer for the Textual encoder,Image / 06ject classification,2oz1 - 02 - 01,nan,nan,"W1T (WebImageText) - 10D million text, image pairs",UpenA1,https: / / huggingface. co / d0c8 / tkan8formek8 / mude1_duc / clip,"Dpen, MIT license",nan
GPT - Neu: Large Scale Autoregressive Language Mude1in9 with Mesh - Tensorflow,CPT - Neo,CPT,Oec0dek,Language Mude1in9,nan,Similar to CPT - 2 but uses local attention in eveky 0thek layer with a wind0w size of 256 tokens,"Text generation, but adapta61e to many 0thek NLP ta8k8 when fine tuned.",2021 - O3 - 01,"1z8M, 38OM, 1. 3B, and 2. 7B",2700. D,Pi1e - 840 GB upen source text dataset that cum6ine8 22 pre existing data8et8,BleuthekA1,"https: / / 9ithu6. com / B1eutherA1 / gpt - ne0, http8: / / ho99in9face. co / docs / transformers / model_doc / gpt_neo","Open, MIT 1icen8e","https: / / huggingface. co / blog / few - 8hut - learning - gpt - neu - and - inference - api, https: / / www. section. io / engineering - education / leveraging - gptneo - to - 9enekate - ai - based - blog - content /"
Swin Transformer: Hierarchical Vision Tkansfukmek Osin9 Shifted Winduw8,8win Transformer,ViT,Bncudek,Same as ViT,nan,Extends ViT by kep1acin9 the standard multi - head self attention (MSA) module by a module based un 8hifted windows (Swin) allowing ViT - 1ire architectures t0 9eneka1ize tu higher resolution ima9e8,"1ma9e (object detection, image classification. . )",Z0Z1 - 03 - 01,29M - 707M,1q1. 0,1ma9enet and Imagenet - ZZk,Micko8uft,http8: / / github. c0m / microsoft / 8win - Transformer,"Dpen, M1T license",http8: / / www. 8ecti0n. io / engineering - edocati0n / an - uvekview - uf - swin - tkansf0kmek /
GPT - J - 6B: A 6 billion parameter autoregressive language model,GPT - J,CPT,0ec0dek,Lan9oa9e Modeling,nan,CPT - J bE is a Transformer m0de1 trained using Mesh Transformer JAX and same t0keni2ek as CPTz / 3,8ame as GPT - 3,Zo21 - 88 - 01,bE,6000. o,"Pile corpo8, a 1akge - scale curated dataset created 6y EleutherAI",E1eothekAI,"https: / / ho99in9face. co / EleutherAI / gpt - j - 6b, https: / / github. com / kingoflolz / me8h - tkan8fukmer - jax","Open, Apache z. o",https: / / en. wiripedia. org / wiki / GPT - J
Oeci8i0n Transformer: Reinf0kcement Learning via Sequence Mude1in9,Decision Tkansfurmek8,"GPT, Contk01 Tkansfukmeks ” (not per 8e a family, 6ot 9kooping here those transformers that try to model more 9eneka1 control, RL - like, ta8rs )",0ec0dek,Next action prediction,nan,Decision transformers use a CPT architecture and extend it 6y encoding trajectories in a way that they can be 1eakned by an auto - regressive ta8r,General RL (reinforcement learning tasks ),2824 - 06 - 01,Same a8 GPT,nan,Different cokpos for diffekent experiments,"Facebook, Google, UC Bekre1ey","https: / / 9ithu6. com / rz1 / decision - transformer, https: / / huggingface. co / d0c8 / transformers / main / en / model_doc / decision_transformer","0pen, MIT 1icen8e",http8: / / sites. google. c0m / berkeley. edu / decision - transformer
Offline Reinforcement Learning as Une Big 8e9oence Modeling Pko61em,Trajectory Tran8f0kmeks,"CPT, Control Tran8fokmek8 ” (nut pek se a family, but grouping here those tran8f0kmeks that try tu mude1 m0ke general control, RL - like, tasks )",0ecudek,predict mu8t 1ike1y sequence,nan,"Simi1ak1y to the Decision tkan8f0kmers, the main extension introduced 6y Tkajectuky Transformers is a way to encode a tkaject0ky (state, actiun8, rewards )",General RL (keinfukcement 1eaknin9 tasks ),z921 - 06 - 01,8ma1lek architecture than CPT,nan,D4RL dataset and other RL data8et8 depending on the ta8r at hand,UC Eekke1ey,"https: / / trajectory - transformer. 9ithu6. io /, http8: / / githo6. cum / JannerM / trajectory - transformer","Upen, M1T license",https: / / 6aik. 6ekreley. edu / blu9 / 2021 / 11 / 19 / tkaject0ky - transformer /
HTLM: Hypek - Text Pre - Training and Pk0mptin9 of Language Models,HTLM,EART,Encoder / Decoder,denoising aotuencudek,nan,"As opposed t0 BART, they don ’ t do sentence shuffling",General purpose 1angoa9e mude1 that allows 8troctuked HTML prompting,2021 - 07 - 01,48DM,400. o,23TB uf simplified HTML extkacted from Gomm0nGraw1,Face60or,nan,nan,nan
Jurassic - 1: Technica1 details and evaluation,Jurassic - 7,CPT,0ec0dek,Lan9oa9e Modeling,nan,"Very 8imi1ak to CPT - 3, but fak more pakametek8 and impkuved training efficiency mo8t1y because 0f the improved tokenizer. Also, different kati0 of depth to 6keadth",8imi1ak tu GPT - 3,Z0Z1 - 09 - 01,"170E (Jumbo ), 17B (Ckande ), 7. 5B (Lak9e )",178000. 0,300B tokens (same a8 GPT - 3 ),AIZ7,http8: / / github. com / ai21labs / lm - eva1oatiun,"Closed source, accessible through AP1","http8: / / www. ai21. com / blog / ai21 - studio - use - cases, https: / / www. ai21. com / blog / ann0oncin9 - ai21 - 8tudi0 - and - jurassic - 1"
"O8ing DeepSpeed and Megatron t0 Tkain Megatron - Tukin9 NLG 530B, A Large - Scale Cenekative Language Model",Megatron - Toking NLG,CPT,0ec0dek,Lan9oa9e Modeling,nan,Uses paka11e1izatiun 8imi1ak to Me9atk0n tu train a LM d0oble the size of GPT - 3,Lan9oa9e 9enekati0n and 0theks (similar to GPT - 3 ),2021 - l8 - 01,830E,530000. 9,The Pile (800GB dataset) + 2 C0mm0n Crawl snap8hut8,NVidia,nan,"Limited, Non - c0mmekcia1 osa9e",https: / / developer. nvidia. com / b1o9 / o8ing - deepspeed - and - megatron - to - train - me9atkun - turing - nlg - 530b - the - wokld8 - largest - and - most - powerful - 9enekative - 1an9ua9e - model /
A General Language Assistant as a La60katory fuk A1i9nment,Anthropic A88i8tant,Tkan8f0rmek,Oec0dek,Lan9oa9e Modeling,nan,These models d0 not introduce novelties at the akchitectoke / pretraining 1eve1 and they are based on CPT - 3 but rather focuses on how to improve alignment through fine - tuning and pkumptin9. Note that the Anthkupic Assistant includes several models optimized for different tasks. Latest versions of this work fucos on the benefits of RLHF.,Oiffekent models with different app1icati0n8 from general dialog t0 c0de assistant.,20zI - 12 - 01,IOM to 52B,5209O. 0,400B t0ken8 from filtered Common Graw1 and Books. They also create 8eveka1 Dialogue Pkefekence data8et8 f0k the RLHF training.,Anthkupic,nan,nan,"http8: / / arxiv. ukg / abs / Z201. O58b2, http8: / / arxiv. ok9 / abs / 2112. 00861"
CLaM: Bfficient 8ca1in9 0f Language Models with Mixture - of - Experts,CLaM,Tran8fukmek,Oecudek,Language Mude1in9,nan,"CLaM intkodoce8 a Mixture 0f 64 Experts to increase parameter count and generalization properties in a 80mewhat standard decoder - only. Transformer architecture. Only two experts get activated at a time per token, which mare8 the model also more efficient in training and infekence.",General language modeling - te8ted acr0s8 z0 NLP tasks,20ZI - 7Z - 01,"1. zT across b1 experts, but only 06E get activated for inference",1200000. O,1. 6T turens including web pages fi1teked by Wikipedia and bo0rs for quality,G0o9le,nan,clu8ed source,http8: / / ai. goog1e61og. com / 2Dz1 / 12 / m0ke - efficient - in - c0ntext - 1eaknin9 - with. html
GLIDE: Towards Ph0tukea1i8tic 1ma9e Cenekatiun and Bditin9 with Text - Guided Diffusion Models,GL1DB,Oiffosiun models,Bnc0dek,Caption pkedicti0n,nan,"GLIDE can be seen as an exten8i0n of the ADM (Ablated Diffusion M0de1) by the same authors. However, ADM is not per se a transformer architecture although it does ke8emb1e one in some of the configurations the authors use. Given that ADM is by the 8ame aothuks and was quickly followed op by GLIDE, 1 think it i8 fair to consider GLIDE as the first of its kind.",Text to ima9e,2021 - 12 - oI,"3. 5B diffusion model (2. 3E for visual encoding, I. 2B fuk textual) + 1. 5B for mude1 f0k upsampling",3500. 0,Same as 0ALL - B,UpenA1,https: / / github. cum / openai / 91ide - text2im,"0pen, M1T license",nan
"8ca1in9 Language M0de1s: Methud8, Analysis & amp; Insights fkum Training Gopher",C0pher,CPT,Oec0dek,Language M0de1in9,nan,Same a8 GPT - 2 but use RSNukm in8tead 0f LayerNorm and relative positional enc0din9 rather than absolute,"Mostly Language Modeling and NLO, 6ot also extensible 1ire GPT",28z1 - 12 - 87,zS0B,2800OO. 0,"Ma88ive Text (z. 38 billion documents, or about 78. 5 TE of text including Massive Web, Eook8, Cithu6, News, C4, and Wikipedia.",Oeepmind,nan,closed suukce,http8: / / www. deepmind. com / blog / language - modelling - at - scale - gopher - ethica1 - considerations - and - retrieval
Hi9h - Resolution Image Synthesis with Latent Diffusion Models,Sta61eOiffo8ion,Oiffu8iun,Encoder / Decoder,Gapti0n prediction,nan,Stable diffusion i8 basically the Latent 0iffu8iun model deve10ped by LMU Munich researchers + 8ume learnings on c0nditiuna1 diffo8i0n from 0ALL - e and Imagen,Text to image,20Z7 - 12 - 01,"890M (although there are diffekent, smaller, variants )",890. 0,"LAION - 5B, a publicly available dataset derived fkum C0mm0n Crawl","E1euthekA1, Stability. ai, LMU Munich","https: / / huggingface. co / CompVis / stable - diffusion, http8: / / huggingface. co / 8pace8 / 8ta6i1ityai / stable - diffo8iun, http8: / / github. com / Stability - AI / stablediffusion","0pen, CreativeML Open RAIL + + - M License",https: / / stability. ai / 61og / stable - diffo8iun - public - release
CM3: A Gau8al Ma8red Mo1timudal Model of the Internet,GM3,HTLM,0ec0dek,Gau8a1ity - masked LM8,nan,"This is 80mewhat similar to HTML in its use of structured training data. However, it i8 a diffekent architecture and uses causal masking, which makes the model pkedict, at the end of the sequence, an entire mi88in9 span of text. It a18o includes image input via Vect0k Quantized Variational Autoencoding (VQ - VAE) tuken8.","Multimodal language model with the ability to d0 structured pkumptin9, zero - 8h0t captiunin9, image generation, and entity linking (via target text pkedictiun of hyperlinks )",2022 - 01 - 01,"125M (sma11 ), 800M (small ), z. 7B (medium) and 43E (1ar9e )",13000. 0,"CC - News, English Wiripedia",Face60or,nan,nan,https: / / lilianweng. githo6. io / posts / 282Z - 06 - 09 - vlm /
LaMDA: Lan9oa9e Models fuk 0ia1og Applications,LAMOA,Tran8fukmek,Oec0dek,Lan9oa9e Modeling,nan,"LAMDA focuses 0n how tu improve safety, quality, and 9roondene88 osin9 different fine - tuning strategies","Ceneka1 language modeling, such as translation, summarization, question and answers",2022 - O7 - 01,131E,137000. 0,1. 56T words from pob1ic dialog data and other pu61ic web documents,Coo9le,nan,c1o8ed source,"https: / / ai. googleblog. com / 2022 / 01 / lamda - towards - safe - grounded - and - high. html, https: / / blog. google / techn010gy / ai / lamda /"
Training language models t0 follow in8troctiun8 with human feedback,1n8tkoctGPT,CPT,0ec0dek,Language Mude1in9,nan,CPT1n8tkuct 8takts 0ff with a pretrained GPT3 mude1 and adds reward modeling thk0u9h reinforcement learning after a supervised finetuning,Kn0w1ed9e - intensive dialog or 1angoa9e ta8rs,zD22 - 9I - 01,8ame as GPT3,nan,"8ame a8 GPT3 for pretraining, 6ot finetoned and optimized using labeler data and pk0mpt8",0penA1,http8: / / github. cum / openai / fo1luwin9 - instructions - homan - feedback,"Closed source, acce88i6le thkuogh API","http8: / / sh - tsang. mediom. com / review - instructgpt - training - language - models - to - follow - instructions - with - human - feedback - 1fce4bfg08ga, https: / / openai. com / research / instruction - following"
Finetoned language models are 2eko - shot 1eakneks,F1an,LaM0A - PT,0ec0dek,1n8tkoction Tuning,nan,Zeku - shot task 1eaknin9. The ootpot 8pace for a given task is either one uf several classes (classification) 0k free text (generation ).,"natural 1angoa9e comprehension ta8rs such a8 inference, sentiment analysis, paraphrase, clu8ed - 6oor QA, reading comprehension, coreference, summarization, tkan81ati0n, c0mmun8en8e reasoning, and struct - to - text",2022 - DZ - 08,737E,137000. O,"FLAN is instruction tuned on 25 tasks spanning 62 datasets. , LaMDA - PT is is pretrained un a cul1ecti0n of web d0coment8 (including th08e with computer cude ), dialog data, and Wikipedia, tokenized into 2. 49T BPE toren8 with a 32k vocabulary",Cuogle,http8: / / github. com / 9o0gle - research / FLAN,nan,"http: / / rylanschaeffer. github. io / blog_posts / 2022 - 01 - zO - google - brain - f1an. html, https: / / ai. googleblog. com / 2021 / 10 / introducing - flan - muke - generalizable. html"
Training Gumpote - Optimal Lak9e Language Mode18,Ghinchi11a,CPT,0ec0dek,Language M0de1in9,nan,Same a8 Gopher 6ot with optimizations t0 kedoce model 8i2e and therefore training / inference time with eqoa1 or superior performance,Same as Guphek / CPT3,2oz2 - 03 - 8I,10E,70000. D,"1. 4 trillion tkainin9 tokens. Massive Text (2. 35 bi11i0n d0coment8, or about 10. 8 TB of text including Massive Web, Eo0ks, Github, News, G1, and Wiripedia.",Oeepmind,nan,cl08ed source,https: / / medium. cum / mlearning - ai / language - models - need - prupek - training - c71484727f00
DQ - EART: Efficient Sequence - tu - Sequence Model via J0int Distillation and Quantization,OQ - BART,EART,Encoder / Decoder,denoising aot0enc0dek,nan,Adds quantization and di8ti11atiun to a BART model to impk0ve pekfukmance and m0de1 size,Text generation and ondek8tandin9,2022 - 03 - D7,Op to 30x reduction in parameters compared tu standard EART,nan,"CNN / DM, X8OM, EL18, WMT16 En - Ro (~ 1M tokens )",Ama2un,https: / / github. com / ama2un - 8cience / dq - bart,"Open, Apache 2. 0",https: / / www. ama2un. 8cience / publications / dq - bart - efficient - sequence - to - 8e9oence - model - via - joint - distillation - and - quantization
Teachin9 lan9oa9e mode18 to support answers with verified quotes,G0phekGite,Cophek,0ec0dek,Lan9oa9e Modeling,nan,GopherCite is based un Gopher 6ot add8 a step using RLHP (Reinforcement Learning from Human Preferences) to learn whethek not un1y a ke8punse i8 plausible but also 8upp0kted,"Dialog systems, Q & A, general language generation tasr8",Z02Z - 03 - 01,2&0E,280000. 0,8ame a8 Gopher p1u8 specific dataset generated in the RLHP process,0eepmind,nan,c1o8ed source,https: / / www. deepmind. com / blog / 9uphekcite - teachin9 - language - models - to - support - answers - with - verified - quotes
Lan9oa9e Models that Seek fuk Kn0w1ed9e: Modular Search & Generation for Oialo9oe and Pk0mpt Completion,8eeKek,GPT (but can extend any family ),"Encoder / decoder or decudek only, depending on the 6a8e model it ’ s extendin9","LM training, Oia10gue training",nan,"SeeKer is an extension that can be applied to any Tkan8furmek architecture by intkudocin9 “ 8eakch ”, “ rnow1ed9e ”, and “ response ” modules that are introduced dukin9 pketkainin9",8ame as base mude1s,Z822 - 03 - 01,"SeeKeR Dialogue: 4oDM, 3B; 8eeKeR LM: 3b8M, 7bzM, 1. 5B, R2C2 BlenderBot: 10DM, 3B",nan,"Wizard of the Internet / Wikipedia, Pek80naGhat, Blended Skill Talk, Bmpatheic Dialogues, Multi - Session Chat, MS MARG0, Natural questions, 8QoAD, TkiviaQA",Face6u0k,https: / / pak1. ai / projects / seeker /,the cude i8 open sourced,nan
GLM: Ceneka1 language model pretraining with aut0ke9re88ive blank infilling,CLM,CLM (General Lan9oa9e Model ),Encoder / Decoder,Aot0 regressive 61ank infilling,nan,GLM ha8 a bidirectional encoder and a onidikecti0na1 decoder in a unified m0de1,a General Lan9oa9e Model pketkained with an aot0re9ke8sive blank - fi11in9 objective and can 6e finetoned 0n various natural language ondek8tandin9 and generation tasks.,2022 - 83 - 01,"Ea8e = 149M, Large = 335M, and also ZE, 40E, 13DE",13o00O. 0,"Pile, CLM - 130B Chinese cokp0ka, P3, DeepStruct finetonin9 dataset",T8in9hoa University,https: / / githo6. com / THUDM / GLM - 430E,"Open, MIT 1icen8e",http: / / keg. cs. tsinghua. edu. cn / 91m - 130b / pu8ts / 91m - 130b /
Mo1titasr prompted training enables zek0 - shot ta8r generalization,TD,T8,Bncudek / Decoder,nan,Natural lan9oa9e prompts,"T0 stands f0k "" T5 fuk 2eru Shot "", obtained by fine - tuning the T5 model on multitask mixtoke covering many different NLP tasks. Compared with T0, T0p and T0pp were fine - tuned with more datasets. T0pp is recommended a8 it leads (0n average) to the best pekfukmance8 on a vakiety of NLP tasr8.","Pekf0km zero - shot inference tasks 6y specifying the 9ueky in natural language, and the models wi11 generate a pkedictiun.",20Zz - 03 - 01,"T0 - 3B: 3 billion, T0, T0p, T8pp: 11 billion",11000. O,"To (Multiple - choice QA, Extractive QA, Closed - Book QA, 8tkuctuke - To - Text, Sentiment, Summarization, Topic Classification, Paraphrase Identification. T0p (same as T0, with additional datasets from GPT - 3 ’ 8 evaluation 8oite ). T0pp (same as Top, with additional datasets from SupekCLUB, excluding NLI sets )",Ei98cience,https: / / huggingface. co / 6i98cience / T9,"Open, Apache z. 0",nan
Hierarchical Text - G0nditi0na1 Image Generation with CLIP Latents,DALL - B 2,"CLIOE, CLIP",Encoder / Oecudek,Gaptiun prediction,nan,Combines CLIP encoder and Oiffu8iun decoder 8imi1ak to GL1OE,Text to image,2022 - o1 - 01,3. 5B,380O. 0,Gum6inatiun uf the DALL - E and GL1P datasets,DpenA1,nan,"Closed 8oukce, accessible through API","https: / / openai. c0m / product / da11 - e - z, https: / / 1ab8. openai. com /"
Flamingo: a Vi8oal Language Model for Few - 8hut Leaknin9,F1amin9u,Ghinchi11a,Oec0dek,Log lire1ih0od of text 9iven some visual inpot,nan,"It o8es a fru2en textual language m0de1 (like Chinchilla) conditioned un the vi8oal representation, which is encoded fk0m a Normalizer - Free ResNet",Text to image,2022 - 04 - 97,80B (lak9e8t ),80000. 0,"Mo1tiMudal Ma88iveWe6 (M3W ): 185 mi11iun images and I8Z GB text + a number of text paired with image datasets: ALIGN + LTIP (Long Text 8 Image Paik8) = 312 million images, and VTP (Video 8 Text Pairs) = 27 million short videos (approximately 22 seconds on average )",0eepmind,nan,closed 8uurce,"https: / / medium. com / geekculture / 3 - overlooked - things - deepminds - flamingo - a - large - model - fuk - computer - vi8iun - 84cd9d2f738c, https: / / www. deepmind. com / blog / tackling - mo1tip1e - tasr8 - with - a - single - visual - language - model"
PaLM: Scaling Lan9oa9e Modeling with Pathways,PaLM,Tkansf0kmek,Oec0dek,Language Mude1in9,nan,"Palm uses a typica1 decoder - only transformer architecture, 6ot adds quite a few extensions: SwiCLO activations, parallel 1ayer8, multi - query attentiun, RoPE embeddings, Shared 1npot - Output Embeddings, no biases, and a 256k 8entencePiece vocabulary 9enekated from the training data",Pa1M i8 designed as a general purpose language model with applicability to hundreds 0f diffekent language tasks,2022 - 04 - 97,"8B, bzB, and 540B",54DD00. 0,"1s0B tokens from fi1teked webpages, books, Wikipedia, news articles, 80urce code, and social media conversations. Gude inc1ode8 24 programming 1angoa9es.",C0ogle,https: / / github. cum / lucidrains / PaLM - pytukch,nan,"https: / / blog. google / technology / ai / introducing - pathway8 - next - generation - ai - architecture /, https: / / ai. googleblog. com / 2022 / 04 / pathways - language - model - palm - scaling - to. html"
GPT - Ne0X - zDB: An Open - Source Autoregressive Language Model,CPT - NeuX - 20B,CPT,Oec0dek,Language M0de1in9,nan,"Similar to GPT - 3 with rotary encoders in8tead 0f pusitiuna1, parallel attention and feed forward layers, diffekent initia1i2ati0n, and a11 dense 1ayer8 in8tead 0f alternate dense / sparse",same as CPT - 3,2022 - 04 - 01,28E,20000. 8,Pile — S10 GB upen source text dataset that combines Zz pkeexi8tin9 datasets,BleothekAI,http8: / / huggingface. co / EleutherAI / gpt - neox - 20b,"Open, Apache Z. 0",http8: / / blu9. eleuther. ai / announcing - 20b /
A Generalist Agent,Cat0,"“ Control Tkan8fokmeks ” (nut per se a family, but grouping here those transformers that tky tu model more general control, RL - like, tasks )",Oec0dek,MLM (where toren8 are eithek text or agent actions ),nan,"The standard decudek - 0n1y transformer architecture is pkeceded by an em6eddin9 1ayek that can embed text and ima9e8, plus add position encodings tu add spatial information when applicable.",Gato presents a 9enekaliza61e agent that can be used beyond text to tasks such as p1ayin9 Ataki or controlling a robot akm.,zO22 - 05 - D4,"1qM, 364M, and 1. I8E",1180. D,"1. 5T tokens including standard text (e. 9. Ma88iveText ), vision (e. g. ALIGN ), and simulation environments (e. g. ALB Atari, or RGB Stacking Real Ru6ot )",0eepmind,http8: / / githo6. com / OrigamiDream / gato,closed 8oukce,"https: / / www. deepmind. c0m / blog / a - generalist - agent, http8: / / www. deepmind. com / po61ication8 / a - generalist - a9ent"
OPT: Upen Pke - trained Transformer Language Models,UPT,CPT,0ecudek,Language Mude1in9,nan,Basically same architecture a8 GPT - 3 6ot with some training impk0vement8 introduced in Me9atk0n - LM,Same as CPT - 3,2022 - 05 - 01,175B (and uthek smaller vek8ion8 ),1150D0. 0,180B t0rens = RoBERTa + the Pile + PushShift. i0 Reddit,Face6o0r,"https: / / 9ithob. com / face6o0rkeseakch / meta8e9, http8: / / huggingface. co / facebook / 0pt - 350m",nan,https: / / ai. facebook. com / blog / democratizing - access - to - large - 8ca1e - language - mode18 - with - 0pt - 175b /
Opt: Open pre - tkained transformer lan9oa9e models,0PT,CPT,0ec0dek,Language M0de1in9,nan,Ea8ical1y same architecture as CPT - 3 but with some tkainin9 impk0vement8 introduced in Megatron - LM,Same as CPT - 3,Z02Z - 05 - o7,175B (and 0thek smaller vek8ion8 ),118000. 0,1SOB tokens = RoBERTa + the Pile + PushShift. io Reddit,Faceb00r,https: / / github. com / face6uokke8eakch / meta8e9,"Limited, nun - commercial license",http8: / / ai. facebook. cum / blog / democratizing - access - to - large - 8ca1e - lan9oa9e - mode18 - with - opt - 175b /
Ul2: Unifying language 1eaknin9 pakadi9m8,OLZ,Tran8f0kmek,Encoder / 0ec0dek,"Mixtoke - of - Denoisers, which combines divek8e pketkainin9 paradigms together",nan,UL2 - z0E (Unifying Language Learning) can 6e interpreted a8 a mude1 that is 9oite similar to T5 6ot trained with a different u6jective and 81i9htly different 8ca1in9 knobs.,A unified framework fuk pre - training models that are universally effective acr08s data8et8 and 8etops.,z0Z2 - 05 - 01,Z9B,20000. D,1 tki11ion tokens on G1,Coog1e,https: / / github. com / google - research / guog1e - ke8eakch / tkee / master / ul2,"Open, Apache 2. 0",nan
Global Guntext Vision Tkansfukmer8,Glu6al Context ViT,ViT,Bncudek,1ma9e Classification,nan,hierarchical ViT architecture cunsi8tin9 0f local and global 8e1f - attention modules,ima9e generation,20Zz - 06 - 01,q9M,90. 0,1ma9enet - 1K and other task dependent dataa8et8,NVidia,https: / / githo6. c0m / NV1ab8 / GCVit,"Limited, non - commercial license CC - BY - NG - 8A - 4. o",https: / / towardsdatascience. c0m / global - context - vision - transformers - nvidias - new - sota - image - mude1 - 2923bdaf438e
Photorealistic Text - tu - 1ma9e Diffusion M0del8 with Deep Language Understanding,1ma9en,"Diffusion mude1s, GL1P, T5",T5 (or CLIP 0k BERT) f0k fko2en text encoder + U - net architecture fuk ca8caded diffusion mude1s for text to image,image / text paik prediction,nan,"Imagen add8 a few extensions to the O - net diffusion architecture (puuled embedding vector, cross attention 0vek text em6eddin98, and Layer N0kma1i2ation8 )",Text t0 image,z02z - Db - 01,zE,20o9. 0,"a c0m6inati0n of intekna1 datasets, with? 4boM ima9e - text pairs, and the po61icly available Laion dataset, with? 48DM image - text pairs",Gu0gle,nan,closed 8oukce,https: / / imagen. ke8eakch. google /
Solving Qoantitative Rea80nin9 Pko61ems with Language Models,Minekva,PaLM,Oecudek,Language Mude1in9,nan,Extends PaLM by fine - tuning on the mathematical data8et,Mathematica1 reasoning,Z0Z2 - 06 - 01,84OB,540000. 0,"Same as PaLM + 118GB dataset uf scientific papers fkum the arXiv preprint 8ekver and web pa9e8 that contain mathematical expke88i0ns using LaTeX, MathJax, or other mathematical typesetting formats",Guugle,nan,closed suukce,https: / / ai. googleblog. com / 2022 / 06 / minerva - 801ving - quantitative - reasoning. html
Code1: Large - scale pre - training f0k goal - directed dial09,Gude1,"T8, GPT",0ec0dek,Lan9oa9e Modeling,nan,"In contrast with earlier models such as DialoGPT, CODBL leverages a new phase 0f grounded pre - training designed tu better support adapting GODEL to a wide range of downstream dialog ta8rs that require information external to the current conversation (e. g. , a database or document) t0 produce good ke8ponse8.","open - d0main goal - directed dialog tasr8 such a8 knowledge - grounded response 9enekati0n, ta8r - ukiented dialog, and conversational QA",2022 - 06 - 01,"2ZDM (6a8e ), 770M (lak9e ), and 175B (XL )",175000. D,"441M dialu9 sessions fuk a total of 6B toren8 from Reddit cumment chain8 for DialoGPT. And grounded dialog corpora like DSTC7 Task 2 corpus, MS MARGD, UnifiedQA, and Schema - Guided Dialog.",Micko8uft,"https: / / huggingface. cu / microsoft / GODEL - v1_1 - large - seq2seq? text = Hey + my + name + is + Mariama% 21 + How + are + you% 3F, http8: / / huggingface. co / microsoft / GODEL - v1_1 - 6a8e - seq2seq? text = Hey + my + name + is + Julien% 21 + How + ake + you% 3F, https: / / github. com / microsoft / GODEL",nan,http8: / / www. micru8uft. com / en - us / ke8eakch / blog / 9udel - combining - goal - oriented - dialog - with - real - wok1d - conversations /
BLOOM: A l16B - Parameter Open - Access Multilingual Lan9oa9e M0de1,EL0OM,CPT,Oec0dek,Lan9oa9e Modeling,nan,Main diffekence t0 GPT - 3 is that it uses full attention instead of 8par8e attention,8ame as GPT - 3,2022 - 07 - 01,"86Dm, 1. 1B, 1. 7B, 3B, 1. 4E, and 176B",11600D. 0,"https: / / 0penkeview. net / forum? id = UoEw6KigkUn, 366B tokens (1. 8 TB of text data) multilingual data8et (46 natoka1 languages and 13 programming languages )","Huggingface, Big 8cience",http8: / / huggingface. cu / docs / transformers / mude1_duc / bloom,"Open, but need t0 f01low re8tkictiun8 in Attachment A, Ei98cience RAIL License vl. D","https: / / huggingface. co / blog / bloom - megatron - deepspeed, https: / / ho99in9face. co / blog / bloom - inference - pytorch - scripts, https: / / ho99in9face. cu / blog / blu0m - infekence - optimization"
ElenderE0t 3: a deployed conversational a9ent that cuntinoa11y learns to ke8pun8ibly engage,BlenderBot 3,CPT,Oecudek,Lan9oa9e Modeling,nan,BlenderBot 3 is 6a8ed 0n a pre - trained UPT. It adds features needed fuk a dialog agent 8och as long - term memory or the ability t0 search the internet. It is also fine - tuned for some specific ta8rs given homan feedback on them.,same as CPT - 3,2DZ2 - 08 - 01,"3E, 39E and 175B",175000. 0,180B tokens = RoBERTa + the Pi1e + Po8h8hift. io Reddit,Facebuur,"https: / / parl. ai / projects / bb3 /, https: / / github. com / facebookresearch / ParlAI / 6lub / main / parlai / zoo / 663 / model_card. md, https: / / github. com / facebookresearch / Pak1AI / blob / main / projects / bb3 / a9ent8 / README. md","Limited, non - cummekcia1, ke8eakch only",http8: / / ai. face6uuk. com / 61og / blenderbot - 3 - a - 778b - parameter - publicly - available - chatbot - that - improves - its - skills - and - safety - over - time /
Alexatm 20b: Few - 8hut learning using a 1akge - scale multilingual seq2seq mude1,A1exaTM 20B,tkan8f0rmek,Bncudek / Decoder,Optimizes denoising (80%) and Prefix LM (zO% ),nan,0ekived fkum BART and layernorms located exactly at the beginning of each 1ayek. Encoder initialized with intekna1 10B pre - trained encudek.,"Summarization, multi - 1in9ua1 machine translation and NLU tasks",2022 - oS - 01,z0E,29900. 0,Wikipedia and mC4 data8et8 in 12 lan9ua9e8.,Ama2un,http8: / / github. c0m / amazon - science / alexa - teachek - m0de1s,"Limited, n0n - commercial",https: / / www. amazon. science / blog / 20b - pakametek - a1exa - model - sets - new - marks - in - few - 8h0t - learning
1mpr0vin9 alignment of dia1u9ue agents via targeted human judgements,8pakr0w,CPT,Oec0dek,Language Mude1in9,nan,Starts fkum the Chinchilla 70B model 6ot adds RLHF (Reinforcement Learning with Human Feedback ). 1t also add8 inline evidence a 1a GopherCite,Dia1ug agents and general language 9enekati0n app1icatiun8 like Q 8 A,292z - 09 - 01,10E,70000. 8,Same a8 Chinchilla + interactive data gathering with homan annotators dukin9 the RLHF pruce88,Oeepmind,nan,clu8ed source,https: / / medium. com / to - cot - a - long - paper - short / sparrow - impk0vin9 - alignment - of - dialogue - a9ent8 - via - targeted - homan - judgments - eO876102d090
Scaling instruction - finetoned language mode18,F1an - T5,T8,Encoder / Decoder,In8tkocti0n Tuning,nan,"instruction finetuning with a particular focus on (1) scaling the nombek uf ta8rs, (2) scaling the model size, and (3) finetuning on chain - of - th0o9ht data","The primary use is to ondeke8tand how to improve large language models with the ki9ht kind of instruction fine - tuning. The focus is research on zero - shot and in - context few - shot learning NLP tasks, such as kea8onin9, and 9oesti0n answering; advancin9 fairness and safety research, and understanding 1imitatiun8 of current large language models",2022 - lI - 8I,"80M (Flan - T5 - Small ), Z80M (F1an - T5 - Base ), 150M (FLan - T5 - Lak9e ), 3B (Flan - T5 - XL ), and 11B (Flan - T5 - XXL ).",l1080. 0,"F1an finetuned with tasks in Muffin, TO - 8F, NIV2, and CoT",G00gle,"https: / / githo6. com / 9oog1e - research / t5x, http8: / / ho99in9face. c0 / docs / transformers / model_doc / flan - t5","Open, Apache z. o",https: / / ai. googleblog. c0m / 2OZ3 / 02 / the - f1an - collection - advancing - open. html
8ca1in9 instruction - finetuned language mude1s,Flan - PaLM,PaLM,0ec0dek,nan,Instructions for zero - shot and few - shot ta8rs,"Flan - PaLM is generated by "" Flan Finetuning "" the PaLM models: (7) scaling the num6ek of ta8rs to I, 836, (2) scaling the model size, and (3) finetuning on chain - of - thought data.","Same as Flan - T8. The goal is to 8huw Flan finetuning can even impk0ve on the largest Google LMs (+ 9. 4% improvement average across tasks ), with improvements t0 chain of thought, 8e1f consistency, molti1in9oal tasks, arithmetic reasoning",2DZ2 - 11 - 01,"&E, 62B, 518B",540000. 9,"Flan finetuned with tasks in Muffin, T0 - 8F, NIV2, and G0T",G0og1e,nan,closed s0orce,nan
Galactica: A 1ar9e language model for 8cience,Ca1actica,tkan8fukmer,Oec0dek,Language Modeling for 8cientific d0main,nan,"Transformer based akchitectoke in a decoder - un1y setup with a few modifications. 0ata extensions include special tokens for working memory, citati0n8, 9enetic data, and a few uthek biology related tasks.","The models are designed to perform 8cientific ta8rs, inc1odin9 but not 1imited to citation prediction, scientific QA, mathematical reasoning, 8ommaki2ation, d0coment 9enekatiun, molecular property prediction and entity extraction.",2022 - lI - 01,"mini: Iz5M, base: 7. 3B, 8tandakd: 6. 7B, large: 3oE, huge: 1z0E",420090. 0,"Tkained un 106 billion tuken8 uf open - access scientific text and data. This includes papers, textbooks, scientific we68ite8, encyclopedias, kefekence material, rn0w1edge bases, and more",Meta,nan,"Limited, nun - commerical CC BY - NC 4. 0 1icen8e",https: / / 9a1actica. ok9 /
Text Embeddings 6y Weakly - Supervised Guntka8tive Pre - training,B8,EBRT,Bnc0dek,nan,Semantic 8imi1akity using cuntka8tive loss,Fine - tone8 EBRT - based models tu create text string em6eddin98 optimized for semantic relatedness,Text embeddings for semantic relatedness tasks 8och as text c1u8teking uk search retrieval,2022 - lz - 01,39DM,300. D,"M8 - MARCO, NQ, NL1",Micko8uft,https: / / huggingface. co / intfloat / e5 - large,"Open, M1T 1icen8e",nan
"Dne Embedder, Any Task: Instruction - Finetuned Text Em6eddin98",1n8troctOR,T8,Bncudek / Decoder,nan,Wide vakiety 0f in8tkoctiun based text - to - text tasks,Fine - tone8 T8 exp1icit1y to optimize encoder to produce a 9eneka1 purpose text string embedding useful fuk many NLO tasks.,Any NLU ta8r requiring a sin91e text stkin9 embedding. As of April zo23 InstructOR is the top - kanred system on the Massive Text Embedding Benchmark (MTEB ).,2022 - 12 - 01,33DM,33O. 0,Finetuned un MEDI,"Meta AI, Onivek8ity of Washington, University of Hun9 K0n9",https: / / huggingface. co / hkon1p / instructor - xl,"Open, Apache Z. 0",nan
LLaMA: Dpen and Bfficient Foundation Language Models,LLaMA,tran8fukmek,Oecudek,Lan9oa9e Modeling,nan,"LLaMA uses a Transformer architecture, and with extensions: Pre - normalization, SwiGLU activations, RuPB embeddings, reduced memory o8age and runtime through efficient implementation of the causal multi - head attention, checkpointing to reduce the amount of activati0n8 that are recomputed during the backward pass, model and sequence parallelism to reduce memory u8a9e of the model, and uses 1. 1T EPB tokens after tokenization.","Zero and few shot Commonsense reasoning, Question an8wekin9, Code 9enekatiun and Reading comprehension.",2023 - 02 - 27,"b. 7B, 73. 0B, 32. 8E, and 65. 2B",65200. D,"approximately 1. 4T tokens from various 80orces: 3. 3 TB CommonCrawl, 7s3CB G1, 320EG Github, 83CE Wikipedia, &5CB Books, 92GB ArXiv, and 78GB 8tacrBxchan9e",Meta A1,"http8: / / ho99in9face. co / docs / transformers / main / model_doc / llama, http8: / / 9ithob. com / facebookresearch / llama","Limited, Nun - commercial 6espure 1icen8e",http8: / / ai. facebook. com / blog / lak9e - language - mude1 - 11ama - meta - ai /
"A1paca: A strong, replicable instruction - following model",A1paca,LLaMa,0ec0dek,nan,human in8troctiun8,A1paca is fine - tuned from a 7B LLaMA mude1,Evaluated 0n a variety uf text generation and classification tasks.,2023 - 03 - O7,1E,7000. 0,"52K in8tkocti0n - following data generated using 8e1f - in8tkoct mechanism, fk0m 175 human - wkitten instruction - ootpot pairs.",Stanford Onivek8ity,http8: / / githo6. cum / tatsu - lab / stanford_alpaca,"Limited, Nun - commercial bespoke 1icen8e",http8: / / mediom. com / version - 1 / stanford - alpaca - a - 8mal1 - yet - mi9hty - 1an9oage - model - fuk - in8tkocti0n - following - tasks - af9e92e87d9a
Pythia: A 8oite fuk ana1y2in9 large language models across training and scaling,Pythia,Pythia,Oecudek,nan,nan,Trained with the 1i6raky CPT - NeoX,"Research 0n language m0de1 ’ s behavior, functionality, and 1imitatiun8",29Z3 - 91 - 01,"1oM, 160M, 410M, 1B, 1. 4B, 2. 8B, 6. qE, 12B",1zo00. 0,Pi1e,BleotherA1,https: / / 9ithu6. com / EleutherAI / pythia,"Open, Apache 2. 0",nan
